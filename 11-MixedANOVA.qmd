---
author: 
  - Mallory L. Barnes
aliases: [more-on-factorial-designs.html]
abstract-title: Chapter notes
abstract: Portions adapted from the Factorial ANOVA chapter, contributors Keryn Bain, Rachel Blakey, Stephanie Brodie, Corey Callaghan, Will Cornwell, Kingsley Griffin, Matt Holland, James Lavender, Andrew Letten, Shinichi Nakagawa, Shaun Nielsen, Alistair Poore, Gordana Popovic, Fiona Robinson and Jakub Stoklosa. "Environmental Computing" <https://environmentalcomputing.net/>
---

```{r, include = FALSE}
source("global_stuff.R")
```

# More On Factorial Designs

```{r}
library(ggplot2)
```

In this chapter, we're diving deeper into factorial designs, a cornerstone of understanding complex data in environmental science. You're already familiar with the idea of having more than one independent variable (IV) in your experiments. These IVs can be structured in various ways: all between-subjects, all within-subjects (like repeated measures), or a mix of both. ANOVA is our trusty tool to analyze these designs, giving us insights into each IV's main effect and their interactions.

## Looking at main effects and interactions

Factorial designs are very common in environmental research. You'll often come across studies showing results from these designs. It's crucial to be comfortable interpreting these results. The key skill here is to recognize patterns of main effects and interactions in data graphs. This can get tricky with more than two IVs, each having multiple levels.

### 2x2 designs

Let's explore 2x2 designs. Here, you can expect two main effects and one interaction. You'll compare means for each main effect and interaction. There are eight possible outcomes in such a design:

1.  no IV1 main effect, no IV2 main effect, no interaction
2.  IV1 main effect, no IV2 main effect, no interaction
3.  IV1 main effect, no IV2 main effect, interaction
4.  IV1 main effect, IV2 main effect, no interaction
5.  IV1 main effect, IV2 main effect, interaction
6.  no IV1 main effect, IV2 main effect, no interaction
7.  no IV1 main effect, IV2 main effect, interaction
8.  no IV1 main effect, no IV2 main effect, interaction

OK, so if you run a 2x2, any of these 8 general patterns could occur in your data. That's a lot to keep track of isn't it? As you develop your skills in examining graphs that plot means, you should be able to look at the graph and visually guesstimate if there is, or is not, a main effect or interaction. You will need you inferential statistics to tell you for sure, but it is worth knowing how to know see the patterns.

Let's visualize these outcomes using R. We'll create bar and line graphs to illustrate these patterns. Bar graphs are great for seeing differences in means directly, while line graphs help us spot interactions â€“ look for crossing lines as a hint of interaction. @fig-11bar22 shows the possible patterns of main effects and interactions in bar graph form. Here is a legend for the labels in the panels.

-   1 = there was a main effect for IV1.
-   \~1 = there was **not** a main effect for IV1
-   2 = there was a main effect for IV2
-   \~2 = there was **not** a main effect of IV2
-   1x2 = there was an interaction
-   \~1x2 = there was **not** an interaction

```{r}
#| label: fig-11bar22
#| fig-cap: "8 Example patterns for means for each of the possible kinds of general outcomes in a 2x2 design."
#| out-width: "100%"



p1 <- data.frame(
  IV1 = c("A", "A", "B", "B"),
  IV2 = c("1", "2", "1", "2"),
  means = c(5, 5, 5, 5)
)

p2 <- data.frame(
  IV1 = c("A", "A", "B", "B"),
  IV2 = c("1", "2", "1", "2"),
  means = c(10, 10, 5, 5)
)

p3 <- data.frame(
  IV1 = c("A", "A", "B", "B"),
  IV2 = c("1", "2", "1", "2"),
  means = c(10, 13, 5, 2)
)

p4 <- data.frame(
  IV1 = c("A", "A", "B", "B"),
  IV2 = c("1", "2", "1", "2"),
  means = c(5, 10, 10, 15)
)

p5 <- data.frame(
  IV1 = c("A", "A", "B", "B"),
  IV2 = c("1", "2", "1", "2"),
  means = c(10, 18, 5, 7)
)

p6 <- data.frame(
  IV1 = c("A", "A", "B", "B"),
  IV2 = c("1", "2", "1", "2"),
  means = c(5, 10, 5, 10)
)

p7 <- data.frame(
  IV1 = c("A", "A", "B", "B"),
  IV2 = c("1", "2", "1", "2"),
  means = c(2, 12, 5, 9)
)

p8 <- data.frame(
  IV1 = c("A", "A", "B", "B"),
  IV2 = c("1", "2", "1", "2"),
  means = c(5, 10, 10, 5)
)

all_22s <- rbind(p1, p2, p3, p4, p5, p6, p7, p8)

type <- c(
  rep("~1, ~2, ~1x2", 4),
  rep("1, ~2, ~1x2", 4),
  rep("1, ~2, 1x2", 4),
  rep("1, 2, ~1x2", 4),
  rep("1, 2, 1x2", 4),
  rep("~1, 2, ~1x2", 4),
  rep("~1, 2, 1x2", 4),
  rep("~1, ~2, 1x2", 4)
)

type <- as.factor(type)

all_22s <- cbind(all_22s, type)

ggplot(all_22s, aes(
  x = IV1,
  y = means,
  group = IV2,
  fill = IV2
)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_classic() +
  facet_wrap( ~ type, nrow = 2) +
  theme(legend.position = "top")
                             
```

@fig-11lines22 shows the same eight patterns in line graph form:

```{r}
#| label: fig-11lines22
#| fig-cap: "Line graphs showing 8 possible general outcomes for a 2x2 design."
#| out-width: "100%"

ggplot(all_22s, aes(x=IV1, y=means, group=IV2, color=IV2))+
  geom_point()+
  geom_line()+
  theme_classic()+
  facet_wrap(~type, nrow=2)+
  theme(legend.position = "top")
                             
```

In line graphs, interactions are more apparent. Parallel lines suggest no interaction, while crossing lines indicate potential interactions. The position of points relative to each other helps identify main effects. Things get complicated fast. When designing experiments, aim for the minimum number of independent variables (IVs) and levels needed to answer your research question. This approach makes interpreting your data more straightforward and your conclusions clearer. Whenever you see that someone ran a 4x3x7x2 design, your head should spin. It's just too complicated.

## Interpreting main effects and interactions

Understanding main effects and interactions is essential for accurately interpreting research data, especially in complex fields like environmental science.

A **main effect** refers to the consistent impact of an independent variable (IV) on a dependent variable (DV). For example, in environmental studies, consider the effect of a specific fertilizer (IV) on plant growth (DV). If using this fertilizer consistently results in increased growth compared to not using it, we observe a clear main effect. This effect remains true regardless of other variables such as soil type or weather conditions.

Often, it is convenient to think of main effects as a consistent influence of one manipulation. However, the picture changes when we introduce an interaction. An interaction occurs when the effect of one IV depends on another IV. By definition, an interactino means that some main effect is **not** behaving consistently across different situations. For instance, the impact of our fertilizer might vary depending on the level of sunlight or the type of soil, indicating an interaction between these factors and the fertilizer. This interaction disrupts the consistency of the main effect, suggesting that the effect of the fertilizer is not uniform across all conditions.

Researchers often phrase their findings to highlight this complexity: "We found a main effect of X, **BUT**, this main effect was qualified by an interaction between X and Y." The use of "BUT" here is crucial. It signals that the main effect cannot be fully understood without considering the interaction. The interaction indicates that the influence of the IV changes under different conditions, making it essential to consider these variables together for a complete understanding.

In environmental science, this becomes particularly relevant when studying ecosystems or climate interactions, where multiple variables interplay in complex ways. The interpretation of main effects and interactions in such contexts is not just about identifying individual effects but understanding how these effects change in different environmental settings.

Here are two generalized examples to help you make sense of these issues:

### A consistent main effect and an interaction

```{r}
#| label: fig-11mainint
#| fig-cap: "Example means showing a generally consistent main effect along with an interaction"
#| out-width: "100%"
#| fig-asp: 0.5


p7 <- data.frame(
  IV1 = c("A", "A", "B", "B"),
  IV2 = c("1", "2", "1", "2"),
  means = c(2, 12, 5, 9)
)

ggplot(p7, aes(
  x = IV1,
  y = means,
  group = IV2,
  color = IV2
)) +
  geom_point() +
  geom_line() +
  theme_classic() +
  theme(legend.position = "top")

```

@fig-11mainint shows a main effect and interaction. There is a main effect of IV2: the level 1 means (red points and line) are both lower than the level 2 means (aqua points and line). There is also an interaction. The size of the difference between the red and aqua points in the A condition (left) is bigger than the size of the difference in the B condition.

**How would we interpret this**? We could say there WAS a main effect of IV2, BUT it was qualified by an IV1 x IV2 interaction.

**What's the qualification**? The size of the IV2 effect changed as a function of the levels of IV1. It was big for level A, and small for level B of IV1.

**What does the qualification mean for the main effect**? Well, first it means the main effect can be changed by the other IV. That's important to know. Does it also mean that the main effect is not a real main effect because there was an interaction? Not really, there is a generally consistent effect of IV2. The green points are above the red points in all cases. Whatever IV2 is doing, it seems to work in at least a couple situations, even if the other IV also causes some change to the influence.

### An inconsistent main effect and an interaction

```{r}
#| label: fig-11mainintInc
#| fig-cap: "Example data showing how an interaction exists, and a main effect does not, even though the means for the main effect may show a difference"
#| out-width: "100%"
#| fig-asp: 0.5

p7 <- data.frame(
  IV1 = c("A", "A", "B", "B"),
  IV2 = c("1", "2", "1", "2"),
  means = c(5, 10, 5, 5)
)

ggplot(p7, aes(
  x = IV1,
  y = means,
  group = IV2,
  color = IV2
)) +
  geom_point() +
  geom_line() +
  theme_classic() +
  theme(legend.position = "top")

```

@fig-11mainintInc shows another 2x2 design. You should see an interaction here straight away. The difference between the aqua and red points in condition A (left two dots) is huge, and there is 0 difference between them in condition B. Is there an interaction? Yes!

Are there any main effects here? With data like this, sometimes an ANOVA will suggest that you do have significant main effects. For example, what is the mean difference between level 1 and 2 of IV2? That is the average of the green points ( (10+5)/2 = 15/2= 7.5 ) compared to the average of the red points (5). There will be a difference of 2.5 for the main effect (7.5 vs. 5).

Starting to see the issue here? From the perspective of the main effect (which collapses over everything and ignores the interaction), there is an overall effect of 2.5. In other words, level 2 adds 2.5 in general compared to level 1. However, we can see from the graph that IV2 does not do anything in general. It does not add 2.5s everywhere. It adds 5 in condition A, and nothing in condition B. It only does one thing in one condition.

What is happening here is that a "main effect" is produced by the process of averaging over a clear interaction.

**How would we interpret this**? We might have to say there was a main effect of IV2, BUT we would definitely say it was qualified by an IV1 x IV2 interaction.

**What's the qualification**? The size of the IV2 effect completely changes as a function of the levels of IV1. It was big for level A, and nonexistent for level B of IV1.

**What does the qualification mean for the main effect**? In this case, we might doubt whether there is a main effect of IV2 at all. It could turn out that IV2 does not have a general influence over the DV all of the time, it may only do something in very specific circumstances, in combination with the presence of other factors.

## Mixed Designs

In this book, we've explored various research designs, emphasizing that they can take different forms. These designs can be categorized as either between-subjects, where different subjects are in each group, or within-subjects, where the same subjects participate in all conditions. When you combine these approaches in a single study, you create what's known as a mixed design.

A mixed design occurs when one of your independent variables (IVs) is treated as a between-subjects factor, while another is treated as a within-subjects factor. This blend offers a unique approach to examining how different variables interact and affect the outcome.

In environmental science research, mixed designs are particularly useful for studying complex interactions between variables that vary both within and between subjects. For instance, consider a study examining the impact of a new agricultural technique (IV1) on crop yield (DV). This technique could be applied to different plots of land (between-subjects factor), while also measuring the impact across different seasons (within-subjects factor). Such a design allows researchers to understand not only the overall effectiveness of the technique but also how its impact varies seasonally.

The key to successfully navigating mixed designs lies in understanding how to calculate the appropriate statistical measures. Specifically, the F-values for each effect in an ANOVA (Analysis of Variance) are constructed using different error terms, depending on whether the IV is a within-subjects or between-subjects factor. While it's possible to run an ANOVA with any combination of between and within-subjects IVs, the complexity increases with the number of variables and their categorizations.

As this is an introductory text, we won't delve into the detailed formulas for constructing ANOVA tables with mixed designs. More advanced textbooks offer comprehensive discussions on this topic, and many resources are available online for those interested in deeper exploration.

## More complicated designs

Up until now we have focused on the simplest case for factorial designs, the 2x2 design, with two IVs, each with 2 levels. It is worth spending some time looking at a few more complicated designs and how to interpret them.

### 3x2 design

In a 3x2 design there are two IVs. IV1 has three levels, and IV2 has two levels. Typically, there would be one DV. Let's apply this to an environmental science scenario.First, let's make the design concrete. 

Imagine a study examining the impact of different irrigation methods (IV1: drip irrigation vs. sprinkler irrigation) on crop yield (DV) across three types of soil (IV2: sandy, loamy, clayey). The main effects would be the overall impact of irrigation method and soil type on crop yield, while the interaction would explore how these effects vary together.

For instance, drip irrigation might consistently produce higher yields than sprinkler irrigation, showing a main effect of IV1. Soil type might also independently affect yield, with loamy soil perhaps leading to the highest yields, followed by clayey and sandy soils, indicating a main effect of IV2. An interaction would occur if, for example, the advantage of drip irrigation over sprinkler irrigation is more pronounced in sandy soil compared to clayey soil. Note that these examples are hypothetical to illustrate the concept.

We might expect data like shown in @fig-1123design:

```{r}
#| label: fig-1123design
#| fig-cap: "Example means for a 3x2 factorial design in environmental science"
#| out-width: "100%"
#| fig-asp: 0.5

crop_yield <- c(2.7, 1.8, 2.9, 2.1, 3.0, 2.4)  # Example crop yields
irrigation <- rep(c("Drip", "Sprinkler"), 3)    # Two levels of irrigation method
soil_type <- as.factor(rep(c("Sandy", "Loamy", "Clayey"), each = 2))  # Three types of soil

df <- data.frame(crop_yield, irrigation, soil_type)

ggplot(df, aes(x = soil_type, y = crop_yield, color = irrigation, group = irrigation)) +
  geom_point() +
  geom_line() +
  theme_classic()

```

The figure shows some pretend means in all conditions. Let's talk about the main effects and interaction.

1. **Main Effect of Irrigation Method**: The main effect of the irrigation method is evident. Drip irrigation (represented by red line) generally leads to higher crop yields compared to sprinkler irrigation (represented by aqua line).

2. **Main Effect of Soil Type**: The main effect of soil type is clearly present. Clayey soils show the highest yield, followed by loamy soils, then sandy soils 

3. **Interaction Between Irrigation Method and Soil Type**: Is there an interaction? Yes, there is. Remember, an interaction occurs when the effect of one IV depends on the levels of an another. The advantage of drip irrigation over sprinkler irrigation is more pronounced in sandy soil compared to clayey soil. So, the size of the irrigation effect (drip vs. sprinkler) changes with the type of soil. There is evidence in the means for an interaction. You would have to conduct an inferential test on the interaction term to see if these differences were likely or unlikely to be due to sampling error.

If there was no interaction and no main effect of repetition, we would see something like the pattern in @fig-1123one.

```{r}
#| label: fig-1123one
#| fig-cap: "Example means for a 3x2 design in environmental science with only one main effect"
#| out-width: "100%"
#| fig-asp: 0.5

crop_yield <- c(2.5, 2.0, 2.5, 2.0, 2.5, 2.0)  # Constant yields for each soil type
irrigation <- rep(c("Drip", "Sprinkler"), 3)    # Two levels of irrigation method
soil_type <- as.factor(rep(c("Sandy", "Loamy", "Clayey"), each = 2))  # Three types of soil

df <- data.frame(crop_yield, irrigation, soil_type)

ggplot(df, aes(x = soil_type, y = crop_yield, color = irrigation, group = irrigation)) +
  geom_point() +
  geom_line() +
  theme_classic()
```

What would you say about the interaction if you saw the pattern in @fig-1123int?

```{r}
#| label: fig-1123int
#| fig-cap: "Example means for a 3x2 design in environmental science showing a different interaction pattern"
#| out-width: "100%"
#| fig-asp: 0.5

crop_yield <- c(2.5, 2.3, 2.5, 1.8, 2.5, 1.8)  # Varied yields indicating different interactions
irrigation <- rep(c("Drip", "Sprinkler"), 3)    # Two levels of irrigation method
soil_type <- as.factor(rep(c("Sandy", "Loamy", "Clayey"), each = 2))  # Three types of soil

df <- data.frame(crop_yield, irrigation, soil_type)

ggplot(df, aes(x = soil_type, y = crop_yield, color = irrigation, group = irrigation)) +
  geom_point() +
  geom_line() +
  theme_classic()
```

The correct answer is that there is evidence in the means for an interaction. Remember, we are measuring the irrigation effect (effect of drip vs. sprinkler) three times. The irrigation effect is the same for clayey and loamy soils, but it is much smaller for sandy soils. The size of the irrigation effect depends on the levels of the soil type IV, so here again there is an interaction.

### 2x2x2 designs

Let's take it up a notch and look at a 2x2x2 design. In a 2x2x2 design, there are three independent variables (IVs), each with two levels. This design allows for the examination of three main effects, three two-way interactions, and one three-way interaction.

We'll add another variable to our example from before: crop type (wheat vs. corn) as our third IV. n this 2x2x2 design, we'll consider three independent variables (IVs): irrigation method (IV1: drip vs. sprinkler), soil type (IV2: sandy vs. loamy), and crop type (IV3: wheat vs. corn). The dependent variable (DV) is crop yield. This setup allows us to examine three main effects, three two-way interactions, and one three-way interaction.

Let's consider a study on the effects of fertilizer type (IV1: organic vs. synthetic), watering frequency (IV2: daily vs. weekly), and crop type (IV3: wheat vs. corn) on overall crop health (DV). This design would enable researchers to explore not only the individual effects of each IV on crop health but also how these factors interact in pairs and altogether.

```{r}
#| label: fig-11222
#| fig-cap: "Example means from a 2x2x2 design in environmental science with no three-way interaction."
#| out-width: "100%"
#| fig-asp: 0.5

crop_yield <- c(2.7, 2.1, 2.9, 2.3,
                2.7, 2.1, 2.9, 2.3)
irrigation <- as.factor(rep(c("Drip", "Sprinkler"), 4))
soil_type <- as.factor(rep(rep(c("Sandy", "Loamy"), each = 2), 2))
crop_type <- as.factor(rep(c("Wheat", "Corn"), each = 4))
df <- data.frame(crop_yield, irrigation, soil_type, crop_type)

ggplot(df,
       aes(
         x = soil_type,
         y = crop_yield,
         color = irrigation,
         group = irrigation
       )) +
  geom_point() +
  geom_line() +
  theme_classic() +
  facet_wrap(~ crop_type)
```

n @fig-11222, we have two panels: one for wheat and one for corn. This can be thought of as two 2x2 designs, one for each crop type. The graphs for wheat and corn are similar, showing a 2x2 interaction between irrigation method and soil type. There is a main effect of irrigation, a main effect of soil type, no main effect of crop type, and no three-way interaction.

A three-way interaction might be observed if, for example, the advantage of drip irrigation over sprinkler is more pronounced for wheat than for corn, and this difference is further influenced by the soil type. Such a complex interaction provides deep insights into how these variables collectively influence crop yield.

What is a three-way interaction anyway? That would occur if there was a difference between the 2x2 interactions. For example, consider the pattern of results in @fig-11222int.

```{r}
#| label: fig-11222int
#| fig-cap: "Example means from a 2x2x2 design in environmental science with a three-way interaction."
#| out-width: "100%"
#| fig-asp: 0.5

crop_yield <- c(2.7, 2.1, 2.9, 2.3,
                2.7, 2.4, 3.0, 2.6)
irrigation <- as.factor(rep(c("Drip", "Sprinkler"), 4))
soil_type <- as.factor(rep(rep(c("Sandy", "Loamy"), each = 2), 2))
crop_type <- as.factor(rep(c("Wheat", "Corn"), each = 4))
df <- data.frame(crop_yield, irrigation, soil_type, crop_type)

ggplot(df,
       aes(
         x = soil_type,
         y = crop_yield,
         color = irrigation,
         group = irrigation
       )) +
  geom_point() +
  geom_line() +
  theme_classic() +
  facet_wrap(~ crop_type)
```

We are looking at a 3-way interaction between modality, repetition and delay. What is going on here? These results would be very strange, here is an interpretation.

In @fig-11222int, the pattern of interaction changes between wheat and corn, indicating a three-way interaction. For instance, the advantage of drip irrigation over sprinkler might be more pronounced in sandy soil for wheat, but less so for corn. This suggests that the combined effect of irrigation method, soil type, and crop type on yield is not uniform across all conditions, highlighting the complexity of agricultural systems.



For auditory stimuli, we see that there is a small forgetting effect when people studied things once, but the forgetting effect gets bigger if they studies things twice. A pattern like this would generally be very strange, usually people would do better if they got to review the material twice.

The visual stimuli show a different pattern. Here, the forgetting effect is large when studying visual things once, and it get's smaller when studying visual things twice.

We see that there is an interaction between delay (the forgetting effect) and repetition for the auditory stimuli; BUT, this interaction effect is **different** from the interaction effect we see for the visual stimuli. The 2x2 interaction for the auditory stimuli is **different** from the 2x2 interaction for the visual stimuli. In other words, there is an interaction between the two interactions, as a result there is a three-way interaction, called a 2x2x2 interaction.

We will note a general pattern here. Imagine you had a 2x2x2x2 design. That would have a 4-way interaction. What would that mean? It would mean that the pattern of the 2x2x2 interaction changes across the levels of the 4th IV. If two three-way interactions are different, then there is a four-way interaction.


## Example of a 3x2 design - real data

*Note - we did most of this in the class activity on Thursday's class, so you can skim this part if you wish* 

Consider an example where a researcher is testing the effects of metal contamination on the number of species found in sessile marine invertebrates (sponges, bryozoans and sea squirts etc.). They would like to know whether copper reduces species richness, but also know that the richness of invertebrates can depend on whether the substrate is vertical or horizontal. Consequently, they ran an experiment where species richness was recorded in replicate samples in each of the six combinations of copper enrichment ("None","Low","High") and orientation ("Vertical","Horizontal"). 

The factorial ANOVA will test:  
* whether there are any differences in richness among the three levels of copper enrichment  
* whether there are any differences in richness among the two levels of substrate orientation  
* whether there is any interaction between copper and orientation

You have three null hypotheses:  
* there is no difference between the means for each level of copper, H~o~: $\mu_{None} = \mu_{Low} = \mu_{High}$  
* there is no difference between the means for each level of orientation, $H~o~: $\mu_{Vertical} = \mu_{Horizontal}$  
* there is no interaction between the factors

This is far better than running two separate single factor ANOVAs that contrast copper effects for each level of orientation because you have more statistical power (higher degrees of freedom) for the tests of interest, and you get a formal test of the interaction between factors which is often scientifically interesting.

Note that an ANOVA is a linear model, just like linear regression except that the predictor variables are categorical rather than continuous. With two predictor variables, the linear model is:

$$y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha\beta)_{ij} + \varepsilon_{ijk}$$

where $\mu$ is the overall mean, $\alpha_i$ is the effect of the i^th^ group of the first factor, and $\beta_i$ is the effect of the j^th^ group of the second factor and $(\alpha\beta)$ is the interaction.

Athough we have two factors, and an interaction effect, this requires fitting more than 3 parameters in our model because we have 3 levels of Factor A (Copper) and 2 levels of Factor B (Orientation) (if you can figure out how many parameters must be fit in this model, you are officially a stats geek! This is tricky even for those 'in the know').

With two factors, ANOVA partitions the total variance into a component that can be explained by the first predictor variable (among levels of the treatment A), a component that can be explained by the second predictor variable (among levels of the treatment B), a component that can be explained by the interaction, and a component that cannot be explained (within levels, the residual variance). The test statistic, *F*, is calculated three times to test each of the null hypotheses. For two fixed factors, the *F* ratios are:

$$F = \frac{MS_{A}}{MS_{within}}$$
$$F = \frac{MS_{B}}{MS_{within}}$$
$$F = \frac{MS_{AB}}{MS_{within}}$$

where *MS* are the mean squares, a measure of variation. The probability of obtaining the observed value of *F* is calculated from the known probability distribution of *F*, with two degrees of freedom (one for the numerator = the number of levels -1) and one for the denominator. Note that these *F* ratios will change if any factors are random (see below for the distinction between fixed and random factors),
  


This example is from class 
 
Your data should be formatted with the measurements from each replicate as a row and each of the variables as columns, corresponding to the dependent variable *y*, Factor A and Factor B.

We'll download a sample data set for sessile invertebrates and import into R to to see the desired format. Check that your predictor variables are factors with the function `str`

```{r, eval=FALSE}
Sessile <- read.csv(file = "data/Sessile.csv", header = TRUE)
str(Sessile)
```

With our predictor variable correctly assigned as factors, we can now run the analysis. As with other forms of linear models we have a model formula with the dependent variable, *y*, to the left of the ~ and the predictor variables to the right. For this two factor design, we use:

```{r, eval=FALSE}
Sessile.aov <- aov(Richness ~ Copper * Orientation, data = Sessile)
```

Note that when you specify a model with * between the two predictors, R automatically includes both variables and their interaction. This same model could also be written as:

```{r, eval = FALSE}
Sessile.aov <- aov(Richness ~ Copper + Orientation + Copper:Orientation, data = Sessile)
```

The output from this analysis can be seen by using the `summary` function on the object created.

```{r, eval=FALSE}
summary(Sessile.aov)
# Calculate the mean species richness for each combination of Copper and Orientation
means_table <- aggregate(Richness ~ Copper + Orientation, data = Sessile, FUN = mean)

# Print the table of means
print(means_table)

# Set up the plot limits
xlim <- c("None", "Low", "High")
ylim <- c(40, 70)

# Create an empty plot
plot(1, type="n", xlab="Copper Level", ylab="Species Richness", xlim=c(1, 3), ylim=ylim, axes=FALSE)

# Define the axis
axis(1, at=1:3, labels=xlim)
axis(2)

# Add a legend
legend("topright", legend=c("Vertical Substrate", "Horizontal Substrate"))


```


Exactly the same model can also be run using the linear model function, `lm`.

```{r,eval=FALSE}
Sessile.lm <- lm(Richness ~ Copper * Orientation, data = Sessile)
anova(Sessile.lm)
```
  

### Interpreting the results
 
```{r,echo=FALSE}
Sessile <- read.csv(file = "data/Sessile.csv", header = TRUE)
Sessile.aov <- aov(Richness ~ Copper * Orientation, data = Sessile)
summary(Sessile.aov)
```
The summary output of an ANOVA object is a table with the degrees of freedom (Df), sums of squares (Sum Sq), mean squares (Mean Sq) for the each of predictor variable (i.e., variation among levels of your treatments), their interaction and for the Residuals (i.e., varation within the levels). The test statistic, *F* value and its associated *p*-value (Pr(>F)) are also presented.

Check that you have the correct degrees of freedom. For a two factor design with fixed factors they are:   
* Factor A: *a* - 1 (where *a* = number of levels of Factor A)  
* Factor B: *b* - 1 (where *b* = number of levels of Factor B)  
* Interaction (AB): (*a*-1)(*b*-1)  
* Residual: *ab*(*n* -1) (where *n* = sample size)

The sums of squares and mean squares are measures of variation. There are three *F* statistics, corresponding to a test of each of the main effects and one for the interaction. The *p*-values are the probabilities of the observed *F* values from the *F* distribution (with the given degrees of freedom). 

In this example, there is strong evidence to reject all three null hypotheses:  
* that all levels of the copper treatment are equal (P < 0.001),  
* that the vertical and horizontal orientations are equal (P < 0.001)  
* that there is no interaction between copper and orientation (P < 0.001)

A significant interaction means that the effect of one factor depends upon the other. In this example, it would mean that the effect of copper was not consistent between the vertical and horizontal habitats. Consequently, the interpretation of the main effects becomes more complex. See [Understanding interactions](/statistics/linear-models/interactions/) for more help on interpreting interactions in linear models. A quick way to help you understand an interaction if you get one is to examine an interactions plot.

```{r}
interaction.plot(Sessile$Copper, Sessile$Orientation, Sessile$Richness)
```


Here you can see that the effect of copper (a decline in species richness) is more pronounced in the habitats with a vertical orientation, and that the difference between the two habitats changes with exposure to copper.
 

**Multiple comparisons.** If you detect any significant differences in the ANOVA, we are then interested in knowing exactly which levels differ from one another, and which do not. Remember that a significant *p* value in the test you just ran would reject the null hypothesis the means were the same across all groups, but not identify which were different from each other. If there is no interaction, you can run a *post-hoc* test on each of the main effects (only needed if there are more than two levels for an effect). If there is an interaction, you will need to consider *post-hoc* tests that contrast the means from all combinations of both factors.
  

### Assumptions to check
 
The assumptions of factorial ANOVA's are the same as for all linear models including the simpler one-way ANOVA's (see [ANOVA: single factor](/statistics/linear-models/anova/anova-single/)), being independence, normality and homogeneity of variances. We also need to consider two new issues: 1) whether your factors are fixed or random, and 2) whether your sampling or experimental design is balanced (i.e., has the same number of replicates in each combination of treatments).

**Fixed and random factors.** There is an important distinction between factors whose levels are the only ones of interest (termed fixed), and factors whose levels are a sampled from a larger collection of possible levels (termed random). For example, if we repeated the experiment above at three different sites in Sydney Harbour, chosen from many possible sites, we would consider site a random factor. We are not interested in those sites in particular, but would like to know if our experimental treatments were consistent across sites. On the other hand, if you were only interested in Darling Harbour and Circular Quay, then these two could be considered two levels of a fixed factor. Treating sites as a fixed factor in that case means that you conclusions should not be extrapolated to other possible sites, but restricted to those particular sites.

Statistically, there is a big difference between a fixed factors were you have measured all possible levels of interest (e.g, control vs a single treatment) and random factors where the levels are sampled from all possible levels. In analysis of variance, all this matters because the *F* tests that are being used to test your hypotheses are constructed differently depending on which factors are fixed and random. In the example above, all factors were fixed and the denominator of all *F* tests was $MS_{within}$. In models with all factors random, and models with a mix of fixed and random factors (termed mixed effects models), other components of the variation are used as the denominators in the *F* tests.

If you have random factors, you will need to read more than this help page to establish the correct *F* ratios for your design, and you may need to calculate them manually. Note that the code presented will give correct *F* tests only for designs with all factors fixed. You should also strongly consider analysing your data as a [mixed model](/statistics/mixed-models/)

**Balanced and unbalanced designs.** Ideally, factorial ANOVA should be conducted with a balanced design - one with  the same number of replicates in each combination of factors. Balanced designs are less likely to be affected by minor deviations from the assumptions of normality and homogeneity of variance. Unfortunately, unbalanced designs where you have unequal numbers of replicates for each level are common in practice (e.g. bad weather prevented sampling the second site as intensively, volunteer lost the data sheet etc!).

Unbalanced designs are more susceptible to violating the assumptions of ANOVA and there is no single way to partitioning the $SS_{total}$ into the main effect and interaction components. The `aov` and `lm` functions in R use what are called Type I sums of squares where the terms in the model are fitted sequentially (i.e., how much variation is explained by factor A, then how much additional variation is explained by adding factor B). This means that the order of the terms in model matters: the model formulae `Y ~ A + B + A:B` and `Y ~ B + A + B:A` will give you different results.

There is a fair bit of debate on this in the statistical literature, but many advise using what are called Type II or Type III sums of squares for unbalanced designs. Other software packages like SPSS, SYSTAT and Minitab will automatically use Type III sums of squares where the order of terms in the model doesn't matter. To access these in R, we can use the `Anova` function in the [car](https://cran.r-project.org/web/packages/car/index.html) package.

**Normality.** The assumption of normality can be checked by a frequency histogram of the residuals or by using a quantile plot where the residuals are plotted against the values expected from a normal distribution. The histogram of residuals should follow a normal distribution. If the points in the quantile plot lie mostly on the line, the residuals are normally distributed. Both of these plots can be obtained from the model object created by the `aov` function.

```{r}
par(mfrow = c(1, 2)) # This code put two plots in the same window
hist(Sessile.aov$residuals)
plot(Sessile.aov, which = 2)
```

Violations of normality can be fixed via transformations or by using a different error-distribution in a [generalised linear model (GLM)](/statistics/glms/).

**Homogeneity of variance.** The assumption of homgeneity of variance, namely that the variation in the residuals is approximately equal across the range of the predictor variable, can be checked by plotting the residuals against the fitted values from the `aov` model object.

```{r}
plot(Sessile.aov, which = 1)
```

Heterogeneous variances are indicated by non-random pattern in the residuals vs. fitted plot. Look for an even spread of the residuals on the y axis for each of the levels on the x axis. A fan-shaped distribution with more variance at higher values on the x axis is a common problem when data are skewed. See the testing assumptions of linear models module for more information. If there are strong patterns, one potential solution is to transform the response variable *y*. If this doesn't fix the problem the best solution is to use a different error distribution in a  [generalised linear model (GLM)](/statistics/glms/).

**Independence.** ANOVA assumes that all replicate measures are independent of each other (i.e., equally likely to be sampled from the population of possible values for each level). This issue needs to be considered at the design stage. If data are grouped in any way (e.g., half the invertebrate samples measured at one time, then the other half measured later), then more complex designs are needed to account for additional factors (e.g., a design with an additional factor of sampling time).

There are a variety of measures for dealing with non-independence. These include ensuring all important predictors are in the model; averaging across nested observations; or using a [mixed model](/statistics/mixed-models/)
  

### Communicating the results
 
**Written.**  The results of the main effects and any interaction should be described in the text of a results section. Each *F* test can be described in the text, e.g., "The copper treatment and substrate orientation interacted to affect the species of sessile invertebrates (*F* = 19.33, df = 2,54, *p* < 0.001)". Alternatively, all tests could be put into a Table like the one given in the output following `summary(Sessile.aov)` above. Description of the main tests would be followed by a description of the *post-hoc* results if used.

Remember that the interpretation of the main effects is complicated when there is a significant interaction (see above). In this example, while copper reduced species richness, that effect was not consistent between the two habitats. In other scenarios with an interaction, you might have copper affecting richness in one habitat but not another, preventing you making a simple statement like "copper reduced species richness" because it wouldn't always be true. 

**Visual.** A boxplot or column graph with error bars are suitable for contrasting a continuous variable across levels of categorical variable. See the graphing modules for making publication ready versions of these figures.

```{r}
boxplot(Richness ~ Copper * Orientation, data = Sessile, names = c("High.H", "Low.H", "None.H", "High.V", "Low.V", "None.V"), ylab = "Species richness", xlab = "Copper/Orientation", ylim = c(0, 80))
```
  

