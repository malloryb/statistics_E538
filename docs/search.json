[
  {
    "objectID": "11-MixedANOVA.html#looking-at-main-effects-and-interactions",
    "href": "11-MixedANOVA.html#looking-at-main-effects-and-interactions",
    "title": "11  More On Factorial Designs",
    "section": "11.1 Looking at main effects and interactions",
    "text": "11.1 Looking at main effects and interactions\nFactorial designs are very common in environmental research. You’ll often come across studies showing results from these designs. It’s crucial to be comfortable interpreting these results. The key skill here is to recognize patterns of main effects and interactions in data graphs. This can get tricky with more than two IVs, each having multiple levels.\n\n11.1.1 2x2 designs\nLet’s explore 2x2 designs. Here, you can expect two main effects and one interaction. You’ll compare means for each main effect and interaction. There are eight possible outcomes in such a design:\n\nno IV1 main effect, no IV2 main effect, no interaction\nIV1 main effect, no IV2 main effect, no interaction\nIV1 main effect, no IV2 main effect, interaction\nIV1 main effect, IV2 main effect, no interaction\nIV1 main effect, IV2 main effect, interaction\nno IV1 main effect, IV2 main effect, no interaction\nno IV1 main effect, IV2 main effect, interaction\nno IV1 main effect, no IV2 main effect, interaction\n\nOK, so if you run a 2x2, any of these 8 general patterns could occur in your data. That’s a lot to keep track of isn’t it? As you develop your skills in examining graphs that plot means, you should be able to look at the graph and visually guesstimate if there is, or is not, a main effect or interaction. You will need you inferential statistics to tell you for sure, but it is worth knowing how to know see the patterns.\nLet’s visualize these outcomes using R. We’ll create bar and line graphs to illustrate these patterns. Bar graphs are great for seeing differences in means directly, while line graphs help us spot interactions – look for crossing lines as a hint of interaction. Figure 11.1 shows the possible patterns of main effects and interactions in bar graph form. Here is a legend for the labels in the panels.\n\n1 = there was a main effect for IV1.\n~1 = there was not a main effect for IV1\n2 = there was a main effect for IV2\n~2 = there was not a main effect of IV2\n1x2 = there was an interaction\n~1x2 = there was not an interaction\n\n\n\n\n\n\nFigure 11.1: 8 Example patterns for means for each of the possible kinds of general outcomes in a 2x2 design.\n\n\n\n\nFigure 11.2 shows the same eight patterns in line graph form:\n\n\n\n\n\nFigure 11.2: Line graphs showing 8 possible general outcomes for a 2x2 design.\n\n\n\n\nIn line graphs, interactions are more apparent. Parallel lines suggest no interaction, while crossing lines indicate potential interactions. The position of points relative to each other helps identify main effects. Things get complicated fast. When designing experiments, aim for the minimum number of independent variables (IVs) and levels needed to answer your research question. This approach makes interpreting your data more straightforward and your conclusions clearer. Whenever you see that someone ran a 4x3x7x2 design, your head should spin. It’s just too complicated."
  },
  {
    "objectID": "11-MixedANOVA.html#interpreting-main-effects-and-interactions",
    "href": "11-MixedANOVA.html#interpreting-main-effects-and-interactions",
    "title": "11  More On Factorial Designs",
    "section": "11.2 Interpreting main effects and interactions",
    "text": "11.2 Interpreting main effects and interactions\nUnderstanding main effects and interactions is essential for accurately interpreting research data, especially in complex fields like environmental science.\nA main effect refers to the consistent impact of an independent variable (IV) on a dependent variable (DV). For example, in environmental studies, consider the effect of a specific fertilizer (IV) on plant growth (DV). If using this fertilizer consistently results in increased growth compared to not using it, we observe a clear main effect. This effect remains true regardless of other variables such as soil type or weather conditions.\nOften, it is convenient to think of main effects as a consistent influence of one manipulation. However, the picture changes when we introduce an interaction. An interaction occurs when the effect of one IV depends on another IV. By definition, an interactino means that some main effect is not behaving consistently across different situations. For instance, the impact of our fertilizer might vary depending on the level of sunlight or the type of soil, indicating an interaction between these factors and the fertilizer. This interaction disrupts the consistency of the main effect, suggesting that the effect of the fertilizer is not uniform across all conditions.\nResearchers often phrase their findings to highlight this complexity: “We found a main effect of X, BUT, this main effect was qualified by an interaction between X and Y.” The use of “BUT” here is crucial. It signals that the main effect cannot be fully understood without considering the interaction. The interaction indicates that the influence of the IV changes under different conditions, making it essential to consider these variables together for a complete understanding.\nIn environmental science, this becomes particularly relevant when studying ecosystems or climate interactions, where multiple variables interplay in complex ways. The interpretation of main effects and interactions in such contexts is not just about identifying individual effects but understanding how these effects change in different environmental settings.\nHere are two generalized examples to help you make sense of these issues:\n\n11.2.1 A consistent main effect and an interaction\n\n\n\n\n\nFigure 11.3: Example means showing a generally consistent main effect along with an interaction\n\n\n\n\nFigure 11.3 shows a main effect and interaction. There is a main effect of IV2: the level 1 means (red points and line) are both lower than the level 2 means (aqua points and line). There is also an interaction. The size of the difference between the red and aqua points in the A condition (left) is bigger than the size of the difference in the B condition.\nHow would we interpret this? We could say there WAS a main effect of IV2, BUT it was qualified by an IV1 x IV2 interaction.\nWhat’s the qualification? The size of the IV2 effect changed as a function of the levels of IV1. It was big for level A, and small for level B of IV1.\nWhat does the qualification mean for the main effect? Well, first it means the main effect can be changed by the other IV. That’s important to know. Does it also mean that the main effect is not a real main effect because there was an interaction? Not really, there is a generally consistent effect of IV2. The green points are above the red points in all cases. Whatever IV2 is doing, it seems to work in at least a couple situations, even if the other IV also causes some change to the influence.\n\n\n11.2.2 An inconsistent main effect and an interaction\n\n\n\n\n\nFigure 11.4: Example data showing how an interaction exists, and a main effect does not, even though the means for the main effect may show a difference\n\n\n\n\nFigure 11.4 shows another 2x2 design. You should see an interaction here straight away. The difference between the aqua and red points in condition A (left two dots) is huge, and there is 0 difference between them in condition B. Is there an interaction? Yes!\nAre there any main effects here? With data like this, sometimes an ANOVA will suggest that you do have significant main effects. For example, what is the mean difference between level 1 and 2 of IV2? That is the average of the green points ( (10+5)/2 = 15/2= 7.5 ) compared to the average of the red points (5). There will be a difference of 2.5 for the main effect (7.5 vs. 5).\nStarting to see the issue here? From the perspective of the main effect (which collapses over everything and ignores the interaction), there is an overall effect of 2.5. In other words, level 2 adds 2.5 in general compared to level 1. However, we can see from the graph that IV2 does not do anything in general. It does not add 2.5s everywhere. It adds 5 in condition A, and nothing in condition B. It only does one thing in one condition.\nWhat is happening here is that a “main effect” is produced by the process of averaging over a clear interaction.\nHow would we interpret this? We might have to say there was a main effect of IV2, BUT we would definitely say it was qualified by an IV1 x IV2 interaction.\nWhat’s the qualification? The size of the IV2 effect completely changes as a function of the levels of IV1. It was big for level A, and nonexistent for level B of IV1.\nWhat does the qualification mean for the main effect? In this case, we might doubt whether there is a main effect of IV2 at all. It could turn out that IV2 does not have a general influence over the DV all of the time, it may only do something in very specific circumstances, in combination with the presence of other factors."
  },
  {
    "objectID": "11-MixedANOVA.html#mixed-designs",
    "href": "11-MixedANOVA.html#mixed-designs",
    "title": "11  More On Factorial Designs",
    "section": "11.3 Mixed Designs",
    "text": "11.3 Mixed Designs\nIn this book, we’ve explored various research designs, emphasizing that they can take different forms. These designs can be categorized as either between-subjects, where different subjects are in each group, or within-subjects, where the same subjects participate in all conditions. When you combine these approaches in a single study, you create what’s known as a mixed design.\nA mixed design occurs when one of your independent variables (IVs) is treated as a between-subjects factor, while another is treated as a within-subjects factor. This blend offers a unique approach to examining how different variables interact and affect the outcome.\nIn environmental science research, mixed designs are particularly useful for studying complex interactions between variables that vary both within and between subjects. For instance, consider a study examining the impact of a new agricultural technique (IV1) on crop yield (DV). This technique could be applied to different plots of land (between-subjects factor), while also measuring the impact across different seasons (within-subjects factor). Such a design allows researchers to understand not only the overall effectiveness of the technique but also how its impact varies seasonally.\nThe key to successfully navigating mixed designs lies in understanding how to calculate the appropriate statistical measures. Specifically, the F-values for each effect in an ANOVA (Analysis of Variance) are constructed using different error terms, depending on whether the IV is a within-subjects or between-subjects factor. While it’s possible to run an ANOVA with any combination of between and within-subjects IVs, the complexity increases with the number of variables and their categorizations.\nAs this is an introductory text, we won’t delve into the detailed formulas for constructing ANOVA tables with mixed designs. More advanced textbooks offer comprehensive discussions on this topic, and many resources are available online for those interested in deeper exploration."
  },
  {
    "objectID": "11-MixedANOVA.html#more-complicated-designs",
    "href": "11-MixedANOVA.html#more-complicated-designs",
    "title": "11  More On Factorial Designs",
    "section": "11.4 More complicated designs",
    "text": "11.4 More complicated designs\nUp until now we have focused on the simplest case for factorial designs, the 2x2 design, with two IVs, each with 2 levels. It is worth spending some time looking at a few more complicated designs and how to interpret them.\n\n11.4.1 3x2 design\nIn a 3x2 design there are two IVs. IV1 has three levels, and IV2 has two levels. Typically, there would be one DV. Let’s apply this to an environmental science scenario.First, let’s make the design concrete.\nImagine a study examining the impact of different irrigation methods (IV1: drip irrigation vs. sprinkler irrigation) on crop yield (DV) across three types of soil (IV2: sandy, loamy, clayey). The main effects would be the overall impact of irrigation method and soil type on crop yield, while the interaction would explore how these effects vary together.\nFor instance, drip irrigation might consistently produce higher yields than sprinkler irrigation, showing a main effect of IV1. Soil type might also independently affect yield, with loamy soil perhaps leading to the highest yields, followed by clayey and sandy soils, indicating a main effect of IV2. An interaction would occur if, for example, the advantage of drip irrigation over sprinkler irrigation is more pronounced in sandy soil compared to clayey soil. Note that these examples are hypothetical to illustrate the concept.\nThe factorial ANOVA will test:\n\nwhether there are any differences in crop yield among the three levels of soil type\nwhether there are any differences in crop yield among the two levels of irrigation\nwhether there is any interaction between irrigation type and soil type\n\nYou have three null hypotheses:\n\nThere is no difference between the means for each level of soil type:\n\nH0: \\(\\mu_{Clay} = \\mu_{Loam} = \\mu_{Sand}\\)\n\n\nThere is no difference between the means for each level of irrigation:\n\nH0: \\(\\mu_{Drip} = \\mu_{Sprinkler}\\)\n\n\nThere is no interaction between the factors.\n\nRemember, this is far better than running two separate single factor ANOVAs that contrast irrigation effects for each level of soil type because you have more statistical power (higher degrees of freedom) for the tests of interest, and you get a formal test of the interaction between factors which is often scientifically interesting.\nWe might expect data like shown in Figure 11.5:\n\n\n\n\n\nFigure 11.5: Example means for a 3x2 factorial design in environmental science\n\n\n\n\nThe figure shows some pretend means in all conditions. Let’s talk about the main effects and interaction.\n\nMain Effect of Irrigation Method: The main effect of the irrigation method is evident. Drip irrigation (represented by red line) generally leads to higher crop yields compared to sprinkler irrigation (represented by aqua line).\nMain Effect of Soil Type: The main effect of soil type is clearly present. Clayey soils show the highest yield, followed by loamy soils, then sandy soils\nInteraction Between Irrigation Method and Soil Type: Is there an interaction? Yes, there is. Remember, an interaction occurs when the effect of one IV depends on the levels of an another. The advantage of drip irrigation over sprinkler irrigation is more pronounced in sandy soil compared to clayey soil. So, the size of the irrigation effect (drip vs. sprinkler) changes with the type of soil. There is evidence in the means for an interaction. You would have to conduct an inferential test on the interaction term to see if these differences were likely or unlikely to be due to sampling error.\n\nIf there was no interaction and no main effect of soil type, we would see something like the pattern in Figure 11.6.\n\n\n\n\n\nFigure 11.6: Example means for a 3x2 design in environmental science with only one main effect\n\n\n\n\nWhat would you say about the interaction if you saw the pattern in Figure 11.7?\n\n\n\n\n\nFigure 11.7: Example means for a 3x2 design in environmental science showing a different interaction pattern\n\n\n\n\nThe correct answer is that there is evidence in the means for an interaction. Remember, we are measuring the irrigation effect (effect of drip vs. sprinkler) three times. The irrigation effect is the same for clayey and loamy soils, but it is much smaller for sandy soils. The size of the irrigation effect depends on the levels of the soil type IV, so here again there is an interaction.\n\n\n11.4.2 2x2x2 designs\nLet’s take it up a notch and look at a 2x2x2 design. In a 2x2x2 design, there are three independent variables (IVs), each with two levels. This design allows for the examination of three main effects, three two-way interactions, and one three-way interaction.\nWe’ll add another independent variable to our example from before: crop type (wheat vs. corn) as our third IV. So overall, in this 2x2x2 design, we’ll consider three independent variables (IVs): irrigation method (IV1: drip vs. sprinkler), soil type (IV2: sandy vs. clayey), and crop type (IV3: wheat vs. corn). The dependent variable (DV) is still crop yield. This design helps us understand not just individual effects but also how these factors interact in various combinations.\n\n\n\n\n\nFigure 11.8: Example means from a 2x2x2 design in environmental science with no three-way interaction.\n\n\n\n\nn Figure 11.8, we have two panels: one for corn and one for wheat. You can think of the 2x2x2 as two 2x2 designs, one for each crop type. The key takeaway? Both wheat and corn show similar patterns, indicating a 2x2 interaction between irrigation method and soil type. We observe main effects for irrigation and soil type, but no main effect for crop type, and importantly, no three-way interaction.\nBut what exactly is a three-way interaction? It occurs when the pattern of a 2x2 interaction differs across the levels of the third variable. Let’s visualize this with Figure 11.9.\n\n\n\n\n\nFigure 11.9: Example means from a 2x2x2 design in environmental science with a three-way interaction.\n\n\n\n\nWe are looking at a 3-way interaction between irrigation type, crop type, and soil type. What is going on here?\nFor corn crop yields, we see that there is a smaller irrigation effect in clayey soils, but the effect of irrigation gets bigger in sandy soils. A pattern like this might make sense, sandy soils don’t retain much water so the irrigation method might matter more.\nThe wheat crop yields show a different pattern. Here, the irrigation effect is large in clayey soils and smaller in sandy soils. This difference in patterns between corn and wheat yields indicates a three-way interaction among irrigation type, soil type, and crop type.\nIn other words, the 2x2 interaction for the corn is different from the 2x2 interaction for the wheat. This can be conceptualized as an interaction between the two interactions, and as a result there is a three-way interaction, called a 2x2x2 interaction.\nA general pattern here. Imagine you had a 2x2x2x2 design. That would have a 4-way interaction. What would that mean? It would mean that the pattern of the 2x2x2 interaction changes across the levels of the 4th IV. If two three-way interactions are different, then there is a four-way interaction.This becomes very complicated very quickly, another reminder of why simplicity in design is desirable.\n\n\n11.4.3 Understanding and Interpreting Interactions in Environmental Science\nSo, you’ve got a handle on what interactions are and what they might look like. But there’s still a key question hanging in the air: Why do interactions matter?\n\n11.4.3.1 Interpreting the Results\nRemember our example exploring the impact of different irrigation methods on crop yield across various soil types? The data in Figure 11.5 revealed something interesting: drip irrigation significantly boosts crop yield in sandy soil, but this effect diminishes in loamy and clayey soils. This is what we call an interaction effect: the impact of the irrigation method (drip vs. sprinkler) on crop yield varies depending on the soil type.\nThis interaction is important. It tells us that the effectiveness of an irrigation method is not uniform across all soil types. It suggests that environmental factors (like soil type) can influence how well an intervention (like irrigation method) works.\n\n\n11.4.3.2 Practical Implications in Environmental Science\nUnderstanding these interactions has real-world implications for environmental management and policy-making. Consider these examples:\n\nResource Allocation: By understanding how different soil types interact with various irrigation methods, farmers can tailor their agricultural practices more precisely. For instance, in areas with clayey soil, which retains water well, less frequent irrigation might be more suitable, reducing water usage and preserving natural resources.\nClimate Adaptation Strategies: Understanding these interactions could play a role in developing climate adaptation strategies. For regions facing increased rainfall variability due to climate change, selecting the right combination of soil management and irrigation techniques can help in maintaining crop yields despite changing weather patterns.\nPolicy Formulation: Insights from these interactions can guide the creation of more nuanced agricultural policies. For example, providing subsidies or incentives for adopting certain irrigation methods in specific soil types could optimize crop yield and promote sustainability.\n\nThink about it: are there other environmental factors where understanding interactions could be crucial for effective management and policy-making?\n\n\n\n\nBarnes, Mallory L. 2023. Statistics for Environmental Science."
  },
  {
    "objectID": "12-Simulation.html#reasons-to-simulate",
    "href": "12-Simulation.html#reasons-to-simulate",
    "title": "12  Simulating Data",
    "section": "12.1 Reasons to simulate",
    "text": "12.1 Reasons to simulate\nThere are many good reasons to learn simulation techniques, here are some:\n\nYou force yourself to consider the details of your design, how many subjects, how many conditions, how many observations per condition per subject, and how you will store and represent the data to describe all of these details when you run the experiment\nYou force yourself to consider the kinds of numbers you will be collecting. Specifically, the distributional properties of those numbers. You will have to make decisions about the distributions that you sample from in your simulation, and thinking about this issue helps you better understand your own data when you get it.\nYou learn a bit of computer programming, and this is a very useful general skill that you can build upon to do many things.\nYou can make reasonable and informed assumptions about how your experiment might turn out, and then use the results of your simulation to choose parameters for your design (such as number of subjects, number of observations per condition and subject) that will improve the sensitivity of your design to detect the effects you are interested in measuring.\nYou can even run simulations on the data that you collect to learn more about how it behaves, and to do other kinds of advanced statistics that we don’t discuss in this book.\nYou get to improve your intuitions about how data behaves when you measure it. You can test your intuitions by running simulations, and you can learn things you didn’t know to begin with. Simulations can be highly informative.\nWhen you simulate data in advance of collecting real data, you can work out exactly what kinds of tests you are planning to perform, and you will have already written your analysis code, so it will be ready and waiting for you as soon as you collect the data\n\nOK, so that’s just a few reasons why simulations are useful."
  },
  {
    "objectID": "12-Simulation.html#simulation-overview",
    "href": "12-Simulation.html#simulation-overview",
    "title": "12  Simulating Data",
    "section": "12.2 Simulation overview",
    "text": "12.2 Simulation overview\nThe basic idea here is actually pretty simple. You make some assumptions about how many subjects will be in your design (set N), you make some assumptions about the distributions that you will be sampling your scores from, then you use R to fabricate fake data according to the parameters you set. Once you build some simulated data, you can conduct a statistical analysis that you would be planning to run on the real data. Then you can see what happens. More importantly, you can repeat the above process many times. This is similar to conducting a replication of your experiment to see if you find the same thing, only you make the computer replicate your simulation 1000s of times. This way you can see how your simulated experiment would turn out over the long run. For example, you might find that the experiment you are planning to run will only produce a “signficant” result 25% of the time, that’s not very good. Your simulation might also tell you that if you increase your N by say 25, that could really help, and your new experiment with N=25 might succeed 90% of the time. That’s information worth knowing.\nBefore we go into more simulation details, let’s just run a quick one. We’ll do an independent samples \\(t\\)-test. Imagine we have a study with N=10 in each group. There are two groups. We are measuring heart rate. Let’s say we know that heart rate is on average 100 beats per minute with a standard deviation of 7. We are going to measure heart rate in condition A where nothing happens, and we are going to measure heart rate in condition B while they watch a scary movie. We think the scary movie might increase heart rate by 5 beats per minute. Let’s run a simulation of this:\n\ngroup_A &lt;- rnorm(10,100,7)\ngroup_B &lt;- rnorm(10,105, 7)\nt.test(group_A,group_B,var.equal = TRUE)\n#&gt; \n#&gt;  Two Sample t-test\n#&gt; \n#&gt; data:  group_A and group_B\n#&gt; t = -0.70226, df = 18, p-value = 0.4915\n#&gt; alternative hypothesis: true difference in means is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -7.224651  3.604792\n#&gt; sample estimates:\n#&gt; mean of x mean of y \n#&gt;  102.1628  103.9727\n\nWe sampled 10 scores from a normal distribution for each group. We changed the mean for group_b to 105, because we were thinking their heart rate would be 5 more than group A. We ran one \\(t\\)-test, and we got a result. This result tells us what happens for this one simulation.\nWe could learn more by repeating the simulation 1000 times, saving the \\(p\\)-values from each replication, and then finding out how many of our 1000 simulated experiments give us a significant result:\n\nsave_ps&lt;-length(1000)\nfor(i in 1:1000){\n  group_A &lt;- rnorm(10,100,7)\n  group_B &lt;- rnorm(10,105, 7)\n  t_results &lt;- t.test(group_A,group_B,var.equal = TRUE)\n  save_ps[i] &lt;- t_results$p.value\n}\n\nprop_p&lt;-length(save_ps[save_ps&lt;0.05])/1000\nprint(prop_p)\n#&gt; [1] 0.318\n\nNow this is more interesting. We found that 31.8% of simulated experiments had a \\(p\\)-value less than 0.05. That’s not very good. If you were going to collect data in this kind of experiment, and you made the correct assumptions about the mean and standard deviation of the distribution, and you made the correct assumption about the size of difference between the groups, you would be planning to run an experiment that would not work-out most of the time.\nWhat happens if we increase the number of subject to 50 in each group?\n\nsave_ps&lt;-length(1000)\nfor(i in 1:1000){\n  group_A &lt;- rnorm(50,100,7)\n  group_B &lt;- rnorm(50,105, 7)\n  t_results &lt;- t.test(group_A,group_B,var.equal = TRUE)\n  save_ps[i] &lt;- t_results$p.value\n}\n\nprop_p&lt;-length(save_ps[save_ps&lt;0.05])/1000\nprint(prop_p)\n#&gt; [1] 0.945\n\nOoh, look, almost all of the experiments are significant now. So, it would be better to use 50 subjects per group than 10 per group according to this simulation.\nOf course, you might already be wondering so many different kinds of things. How can we plausibly know the parameters for the distribution we are sampling from? Isn’t this all just guess work? We’ll discuss some of these issues as we move forward in this chapter.\n\n\n\n\n  \n      \n         11  More On Factorial Designs\n                \n  \n  \n      \n        13  Thinking about answering questions with data"
  },
  {
    "objectID": "13-Thinking.html#effect-size-and-power",
    "href": "13-Thinking.html#effect-size-and-power",
    "title": "13  Thinking about answering questions with data",
    "section": "13.1 Effect-size and power",
    "text": "13.1 Effect-size and power\nIf you already know something about statistics while you were reading this book, you might have noticed that we neglected to discuss the topic of effect-size, and we barely talked about statistical power. We will talk a little bit about these things here.\nFirst, it is worth pointing out that over the years, at least in Psychology, many societies and journals have made recommendations about how researchers should report their statistical analyses. Among the recommendations is that measures of “effect size” should be reported. Similarly, many journals now require that researchers report an “a priori” power-analysis (the recommendation is this should be done before the data is collected). Because these recommendations are so prevalent, it is worth discussing what these ideas refer to. At the same time, the meaning of effect-size and power somewhat depend on your “philosophical” bent, and these two ideas can become completely meaningless depending on how you think of statistics. For these complicating reasons we have suspended our discussion of the topic until now.\nThe question or practice of using measures of effect size and conducting power-analyses are also good examples of the more general need to think about about what you are doing. If you are going to report effect size, and conduct power analyses, these activities should not be done blindly because someone else recommends that you do them, these activities and other suitable ones should be done as a part of justifying what you are doing. It is a part of thinking about how to make your data answer questions for you.\n\n13.1.1 Chance vs. real effects\nLet’s rehash something we’ve said over and over again. First, researchers are interested in whether their manipulation causes a change in their measurement. If it does, they can become confident that they have uncovered a causal force (the manipulation). However, we know that differences in the measure between experimental conditions can arise by chance alone, just by sampling error. In fact, we can create pictures that show us the window of chance for a given statistic, these tells us roughly the range and likelihoods of getting various differences just by chance. With these windows in hand, we can then determine whether the differences we found in some data that we collected were likely or unlikely to be due to chance. We also learned that sample-size plays a big role in the shape of the chance window. Small samples give chance a large opportunity make big differences. Large samples give chance a small opportunity to make big differences. The general lesson up to this point has been, design an experiment with a large enough sample to detect the effect of interest. If your design isn’t well formed, you could easily be measuring noise, and your differences could be caused by sampling error. Generally speaking, this is still a very good lesson: better designs produce better data; and you can’t fix a broken design with statistics.\nThere is clearly another thing that can determine whether or not your differences are due to chance. That is the effect itself. If the manipulation does cause a change, then there is an effect, and that effect is a real one. Effects refer to differences in the measurement between experimental conditions. The thing about effects is that they can be big or small, they have a size.\nFor example, you can think of a manipulation in terms of the size of its hammer. A strong manipulation is like a jack-hammer: it is loud, it produces a big effect, it creates huge differences. A medium manipulation is like regular hammer: it works, you can hear it, it drives a nail into wood, but it doesn’t destroy concrete like a jack-hammer, it produces a reliable effect. A small manipulation is like tapping something with a pencil: it does something, you can barely hear it, and only in a quiet room, it doesn’t do a good job of driving a nail into wood, and it does nothing to concrete, it produces tiny, unreliable effects. Finally, a really small effect would be hammering something with a feather, it leaves almost no mark and does nothing that is obviously perceptiple to nails or pavement. The lesson is, if you want to break up concrete, use a jack-hammer; or, if you want to measure your effect, make your manipulation stronger (like a jack-hammer) so it produces a bigger difference.\n\n\n13.1.2 Effect size: concrete vs. abstract notions\nGenerally speaking, the big concept of effect size, is simply how big the differences are, that’s it. However, the biggness or smallness of effects quickly becomes a little bit complicated. On the one hand, the raw difference in the means can be very meaningful. Let’s saw we are measuring performance on a final exam, and we are testing whether or not a miracle drug can make you do better on the test. Let’s say taking the drug makes you do 5% better on the test, compared to not taking the drug. You know what 5% means, that’s basically a whole letter grade. Pretty good. An effect-size of 25% would be even better right! Lot’s of measures have a concrete quality to them, and we often want to the size of the effect expressed in terms of the original measure.\nLet’s talk about concrete measures some more. How about learning a musical instrument. Let’s say it takes 10,000 hours to become an expert piano, violin, or guitar player. And, let’s say you found something online that says that using their method, you will learn the instrument in less time than normal. That is a claim about the effect size of their method. You would want to know how big the effect is right? For example, the effect-size could be 10 hours. That would mean it would take you 9,980 hours to become an expert (that’s a whole 10 hours less). If I knew the effect-size was so tiny, I wouldn’t bother with their new method. But, if the effect size was say 1,000 hours, that’s a pretty big deal, that’s 10% less (still doesn’t seem like much, but saving 1,000 hours seems like a lot).\nJust as often as we have concrete measures that are readily interpretable, Psychology often produces measures that are extremely difficult to interpret. For example, questionnaire measures often have no concrete meaning, and only an abstract statistical meaning. If you wanted to know whether a manipulation caused people to more or less happy, and you used to questionnaire to measure happiness, you might find that people were 50 happy in condition 1, and 60 happy in condition 2, that’s a difference of 10 happy units. But how much is 10? Is that a big or small difference? It’s not immediately obvious. What is the solution here? A common solution is to provide a standardized measure of the difference, like a z-score. For example, if a difference of 10 reflected a shift of one standard deviation that would be useful to know, and that would be a sizeable shift. If the difference was only a .1 shift in terms of standard deviation, then the difference of 10 wouldn’t be very large. We elaborate on this idea next in describing cohen’s d.\n\n\n13.1.3 Cohen’s d\nLet’s look a few distributions to firm up some ideas about effect-size. Figure 13.1 has four panels. The first panel (0) represents the null distribution of no differences. This is the idea that your manipulation (A vs. B) doesn’t do anything at all, as a result when you measure scores in conditions A and B, you are effectively sampling scores from the very same overall distribution. The panel shows the distribution as green for condition B, but the red one for condition A is identical and drawn underneath (it’s invisible). There is 0 difference between these distributions, so it represent a null effect.\n\n\n\n\n\nFigure 13.1: Each panel shows hypothetical distributions for two conditions. As the effect-size increases, the difference between the distributions become larger.\n\n\n\n\nThe remaining panels are hypothetical examples of what a true effect could look like, when your manipulation actually causes a difference. For example, if condition A is a control group, and condition B is a treatment group, we are looking at three cases where the treatment manipulation causes a positive shift in the mean of distribution. We are using normal curves with mean =0 and sd =1 for this demonstration, so a shift of .5 is a shift of half of a standard deviation. A shift of 1 is a shift of 1 standard deviation, and a shift of 2 is a shift of 2 standard deviations. We could draw many more examples showing even bigger shifts, or shifts that go in the other direction.\nLet’s look at another example, but this time we’ll use some concrete measurements. Let’s say we are looking at final exam performance, so our numbers are grade percentages. Let’s also say that we know the mean on the test is 65%, with a standard deviation of 5%. Group A could be a control that just takes the test, Group B could receive some “educational” manipulation designed to improve the test score. These graphs then show us some hypotheses about what the manipulation may or may not be doing.\n\n\n\n\n\nFigure 13.2: Each panel shows hypothetical distributions for two conditions. As the effect-size increases, the difference between the distributions become larger.\n\n\n\n\nThe first panel shows that both condition A and B will sample test scores from the same distribution (mean =65, with 0 effect). The other panels show shifted mean for condition B (the treatment that is supposed to increase test performance). So, the treatment could increase the test performance by 2.5% (mean 67.5, .5 sd shift), or by 5% (mean 70, 1 sd shift), or by 10% (mean 75%, 2 sd shift), or by any other amount. In terms of our previous metaphor, a shift of 2 standard deviations is more like jack-hammer in terms of size, and a shift of .5 standard deviations is more like using a pencil. The thing about research, is we often have no clue about whether our manipulation will produce a big or small effect, that’s why we are conducting the research.\nYou might have noticed that the letter \\(d\\) appears in the above figure. Why is that? Jacob Cohen (cohen1988?) used the letter \\(d\\) in defining the effect-size for this situation, and now everyone calls it Cohen’s \\(d\\). The formula for Cohen’s \\(d\\) is:\n\\(d = \\frac{\\text{mean for condition 1} - \\text{mean for condition 2}}{\\text{population standard deviation}}\\)\nIf you notice, this is just a kind of z-score. It is a way to standardize the mean difference in terms of the population standard deviation.\nIt is also worth noting again that this measure of effect-size is entirely hypothetical for most purposes. In general, researchers do not know the population standard deviation, they can only guess at it, or estimate it from the sample. The same goes for means, in the formula these are hypothetical mean differences in two population distributions. In practice, researchers do not know these values, they guess at them from their samples.\nBefore discussing why the concept of effect-size can be useful, we note that Cohen’s \\(d\\) is useful for understanding abstract measures. For example, when you don’t know what a difference of 10 or 20 means as a raw score, you can standardize the difference by the sample standard deviation, then you know roughly how big the effect is in terms of standard units. If you thought a 20 was big, but it turned out to be only 1/10th of a standard deviation, then you would know the effect is actually quite small with respect to the overall variability in the data."
  },
  {
    "objectID": "13-Thinking.html#power",
    "href": "13-Thinking.html#power",
    "title": "13  Thinking about answering questions with data",
    "section": "13.2 Power",
    "text": "13.2 Power\nWhen there is a true effect out there to measure, you want to make sure your design is sensitive enough to detect the effect, otherwise what’s the point. We’ve already talked about the idea that an effect can have different sizes. The next idea is that your design can be more less sensitive in its ability to reliabily measure the effect. We have discussed this general idea many times already in the textbook, for example we know that we will be more likely to detect “significant” effects (when there are real differences) when we increase our sample-size. Here, we will talk about the idea of design sensitivity in terms of the concept of power. Interestingly, the concept of power is a somewhat limited concept, in that it only exists as a concept within some philosophies of statistics.\n\n13.2.1 A digresssion about hypothesis testing\nIn particular, the concept of power falls out of the Neyman-Pearson concept of null vs. alternative hypothesis testing. Up to this point, we have largely avoided this terminology. This is perhaps a disservice in that the Neyman-Pearson ideas are by now the most common and widespread, and in the opinion of some of us, they are also the most widely misunderstood and abused idea, which is why we have avoided these ideas until now.\nWhat we have been mainly doing is talking about hypothesis testing from the Fisherian (Sir Ronald Fisher, the ANOVA guy) perspective. This is a basic perspective that we think can’t be easily ignored. It is also quite limited. The basic idea is this:\n\nWe know that chance can cause some differences when we measure something between experimental conditions.\nWe want to rule out the possibility that the difference that we observed can not be due to chance\nWe construct large N designs that permit us to do this when a real effect is observed, such that we can confidently say that big differences that we find are so big (well outside the chance window) that it is highly implausible that chance alone could have produced.\nThe final conclusion is that chance was extremely unlikely to have produced the differences. We then infer that something else, like the manipulation, must have caused the difference.\nWe don’t say anything else about the something else.\nWe either reject the null distribution as an explanation (that chance couldn’t have done it), or retain the null (admit that chance could have done it, and if it did we couldn’t tell the difference between what we found and what chance could do)\n\nNeyman and Pearson introduced one more idea to this mix, the idea of an alternative hypothesis. The alternative hypothesis is the idea that if there is a true effect, then the data sampled into each condition of the experiment must have come from two different distributions. Remember, when there is no effect we assume all of the data cam from the same distribution (which by definition can’t produce true differences in the long run, because all of the numbers are coming from the same distribution). The graphs of effect-sizes from before show examples of these alternative distributions, with samples for condition A coming from one distribution, and samples from condition B coming from a shifted distribution with a different mean.\nSo, under the Neyman-Pearson tradition, when a researcher find a signifcant effect they do more than one things. First, they reject the null-hypothesis of no differences, and they accept the alternative hypothesis that there was differences. This seems like a sensible thing to do. And, because the researcher is actually interested in the properties of the real effect, they might be interested in learning more about the actual alternative hypothesis, that is they might want to know if their data come from two different distributions that were separated by some amount…in other words, they would want to know the size of the effect that they were measuring.\n\n\n13.2.2 Back to power\nWe have now discussed enough ideas to formalize the concept of statistical power. For this concept to exist we need to do a couple things.\n\nAgree to set an alpha criterion. When the p-value for our test-statistic is below this value we will call our finding statistically significant, and agree to reject the null hypothesis and accept the “alternative” hypothesis (sidenote, usually it isn’t very clear which specific alternative hypothesis was accepted)\nIn advance of conducting the study, figure out what kinds of effect-sizes our design is capable of detecting with particular probabilites.\n\nThe power of a study is determined by the relationship between\n\nThe sample-size of the study\nThe effect-size of the manipulation\nThe alpha value set by the researcher.\n\nTo see this in practice let’s do a simulation. We will do a t-test on a between-groups design 10 subjects in each group. Group A will be a control group with scores sampled from a normal distribution with mean of 10, and standard deviation of 5. Group B will be a treatment group, we will say the treatment has an effect-size of Cohen’s \\(d\\) = .5, that’s a standard deviation shift of .5, so the scores with come from a normal distribution with mean =12.5 and standard deivation of 5. Remember 1 standard deviation here is 5, so half of a standard deviation is 2.5.\nThe following R script runs this simulated experiment 1000 times. We set the alpha criterion to .05, this means we will reject the null whenever the \\(p\\)-value is less than .05. With this specific design, how many times out of of 1000 do we reject the null, and accept the alternative hypothesis?\n\n#&gt; [1] 180\n\nThe answer is that we reject the null, and accept the alternative 180 times out of 1000. In other words our experiment succesfully accepts the alternative hypothesis 18 percent of the time, this is known as the power of the study. Power is the probability that a design will succesfully detect an effect of a specific size.\nImportantly, power is completely abstract idea that is completely determined by many assumptions including N, effect-size, and alpha. As a result, it is best not to think of power as a single number, but instead as a family of numbers.\nFor example, power is different when we change N. If we increase N, our samples will more precisely estimate the true distributions that they came from. Increasing N reduces sampling error, and shrinks the range of differences that can be produced by chance. Lets’ increase our N in this simulation from 10 to 20 in each group and see what happens.\n\n#&gt; [1] 335\n\nNow the number of significant experiments i 335 out of 1000, or a power of 33.5 percent. That’s roughly doubled from before. We have made the design more sensitive to the effect by increasing N.\nWe can change the power of the design by changing the alpha-value, which tells us how much evidence we need to reject the null. For example, if we set the alpha criterion to 0.01, then we will be more conservative, only rejecting the null when chance can produce the observed difference 1% of the time. In our example, this will have the effect of reducing power. Let’s keep N at 20, but reduce the alpha to 0.01 and see what happens:\n\n#&gt; [1] 150\n\nNow only 150 out of 1000 experiments are significant, that’s 15 power.\nFinally, the power of the design depends on the actual size of the effect caused by the manipulation. In our example, we hypothesized that the effect caused a shift of .5 standard deviations. What if the effect causes a bigger shift? Say, a shift of 2 standard deviations. Let’s keep N= 20, and alpha &lt; .01, but change the effect-size to two standard deviations. When the effect in the real-world is bigger, it should be easier to measure, so our power will increase.\n\n#&gt; [1] 1000\n\nNeat, if the effect-size is actually huge (2 standard deviation shift), then we have power 100 percent to detect the true effect.\n\n\n13.2.3 Power curves\nWe mentioned that it is best to think of power as a family of numbers, rather than as a single number. To elaborate on this consider the power curve below. This is the power curve for a specific design: a between groups experiments with two levels, that uses an independent samples t-test to test whether an observed difference is due to chance. Critically, N is set to 10 in each group, and alpha is set to .05\nIn Figure 13.3 power (as a proportion, not a percentage) is plotted on the y-axis, and effect-size (Cohen’s d) in standard deviation units is plotted on the x-axis.\n\n\n\n\n\nFigure 13.3: This figure shows power as a function of effect-size (Cohen’s d) for a between-subjects independent samples t-test, with N=10, and alpha criterion 0.05.\n\n\n\n\nA power curve like this one is very helpful to understand the sensitivity of a particular design. For example, we can see that a between subjects design with N=10 in both groups, will detect an effect of d=.5 (half a standard deviation shift) about 20% of the time, will detect an effect of d=.8 about 50% of the time, and will detect an effect of d=2 about 100% of the time. All of the percentages reflect the power of the design, which is the percentage of times the design would be expected to find a \\(p\\) &lt; 0.05.\nLet’s imagine that based on prior research, the effect you are interested in measuring is fairly small, d=0.2. If you want to run an experiment that will detect an effect of this size a large percentage of the time, how many subjects do you need to have in each group? We know from the above graph that with N=10, power is very low to detect an effect of d=0.2. Let’s make Figure 13.4 and vary the number of subjects rather than the size of the effect.\n\n\n\n\n\nFigure 13.4: This figure shows power as a function of N for a between-subjects independent samples t-test, with d=0.2, and alpha criterion 0.05.\n\n\n\n\nThe figure plots power to detect an effect of d=0.2, as a function of N. The green line shows where power = .8, or 80%. It looks like we would nee about 380 subjects in each group to measure an effect of d=0.2, with power = .8. This means that 80% of our experiments would succesfully show p &lt; 0.05. Often times power of 80% is recommended as a reasonable level of power, however even when your design has power = 80%, your experiment will still fail to find an effect (associated with that level of power) 20% of the time!"
  },
  {
    "objectID": "13-Thinking.html#planning-your-design",
    "href": "13-Thinking.html#planning-your-design",
    "title": "13  Thinking about answering questions with data",
    "section": "13.3 Planning your design",
    "text": "13.3 Planning your design\nOur discussion of effect size and power highlight the importance of the understanding the statistical limitations of an experimental design. In particular, we have seen the relationship between:\n\nSample-size\nEffect-size\nAlpha criterion\nPower\n\nAs a general rule of thumb, small N designs can only reliably detect very large effects, whereas large N designs can reliably detect much smaller effects. As a researcher, it is your responsibility to plan your design accordingly so that it is capable of reliably detecting the kinds of effects it is intended to measure."
  },
  {
    "objectID": "13-Thinking.html#some-considerations",
    "href": "13-Thinking.html#some-considerations",
    "title": "13  Thinking about answering questions with data",
    "section": "13.4 Some considerations",
    "text": "13.4 Some considerations\n\n13.4.1 Low powered studies\nConsider the following case. A researcher runs a study to detect an effect of interest. There is good reason, from prior research, to believe the effect-size is d=0.5. The researcher uses a design that has 30% power to detect the effect. They run the experiment and find a significant p-value, (p&lt;.05). They conclude their manipulation worked, because it was unlikely that their result could have been caused by chance. How would you interpret the results of a study like this? Would you agree with thte researchers that the manipulation likely caused the difference? Would you be skeptical of the result?\nThe situation above requires thinking about two kinds of probabilities. On the one hand we know that the result observed by the researchers does not occur often by chance (p is less than 0.05). At the same time, we know that the design was underpowered, it only detects results of the expected size 30% of the time. We are face with wondering what kind of luck was driving the difference. The researchers could have gotten unlucky, and the difference really could be due to chance. In this case, they would be making a type I error (saying the result is real when it isn’t). If the result was not due to chance, then they would also be lucky, as their design only detects this effect 30% of the time.\nPerhaps another way to look at this situation is in terms of the replicability of the result. Replicability refers to whether or not the findings of the study would be the same if the experiment was repeated. Because we know that power is low here (only 30%), we would expect that most replications of this experiment would not find a significant effect. Instead, the experiment would be expected to replicate only 30% of the time.\n\n\n13.4.2 Large N and small effects\nPerhaps you have noticed that there is an intriguiing relationship between N (sample-size) and power and effect-size. As N increases, so does power to detect an effect of a particular size. Additionally, as N increases, a design is capable of detecting smaller and smaller effects with greater and greater power. For example, if N was large enough, we would have high power to detect very small effects, say d= 0.01, or even d=0.001. Let’s think about what this means.\nImagine a drug company told you that they ran an experiment with 1 billion people to test whether their drug causes a significant change in headache pain. Let’s say they found a significant effect (with power =100%), but the effect was very small, it turns out the drug reduces headache pain by less than 1%, let’s say 0.01%. For our imaginary study we will also assume that this effect is very real, and not caused by chance.\nClearly the design had enough power to detect the effect, and the effect was there, so the design did detect the effect. However, the issue is that there is little practical value to this effect. Nobody is going to by a drug to reduce their headache pain by 0.01%, even if it was “scientifcally proven” to work. This example brings up two issues. First, increasing N to very large levels will allow designs to detect almost any effect (even very tiny ones) with very high power. Second, sometimes effects are meaningless when they are very small, especially in applied research such as drug studies.\nThese two issues can lead to interesting suggestions. For example, someone might claim that large N studies aren’t very useful, because they can always detect really tiny effects that are practically meaningless. On the other hand, large N studies will also detect larger effects too, and they will give a better estimate of the “true” effect in the population (because we know that larger samples do a better job of estimating population parameters). Additionally, although really small effects are often not interesting in the context of applied research, they can be very important in theoretical research. For example, one theory might predict that manipulating X should have no effect, but another theory might predict that X does have an effect, even if it is a small one. So, detecting a small effect can have theoretical implication that can help rule out false theories. Generally speaking, researchers asking both theoretical and applied questions should think about and establish guidelines for “meaningful” effect-sizes so that they can run designs of appropriate size to detect effects of “meaningful size”.\n\n\n13.4.3 Small N and Large effects\nAll other things being equal would you trust the results from a study with small N or large N? This isn’t a trick question, but sometimes people tie themselves into a knot trying to answer it. We already know that large sample-sizes provide better estimates of the distributions the samples come from. As a result, we can safely conclude that we should trust the data from large N studies more than small N studies.\nAt the same time, you might try to convince yourself otherwise. For example, you know that large N studies can detect very small effects that are practically and possibly even theoretically meaningless. You also know that that small N studies are only capable of reliably detecting very large effects. So, you might reason that a small N study is better than a large N study because if a small N study detects an effect, that effect must be big and meaningful; whereas, a large N study could easily detect an effect that is tiny and meaningless.\nThis line of thinking needs some improvement. First, just because a large N study can detect small effects, doesn’t mean that it only detects small effects. If the effect is large, a large N study will easily detect it. Large N studies have the power to detect a much wider range of effects, from small to large. Second, just because a small N study detected an effect, does not mean that the effect is real, or that the effect is large. For example, small N studies have more variability, so the estimate of the effect size will have more error. Also, there is 5% (or alpha rate) chance that the effect was spurious. Interestingly, there is a pernicious relationship between effect-size and type I error rate\n\n\n13.4.4 Type I errors are convincing when N is small\nSo what is this pernicious relationship between Type I errors and effect-size? Mainly, this relationship is pernicious for small N studies. For example, the following figure illustrates the results of 1000s of simulated experiments, all assuming the null distribution. In other words, for all of these simulations there is no true effect, as the numbers are all sampled from an identical distribution (normal distribution with mean =0, and standard deviation =1). The true effect-size is 0 in all cases.\nWe know that under the null, researchers will find p values that are less 5% about 5% of the time, remember that is the definition. So, if a researcher happened to be in this situation (where there manipulation did absolutely nothing), they would make a type I error 5% of the time, or if they conducted 100 experiments, they would expect to find a significant result for 5 of them.\nFigure 13.5 reports the findings from only the type I errors, where the simulated study did produce p &lt; 0.05. For each type I error, we calculated the exact p-value, as well as the effect-size (cohen’s D) (mean difference divided by standard deviation). We already know that the true effect-size is zero, however take a look at this graph, and pay close attention to the smaller sample-sizes.\n\n\n\n\n\nFigure 13.5: Effect size as a function of p-values for type 1 Errors under the null, for a paired samples t-test.\n\n\n\n\nFor example, look at the red dots, when sample size is 10. Here we see that the effect-sizes are quite large. When p is near 0.05 the effect-size is around .8, and it goes up and up as when p gets smaller and smaller. What does this mean? It means that when you get unlucky with a small N design, and your manipulation does not work, but you by chance find a “significant” effect, the effect-size measurement will show you a “big effect”. This is the pernicious aspect. When you make a type I error for small N, your data will make you think there is no way it could be a type I error because the effect is just so big!. Notice that when N is very large, like 1000, the measure of effect-size approaches 0 (which is the true effect-size in the simulation shown in Figure 13.6).\n\n\n\n\n\nFigure 13.6: Each panel shows a histogram of a different sampling statistic.\n\n\n\n\n\n\n\n\nBarnes, Mallory L. 2023. Statistics for Environmental Science."
  },
  {
    "objectID": "14-Gifs.html#correlation-gifs",
    "href": "14-Gifs.html#correlation-gifs",
    "title": "14  GIFs",
    "section": "14.1 Correlation GIFs",
    "text": "14.1 Correlation GIFs\nNote regression lines and confidence bands can be added using geom_smooth(method=lm, se=T)\n\n14.1.1 N=10, both variables drawn from a uniform distribution\n\n\n\n\n\n\nall_df&lt;-data.frame()\nfor(sim in 1:10){\n  North_pole &lt;- runif(10,1,10)\n  South_pole &lt;- runif(10,1,10)\n  t_df&lt;-data.frame(simulation=rep(sim,10),\n                                  North_pole,\n                                  South_pole)\n  all_df&lt;-rbind(all_df,t_df)\n}\n\n\nggplot(all_df,aes(x=North_pole,y=South_pole))+\n  geom_point()+\n  geom_smooth(method=lm, se=FALSE)+\n  theme_classic()+\n  transition_states(\n    simulation,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\n\n\n14.1.2 Correlation between random deviates from uniform distribution across four sample sizes\nN= 10,50,100,1000 All values sampled from a uniform distribution\n\n\n\n\n\n\nall_df&lt;-data.frame()\nfor(sim in 1:10){\n  for(n in c(10,50,100,1000)){\n  North_pole &lt;- runif(n,1,10)\n  South_pole &lt;- runif(n,1,10)\n  t_df&lt;-data.frame(nsize=rep(n,n),\n                   simulation=rep(sim,n),\n                                  North_pole,\n                                  South_pole)\n  all_df&lt;-rbind(all_df,t_df)\n  }\n}\n\n\nggplot(all_df,aes(x=North_pole,y=South_pole))+\n  geom_point()+\n  geom_smooth(method=lm, se=FALSE)+\n  theme_classic()+\n  facet_wrap(~nsize)+\n  transition_states(\n    simulation,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\n\n\n14.1.3 Correlation between random deviates from normal distribution across four sample sizes\nN= 10,50,100,1000 All values sampled from the same normal distribution (mean=0, sd=1)\n\n\n\n\n\n\nall_df&lt;-data.frame()\nfor(sim in 1:10){\n  for(n in c(10,50,100,1000)){\n  North_pole &lt;- rnorm(n,0,1)\n  South_pole &lt;- rnorm(n,0,1)\n  t_df&lt;-data.frame(nsize=rep(n,n),\n                   simulation=rep(sim,n),\n                                  North_pole,\n                                  South_pole)\n  all_df&lt;-rbind(all_df,t_df)\n  }\n}\n\n\nggplot(all_df,aes(x=North_pole,y=South_pole))+\n  geom_point()+\n  geom_smooth(method=lm, se=FALSE)+\n  theme_classic()+\n  facet_wrap(~nsize)+\n  transition_states(\n    simulation,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\n\n\n14.1.4 Correlation between X and Y variables that have a true correlation as a function of sample-size\n\n\n\n\n\n\nlibrary(MASS)\nr&lt;-.7\n\nproportional_permute&lt;-function(x,prop){\n  indices&lt;-seq(1:length(x))\n  s_indices&lt;-sample(indices)\n  n_shuffle&lt;-round(length(x)*prop)\n  switch&lt;-sample(indices)\n  x[s_indices[1:n_shuffle]]&lt;-x[switch[1:n_shuffle]]\n  return(x)\n}\n\nall_df&lt;-data.frame()\nfor(sim in 1:10){\n  for(samples in c(10,50,100,1000)){\n    #data &lt;- mvrnorm(n=samples, mu=c(0, 0), Sigma=matrix(c(1, r, r, 1), nrow=2), empirical=TRUE)\n    #North_pole &lt;- data[, 1]  # standard normal (mu=0, sd=1)\n    #South_pole &lt;- data[, 2] \n    \n    North_pole &lt;- runif(samples,1,10)\n    South_pole &lt;- proportional_permute(North_pole,.5)+runif(samples,-5,5)\n\n    t_df&lt;-data.frame(nsize=rep(samples,samples),\n                   simulation=rep(sim,samples),\n                                  North_pole,\n                                  South_pole)\n  all_df&lt;-rbind(all_df,t_df)\n  }\n}\n\nggplot(all_df,aes(x=North_pole,y=South_pole))+\n  geom_point()+\n  geom_smooth(method=lm, se=FALSE)+\n  theme_classic()+\n  facet_wrap(~nsize)+\n  transition_states(\n    simulation,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\n\n\n14.1.5 Type I errors, sampling random deviates from normal distribution with regression lines\nThese scatter plots only show what would be type I errors (assuming alpha=.05). The X and Y values were both sampled from the same normal distribution (mean = 0, sd=1). 1000 simulations were conducted for each sample size (10,50,100,1000). For each, the animation shows 10 scatter plots where the observed “correlation” would have passed a significance test. According to definition, these correlations only arise from random normal deviates 5% of the time, but when they do arise for small sample sizes, they look fairly convincing.\n\n\n\n\n\n\nall_df&lt;-data.frame()\nfor(n in c(10,50,100,1000)){\n  count_sims&lt;-0\n  for(sim in 1:1000){\n    North_pole &lt;- rnorm(n,0,1)\n    South_pole &lt;- rnorm(n,0,1)\n    if(cor.test(North_pole,South_pole)$p.value&lt;.05){\n      count_sims&lt;-count_sims+1\n    t_df&lt;-data.frame(nsize=rep(n,n),\n                     simulation=rep(count_sims,n),\n                     North_pole,\n                     South_pole)\n    all_df&lt;-rbind(all_df,t_df)\n    \n    if(count_sims==10){\n      break\n    }\n    }\n  }\n}\n\n\nggplot(all_df,aes(x=North_pole,y=South_pole))+\n  geom_point()+\n  geom_smooth(method=lm, se=TRUE)+\n  theme_classic()+\n  facet_wrap(~nsize)+\n  transition_states(\n    simulation,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\n\n\n14.1.6 Cell-size and correlation\nThis simulation illustrates how the behavior of correlating two random normal samples as a function of cell-size. The sample-size is always set at N=10. For each panel, the simulation uses an increasing cell-size to estimate the mean for X and Y. When cell-size is 1, 10 X and Y values are drawn from the same normal (u=0, sd=1). When cell-size is 5, for each X,Y score in the plot, 5 samples were drawn from the same normal, and then the mean of the samples is plotted. The effect of cell-size shrinks the dot cloud, as both X and Y scores provide better estimates of the population mean = 0. Cell-size has no effect on the behavior of r, which swings around because sample-size N is small. These are all random, so there is always a 5% type I error rate (alpha =.05).\n\n\n\n\n\n\nget_sampling_means&lt;-function(m,sd,cell_size,s_size){\n  save_means&lt;-length(s_size)\n  for(i in 1:s_size){\n    save_means[i]&lt;-mean(rnorm(cell_size,m,sd))\n  }\n  return(save_means)\n}\n\nall_df&lt;-data.frame()\nfor(n in c(1,5,10,100)){\n  count_sims&lt;-0\n  for(sim in 1:10){\n    North_pole &lt;- get_sampling_means(0,1,n,10)\n    South_pole &lt;- get_sampling_means(0,1,n,10)\n      count_sims&lt;-count_sims+1\n      t_df&lt;-data.frame(nsize=rep(n,10),\n                       simulation=rep(count_sims,10),\n                       North_pole,\n                       South_pole)\n      all_df&lt;-rbind(all_df,t_df)\n  }\n}\n\n\nggplot(all_df,aes(x=North_pole,y=South_pole))+\n  geom_point()+\n  geom_smooth(method=lm, se=TRUE)+\n  theme_classic()+\n  facet_wrap(~nsize)+\n  ggtitle(\"Random scatterplots, N=10, Cell-size = 1,5,10,100\")+\n  transition_states(\n    simulation,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\n\n\n14.1.7 Regression\nWe look at how the residuals (error from points to line) behave as the regression lines moves above and below it’s true value. The total error associated with all of the red lines is represents by the grey area. This total error is smallest (minimized) when the black line overlaps with the blue regression line (the best fit line). The total error expands as the black line moves away from the regression. That’s why the regression line is the least wrong (best fit) line to skewer the data (according to least squares definition)\n\n\n\n\n\n\nd &lt;- mtcars\nfit &lt;- lm(mpg ~ hp, data = d)\nd$predicted &lt;- predict(fit)   # Save the predicted values\nd$residuals &lt;- residuals(fit) # Save the residual values\n\ncoefs&lt;-coef(lm(mpg ~ hp, data = mtcars))\ncoefs[1]\ncoefs[2]\n\nx&lt;-d$hp\nmove_line&lt;-c(seq(-6,6,.5),seq(6,-6,-.5))\ntotal_error&lt;-length(length(move_line))\ncnt&lt;-0\nfor(i in move_line){\n  cnt&lt;-cnt+1\n  predicted_y &lt;- coefs[2]*x + coefs[1]+i\n  error_y &lt;- (predicted_y-d$mpg)^2\n  total_error[cnt]&lt;-sqrt(sum(error_y)/32)\n}\n\nmove_line_sims&lt;-rep(move_line,each=32)\ntotal_error_sims&lt;-rep(total_error,each=32)\nsims&lt;-rep(1:50,each=32)\n\nd&lt;-d %&gt;% slice(rep(row_number(), 50))\n\nd&lt;-cbind(d,sims,move_line_sims,total_error_sims)\n\n\nanim&lt;-ggplot(d, aes(x = hp, y = mpg, frame=sims)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"lightblue\") +  \n  geom_abline(intercept = 30.09886+move_line_sims, slope = -0.06822828)+\n  lims(x = c(0,400), y = c(-10,40))+\n  geom_segment(aes(xend = hp, yend = predicted+move_line_sims, color=\"red\"), alpha = .5) + \n  geom_point() +\n  geom_ribbon(aes(ymin = predicted+move_line_sims - total_error_sims, ymax = predicted+move_line_sims + total_error_sims), fill = \"lightgrey\", alpha=.2)+ \n  theme_classic()+\n  theme(legend.position=\"none\")+\n  xlab(\"X\")+ylab(\"Y\")+\n  transition_manual(frames=sims)+\n  enter_fade() + \n  exit_fade()+\n  ease_aes('sine-in-out')\n\nanimate(anim,fps=5)"
  },
  {
    "objectID": "14-Gifs.html#sampling-distributions",
    "href": "14-Gifs.html#sampling-distributions",
    "title": "14  GIFs",
    "section": "14.2 Sampling distributions",
    "text": "14.2 Sampling distributions\n\n14.2.1 Sampling from a uniform distribution\nAnimation shows histograms for N=20, sampled from a uniform distribution, along with mean (red line). Uniform distribution in this case is integer values from 1 to 10.\n\n\n\n\n\n\na&lt;-round(runif(20*10,1,10))\ndf&lt;-data.frame(a,sample=rep(1:10,each=20))\ndf2&lt;-aggregate(a~sample,df,mean)\ndf&lt;-cbind(df,mean_loc=rep(df2$a,each=20))\n\nlibrary(gganimate)\n\nggplot(df,aes(x=a, group=sample,frame=sample)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept=mean_loc,frame = sample),color=\"red\")+\n  scale_x_continuous(breaks=seq(1,10,1))+\n  theme_classic()+\n  transition_states(\n    sample,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\n\n\n14.2.2 Sampling from uniform with line showing expected value for each number\n\n\n\n\n\n\na&lt;-round(runif(20*10,1,10))\ndf&lt;-data.frame(a,sample=rep(1:10,each=20))\n\n\nlibrary(gganimate)\nggplot(df,aes(x=a))+\n  geom_histogram(bins=10, color=\"white\")+\n  theme_classic()+\n  scale_x_continuous(breaks=seq(1,10,1))+\n  geom_hline(yintercept=2)+\n  ggtitle(\"Small N=20 samples from a uniform distribution\")+\n  transition_states(\n    sample,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\n\n\n14.2.3 Sampling distribution of the mean, Normal population distribution and sample histograms\nThis animation illustrates the relationship between a distribution (population), samples from the distribution, and the sampling distribution of the sample means, all as a function of n\nNormal distribution in red. Individual sample histograms in grey. Vertical red line is mean of individual sample. Histograms for sampling distribution of the sample mean in blue. Vertical blue line is mean of the sampling distribution of the sample mean.\nNote: for purposes of the animation (and because it was easier to do this way), the histograms for the sampling distribution of the sample means have different sizes. When sample-size = 10, the histogram shows 10 sample means. When sample size=100, the histogram shows 100 sample means. I could have simulated many more sample means (say 10000) for each, but then the histograms for the sample means would be static.\nThe y-axis is very rough. The heights of the histograms and distributions were scaled to be in the same range for the animation.\n\n\n\n\n\n\nget_sampling_means&lt;-function(m,sd,s_size){\n  save_means&lt;-length(s_size)\n  for(i in 1:s_size){\n    save_means[i]&lt;-mean(rnorm(s_size,m,sd))\n  }\n  return(save_means)\n}\n\nall_df&lt;-data.frame()\nfor(sims in 1:10){\n  for(n in c(10,50,100,1000)){\n    sample&lt;-rnorm(n,0,1)\n    sample_means&lt;-get_sampling_means(0,1,n)\n    t_df&lt;-data.frame(sims=rep(sims,n),\n                     sample,\n                     sample_means,\n                     sample_size=rep(n,n),\n                     sample_mean=rep(mean(sample),n),\n                     sampling_mean=rep(mean(sample_means),n)\n                     )\n    all_df&lt;-rbind(all_df,t_df)\n  }\n}\n\n\nggplot(all_df, aes(x=sample))+\n  geom_histogram(aes(y=(..density..)/max(..density..)^.8),color=\"white\",fill=\"grey\")+\n  geom_histogram(aes(x=sample_means,y=(..density..)/max(..density..)),fill=\"blue\",color=\"white\",alpha=.5)+\n  stat_function(fun = dnorm, \n                args = list(mean = 0, sd = 1), \n                lwd = .75, \n                col = 'red')+\n  geom_vline(aes(xintercept=sample_mean,frame=sims),color=\"red\")+\n  geom_vline(aes(xintercept=sampling_mean,frame=sims),color=\"blue\")+\n  facet_wrap(~sample_size)+xlim(-3,3)+\n  theme_classic()+ggtitle(\"Population (red), Samples (grey), \\n and Sampling distribution of the mean (blue)\")+ylab(\"Rough likelihoods\")+\n  xlab(\"value\")+\n  transition_states(\n    sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\n\n\n14.2.4 Null and True effect samples and sampling means\nThe null dots show 50 different samples, with the red dot as the mean for each sample. Null dots are all sampled from normal (u=0, sd=1). The true dots show 50 more samples, with red dots for their means. However, the mean of the true shifts between -1.5 and +1.5 standard deviations of 0. This illustrates how a true effect moves in and out of the null range.\n\n\n\n\n\n\nall_df&lt;-data.frame()\nall_df_means&lt;-data.frame()\ndif_sim&lt;-seq(-1.5,1.5,.25)\nfor(sim in 1:13){\n  values&lt;-c(rnorm(25*25,0,1),rnorm(25*25,dif_sim[sim],1))\n  samples&lt;-c(rep(seq(1:25),each=25),rep(seq(1:25),each=25))\n  df&lt;-data.frame(samples,values,sims=rep(sim,50*25),type=rep(c(\"null\",\"true\"),each=625))\n  df_means&lt;-aggregate(values~samples*type,df,mean, sims=rep(sim,50))\n  all_df&lt;-rbind(all_df,df)\n  all_df_means&lt;-rbind(all_df_means,df_means)\n}\n\nall_df&lt;-cbind(all_df,means=rep(all_df_means$values,each=25))\n\nggplot(all_df,aes(y=values,x=samples))+\n  geom_point(aes(color=abs(values)), alpha=.25)+\n  geom_point(aes(y=means,x=samples),color=\"red\")+\n  theme_classic()+\n  geom_vline(xintercept=25.5)+\n  facet_wrap(~type)+\n  geom_hline(yintercept=0)+\n  theme(legend.position=\"none\") +\n  ggtitle(\"null=0, True effect moves from -1.5 sd to 1.5 sd\")+\n  transition_states(\n    sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')"
  },
  {
    "objectID": "14-Gifs.html#statistical-inference",
    "href": "14-Gifs.html#statistical-inference",
    "title": "14  GIFs",
    "section": "14.3 Statistical Inference",
    "text": "14.3 Statistical Inference\n\n14.3.1 Randomization Test\nThis is an attempt at visualizing a randomization test. Samples are taken under two conditions of the IV (A and B). At the beginning of the animation, the original scores in the first condition are shown as green dots on the left, and the original scores in the second condition are the red dots on the right. The means for each group are the purple dots. During the randomization, the original scores are shuffled randomly between the two conditions. After each shuffle, two new means are computed and displayed as the yellow dots. This occurs either for all permutations, or for a large random sample of them. The animation shows the original scores being shuffled around across the randomizations (the colored dots switch their original condition, appearing from side to side).\nFor intuitive inference, one might look at the range of motion of the yellow dots. This is how the mean difference between group 1 and group 2 behaves under randomization. It’s what chance can do. If the difference between the purple dots is well outside the range of motion of the yellow dots, then the mean difference observed in the beginning is not likely produced by chance.\n\n\n\n\n\n\nstudy&lt;-round(runif(10,80,100))\nno_study&lt;-round(runif(10,40,90))\n\nstudy_df&lt;-data.frame(student=seq(1:10),study,no_study)\nmean_original&lt;-data.frame(IV=c(\"studied\",\"didnt_study\"),\n                          means=c(mean(study),mean(no_study)))\nt_df&lt;-data.frame(sims=rep(1,20),\n                 IV=rep(c(\"studied\",\"didnt_study\"),each=10),\n                 values=c(study,no_study),\n                 rand_order=rep(c(0,1),each=10))\n\nraw_df&lt;-t_df\nfor(i in 2:10){\n  new_index&lt;-sample(1:20)\n  t_df$values&lt;-t_df$values[new_index]\n  t_df$rand_order&lt;-t_df$rand_order[new_index]\n  t_df$sims&lt;-rep(i,20)\n  raw_df&lt;-rbind(raw_df,t_df)\n}\n\nraw_df$rand_order&lt;-as.factor(raw_df$rand_order)\nrand_df&lt;-aggregate(values~sims*IV,raw_df,mean)\nnames(rand_df)&lt;-c(\"sims\",\"IV\",\"means\")\n\n\na&lt;-ggplot(raw_df,aes(x=IV,y=values,color=rand_order,size=3))+\n  geom_point(stat=\"identity\",alpha=.5)+\n  geom_point(data=mean_original,aes(x=IV,y=means),stat=\"identity\",shape=21,size=6,color=\"black\",fill=\"mediumorchid2\")+\n  geom_point(data=rand_df,aes(x=IV,y=means),stat=\"identity\",shape=21,size=6,color=\"black\",fill=\"gold\")+\n  theme_classic(base_size = 15)+\n  coord_cartesian(ylim=c(40, 100))+\n  theme(legend.position=\"none\") +\n  ggtitle(\"Randomization test: Original Means (purple), \n          \\n Randomized means (yellow)\n          \\n Original scores (red,greenish)\")+\n  transition_states(\n    sims,\n    transition_length = 1,\n    state_length = 2\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\nanimate(a,nframes=100,fps=5)\n\n\n\n14.3.2 Independent t-test Null\nThis is a simulation of the null distribution for an independent samples t-test, two groups, 10 observations per group.\nThis animation has two panels. The left panel shows means for group A and B, sampled from the same normal distribution (mu=50, sd =10). The dots represent individual scores for each of 10 observations per group.\nThe right panel shows a t-distribution (df=18) along with the observed t-statistic for each simulation.\ngganimate does not yet directly support multiple panels as shown in this gif. I hacked together these two gifs using the magick package. Apologies for the hackiness.\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(magick)\nlibrary(gganimate)\n\nA&lt;-rnorm(100,50,10)\nB&lt;-rnorm(100,50,10)\nDV &lt;- c(A,B)\nIV &lt;- rep(c(\"A\",\"B\"),each=100)\nsims &lt;- rep(rep(1:10,each=10),2)\ndf&lt;-data.frame(sims,IV,DV)\n\nmeans_df &lt;- df %&gt;%\n               group_by(sims,IV) %&gt;%\n               summarize(means=mean(DV),\n                         sem = sd(DV)/sqrt(length(DV)))\n\nstats_df &lt;- df %&gt;%\n              group_by(sims) %&gt;%\n              summarize(ts = t.test(DV~IV,var.equal=TRUE)$statistic)\n\na&lt;-ggplot(means_df, aes(x=IV,y=means, fill=IV))+\n  geom_bar(stat=\"identity\")+\n  geom_point(data=df,aes(x=IV, y=DV), alpha=.25)+\n  geom_errorbar(aes(ymin=means-sem, ymax=means+sem),width=.2)+\n  theme_classic()+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n  \na_gif&lt;-animate(a, width = 240, height = 240)\n\nb&lt;-ggplot(stats_df,aes(x=ts))+\n  geom_vline(aes(xintercept=ts, frame=sims))+\n  geom_line(data=data.frame(x=seq(-5,5,.1),\n                            y=dt(seq(-5,5,.1),df=18)),\n            aes(x=x,y=y))+\n  theme_classic()+\n  ylab(\"density\")+\n  xlab(\"t value\")+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\nb_gif&lt;-animate(b, width = 240, height = 240)\n\n\nd&lt;-image_blank(240*2,240)\n\nthe_frame&lt;-d\nfor(i in 2:100){\n  the_frame&lt;-c(the_frame,d)\n}\n\na_mgif&lt;-image_read(a_gif)\nb_mgif&lt;-image_read(b_gif)\n\nnew_gif&lt;-image_append(c(a_mgif[1], b_mgif[1]))\nfor(i in 2:100){\n  combined &lt;- image_append(c(a_mgif[i], b_mgif[i]))\n  new_gif&lt;-c(new_gif,combined)\n}\n\nnew_gif\n\n\n\n14.3.3 Independent t-test True\nThis is a simulation of an independent samples t-test, two groups, 10 observations per group, assuming a true difference of 2 standard deviations between groups\nThis animation has two panels. The left panel shows means for group A (normal, mu=50, sd=10) and B (normal, mu=70, sd=10). The dots represent individual scores for each of 10 observations per group.\nThe right panel shows a t-distribution (df=18) along with the observed t-statistic for each simulation.\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(magick)\nlibrary(gganimate)\n\nA&lt;-rnorm(100,70,10)\nB&lt;-rnorm(100,50,10)\nDV &lt;- c(A,B)\nIV &lt;- rep(c(\"A\",\"B\"),each=100)\nsims &lt;- rep(rep(1:10,each=10),2)\ndf&lt;-data.frame(sims,IV,DV)\n\nmeans_df &lt;- df %&gt;%\n               group_by(sims,IV) %&gt;%\n               summarize(means=mean(DV),\n                         sem = sd(DV)/sqrt(length(DV)))\n\nstats_df &lt;- df %&gt;%\n              group_by(sims) %&gt;%\n              summarize(ts = t.test(DV~IV,var.equal=TRUE)$statistic)\n\na&lt;-ggplot(means_df, aes(x=IV,y=means, fill=IV))+\n  geom_bar(stat=\"identity\")+\n  geom_point(data=df,aes(x=IV, y=DV), alpha=.25)+\n  geom_errorbar(aes(ymin=means-sem, ymax=means+sem),width=.2)+\n  theme_classic()+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n  \na_gif&lt;-animate(a, width = 240, height = 240)\n\nb&lt;-ggplot(stats_df,aes(x=ts))+\n  geom_vline(aes(xintercept=ts, frame=sims))+\n  geom_vline(xintercept=qt(c(.025, .975), df=18),color=\"green\")+\n  geom_line(data=data.frame(x=seq(-5,5,.1),\n                            y=dt(seq(-5,5,.1),df=18)),\n            aes(x=x,y=y))+\n  theme_classic()+\n  ylab(\"density\")+\n  xlab(\"t value\")+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\nb_gif&lt;-animate(b, width = 240, height = 240)\n\n\nd&lt;-image_blank(240*2,240)\n\nthe_frame&lt;-d\nfor(i in 2:100){\n  the_frame&lt;-c(the_frame,d)\n}\n\na_mgif&lt;-image_read(a_gif)\nb_mgif&lt;-image_read(b_gif)\n\nnew_gif&lt;-image_append(c(a_mgif[1], b_mgif[1]))\nfor(i in 2:100){\n  combined &lt;- image_append(c(a_mgif[i], b_mgif[i]))\n  new_gif&lt;-c(new_gif,combined)\n}\n\nnew_gif\n\n\n\n14.3.4 T-test True sample-size\nThe top row shows 10 simulations of an independent sample t-test, with N=10, and true difference of 1 sd.\nThe bottom row shows 10 simulations with N=50.\nThe observed t-value occurs past the critical value (green) line much more reliably and often when sample size is larger than smaller.\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(magick)\nlibrary(gganimate)\n\nA&lt;-rnorm(100,60,10)\nB&lt;-rnorm(100,50,10)\nDV &lt;- c(A,B)\nIV &lt;- rep(c(\"A\",\"B\"),each=100)\nsims &lt;- rep(rep(1:10,each=10),2)\ndf&lt;-data.frame(sims,IV,DV)\n\nmeans_df &lt;- df %&gt;%\n               group_by(sims,IV) %&gt;%\n               summarize(means=mean(DV),\n                         sem = sd(DV)/sqrt(length(DV)))\n\nstats_df &lt;- df %&gt;%\n              group_by(sims) %&gt;%\n              summarize(ts = t.test(DV~IV,var.equal=TRUE)$statistic)\n\na&lt;-ggplot(means_df, aes(x=IV,y=means, fill=IV))+\n  geom_bar(stat=\"identity\")+\n  geom_point(data=df,aes(x=IV, y=DV), alpha=.25)+\n  geom_errorbar(aes(ymin=means-sem, ymax=means+sem),width=.2)+\n  theme_classic()+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n  \na_gif&lt;-animate(a, width = 240, height = 240)\n\nb&lt;-ggplot(stats_df,aes(x=ts))+\n  geom_vline(aes(xintercept=ts, frame=sims))+\n  geom_vline(xintercept=qt(c(.025, .975), df=18),color=\"green\")+\n  geom_line(data=data.frame(x=seq(-5,5,.1),\n                            y=dt(seq(-5,5,.1),df=18)),\n            aes(x=x,y=y))+\n  theme_classic()+\n  ylab(\"density\")+\n  xlab(\"t value\")+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\nb_gif&lt;-animate(b, width = 240, height = 240)\n\n\nd&lt;-image_blank(240*2,240)\n\nthe_frame&lt;-d\nfor(i in 2:100){\n  the_frame&lt;-c(the_frame,d)\n}\n\na_mgif&lt;-image_read(a_gif)\nb_mgif&lt;-image_read(b_gif)\n\nnew_gif&lt;-image_append(c(a_mgif[1], b_mgif[1]))\nfor(i in 2:100){\n  combined &lt;- image_append(c(a_mgif[i], b_mgif[i]))\n  new_gif&lt;-c(new_gif,combined)\n}\n\nnew_gif\n\n## increase sample-size\n\nA&lt;-rnorm(50*10,60,10)\nB&lt;-rnorm(50*10,50,10)\nDV &lt;- c(A,B)\nIV &lt;- rep(c(\"A\",\"B\"),each=50*10)\nsims &lt;- rep(rep(1:10,each=50),2)\ndf&lt;-data.frame(sims,IV,DV)\n\nmeans_df &lt;- df %&gt;%\n               group_by(sims,IV) %&gt;%\n               summarize(means=mean(DV),\n                         sem = sd(DV)/sqrt(length(DV)))\n\nstats_df &lt;- df %&gt;%\n              group_by(sims) %&gt;%\n              summarize(ts = t.test(DV~IV,var.equal=TRUE)$statistic)\n\na&lt;-ggplot(means_df, aes(x=IV,y=means, fill=IV))+\n  geom_bar(stat=\"identity\")+\n  geom_point(data=df,aes(x=IV, y=DV), alpha=.25)+\n  geom_errorbar(aes(ymin=means-sem, ymax=means+sem),width=.2)+\n  theme_classic()+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n  \na_gif&lt;-animate(a, width = 240, height = 240)\n\nb&lt;-ggplot(stats_df,aes(x=ts))+\n  geom_vline(aes(xintercept=ts, frame=sims))+\n  geom_vline(xintercept=qt(c(.025, .975), df=98),color=\"green\")+\n  geom_line(data=data.frame(x=seq(-5,5,.1),\n                            y=dt(seq(-5,5,.1),df=98)),\n            aes(x=x,y=y))+\n  theme_classic()+\n  ylab(\"density\")+\n  xlab(\"t value\")+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\nb_gif&lt;-animate(b, width = 240, height = 240)\n\n\nd&lt;-image_blank(240*2,240)\n\nthe_frame&lt;-d\nfor(i in 2:100){\n  the_frame&lt;-c(the_frame,d)\n}\n\na_mgif&lt;-image_read(a_gif)\nb_mgif&lt;-image_read(b_gif)\n\nnew_gif2&lt;-image_append(c(a_mgif[1], b_mgif[1]))\nfor(i in 2:100){\n  combined &lt;- image_append(c(a_mgif[i], b_mgif[i]))\n  new_gif2&lt;-c(new_gif2,combined)\n}\n\n## add new row\n\nfinal_gif &lt;- image_append(c(new_gif[1], new_gif2[1]),stack=TRUE)\nfor(i in 2:100){\n  combined &lt;- image_append(c(new_gif[i], new_gif2[i]),stack=TRUE)\n  final_gif&lt;-c(final_gif,combined)\n}\n\nfinal_gif\n\n\n\n14.3.5 one-factor ANOVA Null\nThree groups, N=10, all observations sampled from same normal distribution (mu=50, sd = 10)\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(magick)\nlibrary(gganimate)\n\n\nA&lt;-rnorm(100,50,10)\nB&lt;-rnorm(100,50,10)\nC&lt;-rnorm(100,50,10)\nDV &lt;- c(A,B,C)\nIV &lt;- rep(rep(c(\"A\",\"B\",\"C\"),each=10),10)\nsims &lt;- rep(1:10,each=30)\ndf&lt;-data.frame(sims,IV,DV)\n\nmeans_df &lt;- df %&gt;%\n  group_by(sims,IV) %&gt;%\n  summarize(means=mean(DV),\n            sem = sd(DV)/sqrt(length(DV)))\n\nstats_df &lt;- df %&gt;%\n  group_by(sims) %&gt;%\n  summarize(Fs = summary(aov(DV~IV))[[1]][[4]][1])\n\na&lt;-ggplot(means_df, aes(x=IV,y=means, fill=IV))+\n  geom_bar(stat=\"identity\")+\n  geom_point(data=df,aes(x=IV, y=DV), alpha=.25)+\n  geom_errorbar(aes(ymin=means-sem, ymax=means+sem),width=.2)+\n  theme_classic(base_size = 20)+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\nb&lt;-ggplot(stats_df,aes(x=Fs))+\n  geom_vline(aes(xintercept=Fs))+\n  geom_vline(xintercept=qf(.95, df1=2,df2=27),color=\"green\")+\n  geom_line(data=data.frame(x=seq(0,6,.1),\n                            y=df(seq(0,6,.1),df1=2,df2=27)),\n            aes(x=x,y=y))+\n  theme_classic(base_size = 20)+\n  ylab(\"density\")+\n  xlab(\"F value\")+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\na_gif&lt;-animate(a,width=480,height=480)\nb_gif&lt;-animate(b,width=480,height=480)\n\na_mgif&lt;-image_read(a_gif)\nb_mgif&lt;-image_read(b_gif)\n\nnew_gif&lt;-image_append(c(a_mgif[1], b_mgif[1]))\nfor(i in 2:100){\n  combined &lt;- image_append(c(a_mgif[i], b_mgif[i]))\n  new_gif&lt;-c(new_gif,combined)\n}\n\nnew_gif\n\n\n\n14.3.6 Factorial Null\n10 simulations, N=10 in each of 4 conditions in a 2x2 (between-subjects). All observations taken from the same normal distribution (mu=50, sd =10).\n\n\n\n\n\n\nA&lt;-rnorm(100,50,10)\nB&lt;-rnorm(100,50,10)\nC&lt;-rnorm(100,50,10)\nD&lt;-rnorm(100,50,10)\nDV &lt;- c(A,B,C,D)\nIV1 &lt;- rep(c(\"A\",\"B\"),each=200)\nIV2&lt;-rep(rep(c(\"1\",\"2\"),each=100),2)\nsims &lt;- rep(1:10,40)\ndf&lt;-data.frame(sims,IV1,IV2,DV)\n\nmeans_df &lt;- df %&gt;%\n  group_by(sims,IV1,IV2) %&gt;%\n  summarize(means=mean(DV),\n            sem = sd(DV)/sqrt(length(DV)))\n\nstats_df &lt;- df %&gt;%\n  group_by(sims) %&gt;%\n  summarize(FIV1 = summary(aov(DV~IV1*IV2))[[1]][[4]][1],\n            FIV2 = summary(aov(DV~IV1*IV2))[[1]][[4]][2],\n            F1x2 = summary(aov(DV~IV1*IV2))[[1]][[4]][3]\n            )\n\na&lt;-ggplot(means_df, aes(x=IV1,y=means, \n                                           group=IV2,\n                                           color=IV2))+\n  geom_point(data=df,aes(x=IV1, y=DV,group=IV2), \n             position=position_dodge(width=.2),\n             size=2,\n             alpha=.25)+\n  geom_point(size=4)+\n  geom_line(size=1.3)+\n  geom_errorbar(aes(ymin=means-sem, ymax=means+sem),width=.2,\n                color=\"black\")+\n  theme_classic(base_size = 20)+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\nb&lt;-ggplot(stats_df,aes(x=FIV1))+\n  geom_vline(aes(xintercept=FIV1),color=\"red\",size=1.2)+\n  geom_vline(aes(xintercept=FIV2),color=\"blue\",size=1.2)+\n  geom_vline(aes(xintercept=F1x2),color=\"purple\",size=1.2)+\n  geom_vline(xintercept=qf(.95, df1=1,df2=36),color=\"green\",size=1.2)+\n  geom_line(data=data.frame(x=seq(0,20,.1),\n                            y=df(seq(0,20,.1),df1=1,df2=36)),\n            aes(x=x,y=y))+\n  theme_classic(base_size = 20)+\n  ylab(\"density\")+\n  xlab(\"F value\")+\n  ggtitle(label=\"\",subtitle=\"red=IV1, blue=IV2, \\n purple=Interaction\")+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )\n\na_gif&lt;-animate(a,width=480,height=480)\nb_gif&lt;-animate(b,width=480,height=480)\n\na_mgif&lt;-image_read(a_gif)\nb_mgif&lt;-image_read(b_gif)\n\nnew_gif&lt;-image_append(c(a_mgif[1], b_mgif[1]))\nfor(i in 2:100){\n  combined &lt;- image_append(c(a_mgif[i], b_mgif[i]))\n  new_gif&lt;-c(new_gif,combined)\n}\n\nimage_animate(new_gif, fps = 10,dispose=\"none\")"
  },
  {
    "objectID": "14-Gifs.html#distributions",
    "href": "14-Gifs.html#distributions",
    "title": "14  GIFs",
    "section": "14.4 Distributions",
    "text": "14.4 Distributions\n\n14.4.1 Normal changing mean\n\n\n\n\n\n\nsome_means&lt;-c(0,1,2,3,4,5,4,3,2,1)\nall_df&lt;-data.frame()\nfor(i in 1:10){\n  dnorm_vec &lt;- dnorm(seq(-10,10,.1),mean=some_means[i],sd=1)\n  x_range   &lt;- seq(-10,10,.1)\n  means &lt;- rep(some_means[i], length(x_range))\n  sims &lt;- rep(i, length(x_range))\n  t_df&lt;-data.frame(sims,means,x_range,dnorm_vec)\n  all_df&lt;-rbind(all_df,t_df)\n}\n\nggplot(all_df, aes(x=x_range,y=dnorm_vec))+\n  geom_line()+\n  theme_classic()+\n  ylab(\"probability density\")+\n  xlab(\"value\")+\n  ggtitle(\"Normal Distribution with changing Mean\")+\n   transition_states(\n    sims,\n    transition_length = 1,\n    state_length = 1\n  )\n  #enter_fade() + \n  #exit_shrink() +\n  #ease_aes('sine-in-out')\n\n\n\n14.4.2 Normal changing sd\n\n\n\n\n\n\nsome_sds&lt;-seq(0.5,5,.5)\nall_df&lt;-data.frame()\nfor(i in 1:10){\n  dnorm_vec &lt;- dnorm(seq(-10,10,.1),mean=0,sd=some_sds[i])\n  x_range   &lt;- seq(-10,10,.1)\n  sds &lt;- rep(some_sds[i], length(x_range))\n  sims &lt;- rep(i, length(x_range))\n  t_df&lt;-data.frame(sims,sds,x_range,dnorm_vec)\n  all_df&lt;-rbind(all_df,t_df)\n}\n\nlabs_df&lt;-data.frame(sims=1:10,\n                    sds=as.character(seq(0.5,5,.5)))\n\nggplot(all_df, aes(x=x_range,y=dnorm_vec, frame=sims))+\n  geom_line()+\n  theme_classic()+\n  ylab(\"probability density\")+\n  xlab(\"value\")+\n  ggtitle(\"Normal Distribution with changing sd\")+\n  geom_label(data = labs_df, aes(x = 5, y = .5, label = sds))+\n   transition_states(\n    sims,\n    transition_length = 2,\n    state_length = 1\n  )+\n  enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\n\n\n\n\nBarnes, Mallory L. 2023. Statistics for Environmental Science."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Adair, G. 1984. “The Hawthorne Effect: A\nReconsideration of the Methodological Artifact.” Journal of\nApplied Psychology 69: 334–45. https://doi.org/10.1037/0021-9010.69.2.334.\n\n\nAnscombe, F. J. 1973. “Graphs in Statistical Analysis.”\nAmerican Statistician 27: 17–21.\n\n\nBehmer, Lawrence P, and Matthew JC Crump. 2017. “Spatial Knowledge\nDuring Skilled Action Sequencing: Hierarchical Versus\nNonhierarchical Representations.” Attention, Perception,\n& Psychophysics 79 (8): 2435–48. https://doi.org/10.3758/s13414-017-1389-3.\n\n\nBickel, P. J., E. A. Hammel, and J. W. O’Connell. 1975. “Sex Bias\nin Graduate Admissions: Data from\nBerkeley.” Science (New York, N.Y.) 187:\n398–404. https://doi.org/10.1126/science.187.4175.398.\n\n\nCampbell, D. T., and J. C. Stanley. 1963. Experimental and\nQuasi-Experimental Designs for Research. Boston, MA:\nHoughton Mifflin.\n\n\nCohen, J. 1988. Statistical Power Analysis for the Behavioral\nSciences. Second. Lawrence Erlbaum.\n\n\nEvans, J. St. B. T., J. L. Barston, and P. Pollard. 1983. “On the\nConflict Between Logic and Belief in Syllogistic Reasoning.”\nMemory and Cognition 11: 295–306. https://doi.org/10.3758/BF03196976.\n\n\nFisher, R. A. 1922. “On the Mathematical Foundation of Theoretical\nStatistics.” Philosophical Transactions of the Royal Society\nA 222: 309–68.\n\n\nHothersall, D. 2004. History of Psychology.\nMcGraw-Hill.\n\n\nIoannidis, John P. A. 2005. “Why Most Published Research Findings\nAre False.” PLoS Medicine 2 (8): 697–701. https://doi.org/10.1371/journal.pmed.0020124.\n\n\nJames, Ella L, Michael B Bonsall, Laura Hoppitt, Elizabeth M Tunbridge,\nJohn R Geddes, Amy L Milton, and Emily A Holmes. 2015. “Computer\nGame Play Reduces Intrusive Memories of Experimental Trauma via\nReconsolidation-Update Mechanisms.” Psychological\nScience 26 (8): 1201–15. https://doi.org/10.1177/0956797615583071.\n\n\nKahneman, D., and A. Tversky. 1973. “On the Psychology of\nPrediction.” Psychological Review 80: 237–51. https://doi.org/10.1037/h0034747.\n\n\nKeynes, John Maynard. 1923. A Tract on Monetary Reform.\nLondon: Macmillan and Company.\n\n\nKühberger, A, A Fritz, and T. Scherndl. 2014. “Publication Bias in\nPsychology: A Diagnosis Based on the Correlation Between\nEffect Size and Sample Size.” Public Library of Science\nOne 9: 1–8.\n\n\nMatejka, Justin, and George Fitzmaurice. 2017. “Same Stats,\nDifferent Graphs: Generating Datasets with Varied Appearance and\nIdentical Statistics Through Simulated Annealing.” In\nProceedings of the 2017 CHI Conference on Human Factors\nin Computing Systems, 1290–94. ACM. https://doi.org/10.1145/3025453.3025912.\n\n\nMaul, Andrew. 2017. “Rethinking Traditional Methods\nof Survey Validation.” Measurement:\nInterdisciplinary Research and Perspectives 15 (2): 51–69. https://doi.org/10.1080/15366367.2017.1348108.\n\n\nMeehl, P. H. 1967. “Theory Testing in Psychology and Physics:\nA Methodological Paradox.” Philosophy of\nScience 34: 103–15. https://doi.org/10.1086/288135.\n\n\nMehr, Samuel A, Lee Ann Song, and Elizabeth S Spelke. 2016. “For\n5-Month-Old Infants, Melodies Are Social.” Psychological\nScience 27 (4): 486–501. https://doi.org/10.1177/0956797615626691.\n\n\nPfungst, O. 1911. Clever Hans (The Horse\nof Mr. Von Osten): A Contribution\nto Experimental Animal and Human Psychology. Translated by C. L.\nRahn. New York: Henry Holt.\n\n\nSalsburg, David. 2001. The Lady Tasting Tea: How\nStatistics Revolutionized Science in the Twentieth Century.\nMacmillan.\n\n\nStevens, S. S. 1946. “On the Theory of Scales of\nMeasurement.” Science (New York, N.Y.) 103: 677–80. https://doi.org/10.1126/science.103.2684.677.\n\n\nStudent, A. 1908. “The Probable Error of a Mean.”\nBiometrika 6: 1–2."
  }
]