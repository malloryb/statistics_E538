<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mallory Barnes">

<title>6&nbsp; Hypothesis Testing – Answering questions with data</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./07-ttests.html" rel="next">
<link href="./05-Foundation_Inference.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-626149efe8f5d16e1d391ba177679bf0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="6&nbsp; Hypothesis Testing – Answering questions with data">
<meta property="og:description" content="">
<meta property="og:image" content="06-Hypothesis_Testing_files/figure-html/fig-5-5.keyfig-1.png">
<meta property="og:site_name" content="Answering questions with data">
<meta name="twitter:title" content="6&nbsp; Hypothesis Testing – Answering questions with data">
<meta name="twitter:description" content="A free textbook teaching introductory statistics for environmental science graduate students, including a lab manual, and course website. Licensed on CC BY SA 4.0">
<meta name="twitter:image" content="imgs/TextbookCover.png">
<meta name="twitter:site" content="@mallory_barnes">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./06-Hypothesis_Testing.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Answering questions with data</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/malloryb/statistics_E538" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-Science_Data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Why Statistics?</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-Describing_Data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Describing Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-Correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Correlation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-SamplesPopulations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Probability, Sampling, and Estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-Foundation_Inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Foundations for inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-Hypothesis_Testing.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-ttests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">t-tests</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-ANOVA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">ANOVA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-RMANOVA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Repeated Measures ANOVA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-FactorialANOVA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Factorial ANOVA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-MixedANOVA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">More On Factorial Designs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-ANCOVA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Analysis of Covariance (ANCOVA)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-Thinking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Thinking about answering questions with data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-Gifs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">GIFs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#hypothesis-testing---the-nuts-bolts" id="toc-hypothesis-testing---the-nuts-bolts" class="nav-link active" data-scroll-target="#hypothesis-testing---the-nuts-bolts"><span class="header-section-number">6.1</span> Hypothesis Testing - The Nuts &amp; Bolts</a>
  <ul class="collapse">
  <li><a href="#clarifying-alpha-p-value-and-confidence-level" id="toc-clarifying-alpha-p-value-and-confidence-level" class="nav-link" data-scroll-target="#clarifying-alpha-p-value-and-confidence-level"><span class="header-section-number">6.1.1</span> Clarifying Alpha, P-value, and Confidence Level</a></li>
  <li><a href="#the-steps-of-hypothesis-testing-applied-to-an-example" id="toc-the-steps-of-hypothesis-testing-applied-to-an-example" class="nav-link" data-scroll-target="#the-steps-of-hypothesis-testing-applied-to-an-example"><span class="header-section-number">6.1.2</span> The Steps of Hypothesis Testing Applied to an Example</a></li>
  <li><a href="#errors-in-hypothesis-testing" id="toc-errors-in-hypothesis-testing" class="nav-link" data-scroll-target="#errors-in-hypothesis-testing"><span class="header-section-number">6.1.3</span> Errors in Hypothesis Testing</a></li>
  <li><a href="#deciphering-significance-with-p-values" id="toc-deciphering-significance-with-p-values" class="nav-link" data-scroll-target="#deciphering-significance-with-p-values"><span class="header-section-number">6.1.4</span> Deciphering Significance with P-values</a></li>
  </ul></li>
  <li><a href="#graphical-review" id="toc-graphical-review" class="nav-link" data-scroll-target="#graphical-review"><span class="header-section-number">6.2</span> Graphical Review</a>
  <ul class="collapse">
  <li><a href="#key-players-in-hypothesis-testing-visualization" id="toc-key-players-in-hypothesis-testing-visualization" class="nav-link" data-scroll-target="#key-players-in-hypothesis-testing-visualization"><span class="header-section-number">6.2.1</span> Key Players in Hypothesis Testing Visualization</a></li>
  </ul></li>
  <li><a href="#graphical-review-of-test-outcomes-that-are-not-in-error" id="toc-graphical-review-of-test-outcomes-that-are-not-in-error" class="nav-link" data-scroll-target="#graphical-review-of-test-outcomes-that-are-not-in-error"><span class="header-section-number">6.3</span> Graphical Review of Test Outcomes that are Not in Error</a>
  <ul class="collapse">
  <li><a href="#graphical-descriptions" id="toc-graphical-descriptions" class="nav-link" data-scroll-target="#graphical-descriptions"><span class="header-section-number">6.3.1</span> Graphical Descriptions:</a></li>
  </ul></li>
  <li><a href="#graphical-review-of-sample-size-effect-when-test-outcomes-are-in-error" id="toc-graphical-review-of-sample-size-effect-when-test-outcomes-are-in-error" class="nav-link" data-scroll-target="#graphical-review-of-sample-size-effect-when-test-outcomes-are-in-error"><span class="header-section-number">6.4</span> Graphical Review of Sample Size Effect when Test Outcomes are in Error</a>
  <ul class="collapse">
  <li><a href="#graphical-descriptions-1" id="toc-graphical-descriptions-1" class="nav-link" data-scroll-target="#graphical-descriptions-1"><span class="header-section-number">6.4.1</span> Graphical Descriptions:</a></li>
  <li><a href="#how-significant-is-significant-interpreting-p-values" id="toc-how-significant-is-significant-interpreting-p-values" class="nav-link" data-scroll-target="#how-significant-is-significant-interpreting-p-values"><span class="header-section-number">6.4.2</span> How Significant is ‘Significant’ – Interpreting p-values</a></li>
  </ul></li>
  <li><a href="#review-of-ways-to-test-h0" id="toc-review-of-ways-to-test-h0" class="nav-link" data-scroll-target="#review-of-ways-to-test-h0"><span class="header-section-number">6.5</span> Review of Ways to Test (H<sub>0</sub>)</a>
  <ul class="collapse">
  <li><a href="#results-statement" id="toc-results-statement" class="nav-link" data-scroll-target="#results-statement"><span class="header-section-number">6.5.1</span> Results Statement</a></li>
  <li><a href="#beyond-the-0.5-cutoff-effect-size-and-power" id="toc-beyond-the-0.5-cutoff-effect-size-and-power" class="nav-link" data-scroll-target="#beyond-the-0.5-cutoff-effect-size-and-power"><span class="header-section-number">6.5.2</span> Beyond the 0.5 cutoff: Effect-size and power</a></li>
  <li><a href="#the-importance-of-knowing-what-youre-doing" id="toc-the-importance-of-knowing-what-youre-doing" class="nav-link" data-scroll-target="#the-importance-of-knowing-what-youre-doing"><span class="header-section-number">6.5.3</span> The importance of knowing what you’re doing</a></li>
  <li><a href="#chance-vs.-real-effects-the-playground-the-superpower-and-the-impact-scale" id="toc-chance-vs.-real-effects-the-playground-the-superpower-and-the-impact-scale" class="nav-link" data-scroll-target="#chance-vs.-real-effects-the-playground-the-superpower-and-the-impact-scale"><span class="header-section-number">6.5.4</span> Chance vs.&nbsp;Real Effects: The Playground, the Superpower, and the Impact Scale</a></li>
  <li><a href="#effect-size-concrete-vs.-abstract-notions" id="toc-effect-size-concrete-vs.-abstract-notions" class="nav-link" data-scroll-target="#effect-size-concrete-vs.-abstract-notions"><span class="header-section-number">6.5.5</span> Effect size: concrete vs.&nbsp;abstract notions</a></li>
  <li><a href="#cohens-d" id="toc-cohens-d" class="nav-link" data-scroll-target="#cohens-d"><span class="header-section-number">6.5.6</span> Cohen’s d</a></li>
  </ul></li>
  <li><a href="#power" id="toc-power" class="nav-link" data-scroll-target="#power"><span class="header-section-number">6.6</span> Power</a>
  <ul class="collapse">
  <li><a href="#a-digresssion-about-hypothesis-testing" id="toc-a-digresssion-about-hypothesis-testing" class="nav-link" data-scroll-target="#a-digresssion-about-hypothesis-testing"><span class="header-section-number">6.6.1</span> A digresssion about hypothesis testing</a></li>
  <li><a href="#back-to-power" id="toc-back-to-power" class="nav-link" data-scroll-target="#back-to-power"><span class="header-section-number">6.6.2</span> Back to power</a></li>
  <li><a href="#power-curves" id="toc-power-curves" class="nav-link" data-scroll-target="#power-curves"><span class="header-section-number">6.6.3</span> Power curves</a></li>
  </ul></li>
  <li><a href="#planning-your-design" id="toc-planning-your-design" class="nav-link" data-scroll-target="#planning-your-design"><span class="header-section-number">6.7</span> Planning your design</a></li>
  <li><a href="#some-considerations" id="toc-some-considerations" class="nav-link" data-scroll-target="#some-considerations"><span class="header-section-number">6.8</span> Some considerations</a>
  <ul class="collapse">
  <li><a href="#low-powered-studies" id="toc-low-powered-studies" class="nav-link" data-scroll-target="#low-powered-studies"><span class="header-section-number">6.8.1</span> Low powered studies</a></li>
  <li><a href="#large-n-and-small-effects" id="toc-large-n-and-small-effects" class="nav-link" data-scroll-target="#large-n-and-small-effects"><span class="header-section-number">6.8.2</span> Large N and small effects</a></li>
  <li><a href="#small-n-and-large-effects" id="toc-small-n-and-large-effects" class="nav-link" data-scroll-target="#small-n-and-large-effects"><span class="header-section-number">6.8.3</span> Small N and Large effects</a></li>
  <li><a href="#type-i-errors-are-convincing-when-n-is-small" id="toc-type-i-errors-are-convincing-when-n-is-small" class="nav-link" data-scroll-target="#type-i-errors-are-convincing-when-n-is-small"><span class="header-section-number">6.8.4</span> Type I errors are convincing when N is small</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/malloryb/statistics_E538/edit/master/06-Hypothesis_Testing.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mallory Barnes </p>
          </div>
  </div>
    
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">August 19, 2025</p>
    </div>
  </div>
    
  </div>
  


</header>


<section id="hypothesis-testing---the-nuts-bolts" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="hypothesis-testing---the-nuts-bolts"><span class="header-section-number">6.1</span> Hypothesis Testing - The Nuts &amp; Bolts</h2>
<p>Hypothesis testing helps us figure out if what we believe about a whole group is likely true, just by looking at a small part of it (a sample).</p>
<hr>
<section id="clarifying-alpha-p-value-and-confidence-level" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="clarifying-alpha-p-value-and-confidence-level"><span class="header-section-number">6.1.1</span> Clarifying Alpha, P-value, and Confidence Level</h3>
<p>Before diving deep, let’s clear up some terms you’ll come across often.</p>
<p><strong>Alpha (</strong><span class="math inline">\(\alpha\)</span>)</p>
<p>Alpha (<span class="math inline">\(\alpha\)</span>) is the significance level of a statistical test, and it quantifies the risk of committing a Type I error. A Type I error happens when we incorrectly reject a true null hypothesis. The standard value for alpha is often set at 0.05, implying a 5% chance of making a Type I error. In other words, we are willing to accept a 5% risk of concluding that a difference exists when there is no actual difference.</p>
<p><strong>P-value</strong></p>
<p>The p-value is another crucial concept in hypothesis testing. It represents the probability of observing the obtained results, or something more extreme, assuming that the null hypothesis is true. A small p-value (usually ≤ 0.05) suggests that the observed data is inconsistent with the null hypothesis, and thus, you have evidence to reject it.</p>
<p><strong>Confidence Level</strong></p>
<p>The confidence level is related but distinct from alpha and p-value. While alpha quantifies the risk of a Type I error, the confidence level indicates how confident we are in our statistical estimates. The confidence level is calculated as the complement of alpha:</p>
<p><span class="math display">\[
\text{Confidence Level} = 1 - \alpha
\]</span></p>
<p>For example, if <span class="math inline">\(\alpha\)</span> is 0.05, the confidence level would be (1 - 0.05 = 0.95) or 95%. This means we are 95% confident that our results fall within a specific range.</p>
<p><strong>Bringing It All Together</strong></p>
<ul>
<li><strong>Alpha (</strong><span class="math inline">\(\alpha\)</span>): Risk of Type I error (usually 5%)</li>
<li><strong>P-value</strong>: Probability of observed data given the null is true</li>
<li><strong>Confidence Level</strong>: Confidence in the range of our estimates (usually 95%)</li>
</ul>
<p>Grasping how these three terms connect and differ is key to making sense of the stats we’ll discuss.</p>
<hr>
</section>
<section id="the-steps-of-hypothesis-testing-applied-to-an-example" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="the-steps-of-hypothesis-testing-applied-to-an-example"><span class="header-section-number">6.1.2</span> The Steps of Hypothesis Testing Applied to an Example</h3>
<p>Let’s say we want to know if the average pollution in a set of water samples is above the legal limit. Or if young deer in a region are, on average, healthy.</p>
<p><strong>Step 1</strong>: Define Your Hypotheses: First, we need to define two hypotheses: the <strong>research hypothesis</strong> and the <strong>null hypothesis</strong>.</p>
<ul>
<li><strong>Research Hypothesis (H<sub>a</sub>)</strong>: This is what we aim to support. <strong>Keep in mind, we can’t exactly “prove” H<sub>a</sub> is correct, we can only say that H<sub>0</sub> isn’t likely</strong>. It can take a few forms based on the question:
<ul>
<li>H<sub>a</sub>: average pollution &gt; legal limit (pollution is too high)</li>
<li>H<sub>a</sub>: average pollution &lt; legal limit (pollution is too low)</li>
<li>H<sub>a</sub>: average pollution ≠ legal limit (pollution is just different)</li>
</ul></li>
<li><strong>Null Hypothesis (H<sub>0</sub>)</strong>: This is the default or ‘no change’ scenario. It’s opposite to the research hypothesis.
<ul>
<li>H<sub>0</sub>: average pollution ≤ legal limit (for the first H<sub>a</sub>)</li>
<li>H<sub>0</sub>: average pollution ≥ legal limit (for the second H<sub>a</sub>)</li>
<li>H<sub>0</sub>: average pollution = legal limit (for the third H<sub>a</sub>)</li>
</ul></li>
</ul>
<p><strong>Step 2</strong>: Choose Your Test Statistic: Based on the data, we’ll compute a <strong>test statistic</strong>. This number will help us decide which hypothesis seems more likely.</p>
<p><strong>Step 3</strong>: Determine the Rejection Region: Before running the test, we decide on a <strong>rejection region</strong>. If our test statistic falls in this region, we’ll reject the null hypothesis.</p>
<p><strong>Step 4</strong>: Check Assumptions: Before drawing conclusions, ensure that the test’s conditions and assumptions are satisfied.</p>
<p><strong>Step 5</strong>: Draw Conclusions: Finally, based on the test statistic and the rejection region, decide whether to reject the null hypothesis.</p>
<hr>
</section>
<section id="errors-in-hypothesis-testing" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="errors-in-hypothesis-testing"><span class="header-section-number">6.1.3</span> Errors in Hypothesis Testing</h3>
<p>Sometimes, even with the best methods, we make incorrect decisions.</p>
<ul>
<li><p><strong>Type I Error</strong> (<span class="math inline">\(\alpha\)</span>): This happens when we mistakenly reject the true null hypothesis. Imagine sending an innocent person to jail. Typically, <span class="math inline">\(\alpha\)</span> is set at 0.05 (5%).</p></li>
<li><p><strong>Type II Error</strong> (<span class="math inline">\(\beta\)</span>): Here, we mistakenly accept a false null hypothesis. Think of it as letting a guilty person go free.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Decision</th>
<th>If the null hypothesis is True</th>
<th>If the null hypothesis is False</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Reject H<sub>0</sub></strong></td>
<td>Type I error (prob = <span class="math inline">\(\alpha\)</span>)</td>
<td>Correct (prob = 1 - <span class="math inline">\(\beta\)</span>)</td>
</tr>
<tr class="even">
<td><strong>Fail to reject H<sub>0</sub></strong></td>
<td>Correct (prob = 1 - <span class="math inline">\(\alpha\)</span>)</td>
<td>Type II error (prob = <span class="math inline">\(\beta\)</span>)</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p><strong>Key Takeaway</strong>: As <span class="math inline">\(\alpha\)</span> gets smaller, <span class="math inline">\(\beta\)</span> gets bigger, and vice-versa.</p>
</blockquote>
<hr>
<hr>
</section>
<section id="deciphering-significance-with-p-values" class="level3" data-number="6.1.4">
<h3 data-number="6.1.4" class="anchored" data-anchor-id="deciphering-significance-with-p-values"><span class="header-section-number">6.1.4</span> Deciphering Significance with P-values</h3>
<p>The p-value is like a reality-check. It tells us how weird our results are if we assume the starting belief (null hypothesis) is spot on.</p>
<ul>
<li><p><strong>One-Tailed Test</strong>: The p-value shows the likelihood of observing an average as extreme as our sample’s if the null hypothesis stands.</p></li>
<li><p><strong>Two-Tailed Test</strong>: This p-value represents the odds of spotting an average as different from the null value as our sample’s.</p></li>
</ul>
<blockquote class="blockquote">
<p><strong>Rule of Thumb</strong>: If the p-value is less than <span class="math inline">\(\alpha\)</span>, we opt to reject the null hypothesis.</p>
</blockquote>
</section>
</section>
<section id="graphical-review" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="graphical-review"><span class="header-section-number">6.2</span> Graphical Review</h2>
<section id="key-players-in-hypothesis-testing-visualization" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="key-players-in-hypothesis-testing-visualization"><span class="header-section-number">6.2.1</span> Key Players in Hypothesis Testing Visualization</h3>
<p>We define and visualize the core components essential to understanding the graphical representations of hypothesis testing:</p>
<ol type="1">
<li><p><strong>Null Distribution</strong> - The hypothesized parent distribution under the assumption that the null hypothesis H<sub>0</sub> is true.</p></li>
<li><p><strong>Inferred Parent Distribution</strong> - The parent distribution inferred from our sample data. This is what we conceptualize as the distribution of H<sub>a</sub>.</p></li>
<li><p><strong>True Parent Distribution</strong> - The actual distribution from which our sample originates.</p></li>
<li><p><strong>Sampling Distribution of the Sample Mean</strong> - Represents the distribution of sample means if we were to draw multiple samples from the parent distribution. This is crucial for making inferences about the <strong>Inferred Parent Distribution</strong>.</p></li>
</ol>
<hr>
<p>In the figure below, we’ve outlined the various elements crucial for hypothesis testing. Think of this section as a handy guide. Whenever you come across detailed graphs later in this chapter, you can circle back here for clarity.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5-5.keyfig" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5-5.keyfig-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5-5.keyfig-1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5-5.keyfig-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.1: Comparison of four distributions essential for hypothesis testing: Null, True Parent, Inferred Parent, and Sampling Distribution of the Sample Mean.
</figcaption>
</figure>
</div>
</div>
</div>
<hr>
<p>For a two-tailed alternative, we are interested in the possibility that a sample comes from a parent distribution that may have a lower or higher location than the null.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5-5.twotailed" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5-5.twotailed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5-5.twotailed-1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5-5.twotailed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.2: Two-tailed distribution
</figcaption>
</figure>
</div>
</div>
</div>
<hr>
<p>In a one-tailed t-test, we’re examining if our sample originates from a parent distribution that’s situated either below or above the null hypothesis. Unlike a two-tailed test, we’re only interested in one of these directions, not both.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5-5.onetailed" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5-5.onetailed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5-5.onetailed-1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5-5.onetailed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.3: Side-by-side comparison of one-tailed t-test scenarios: exploring if our sample comes from a distribution either below or above the null hypothesis.
</figcaption>
</figure>
</div>
</div>
</div>
<hr>
<p>In a “perfect” world in which the null hypothesis is true, the sample’s parent distribution (solid, orange) is exactly the same parent distribution described by the null hypothesis (solid, blue).</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5-5.null_identical_sample" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5-5.null_identical_sample-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5-5.null_identical_sample-1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5-5.null_identical_sample-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.4: True parent distribution &amp; null distribution are the same
</figcaption>
</figure>
</div>
</div>
</div>
<hr>
<p><strong>We never know the true parent distribution of the sample</strong> – we infer it from the sample. Here, the tall dash-dotted line shows the sampling distribution of the mean, from which we infer the parent distribution (green3, dashed).</p>
<p>In this even more perfect world, that parent distribution is the same as the parent distribution described by the null hypothesis and we have taken a perfectly representative sample, so all 3 curves line up perfectly on the same mean. The thick, short, flat dark green line is the confidence interval for the sample mean.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5-5.perfectly_representative_sample" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5-5.perfectly_representative_sample-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5-5.perfectly_representative_sample-1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5-5.perfectly_representative_sample-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.5: Here, we have a perfectly representative sample
</figcaption>
</figure>
</div>
</div>
</div>
<hr>
<p>In an imperfect but convenient world, the sample is not a perfect representation of the parent population, but is fairly close. The sample mean is close to hypothesized mean, and (in the 2-tailed case) the confidence interval for the sample mean “catches” the mean of the null hypothesis (pink dashed line). A hypothesis test will correctly determine that there is not a significant difference between the sample mean and the mean of the null hypothesis.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5-5.sample_imperfect_convenient" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5-5.sample_imperfect_convenient-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5-5.sample_imperfect_convenient-1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5-5.sample_imperfect_convenient-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.6: Imperfect, but convenient
</figcaption>
</figure>
</div>
</div>
</div>
<hr>
<p>In an imperfect and inconvenient world, the random sample is, by chance, sufficiently imperfect that the apparent (inferred) parent distribution is far from the true parent distribution and (in the 2-tailed case) the confidence interval for the sample mean no longer “catches” the mean of the null hypothesis. A hypothesis test will now find a significant difference between the sample mean and the mean of the null hypothesis. <strong>This is a type I error</strong>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5-5.type1_error" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5-5.type1_error-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5-5.type1_error-1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5-5.type1_error-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.7: Type 1 Error
</figcaption>
</figure>
</div>
</div>
</div>
<hr>
<p>In another imperfect and inconvenient world, the sample (dashed dark green lines) really is drawn from the alternative distribution (the sample’s true parent distribution; orange), but is unrepresentative of its parent and similar to the null (solid blue line). The confidence interval (in the 2-tailed case) of the sample “catches” the mean of the null hypothesis although it is far from the mean of the true parent of the sample. A hypothesis test will find no significant difference between the sample mean and the mean of the null hypothesis. <strong>This is a type II error.</strong></p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5-5.type2_error" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5-5.type2_error-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5-5.type2_error-1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5-5.type2_error-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.8: Type 2 Error
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="graphical-review-of-test-outcomes-that-are-not-in-error" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="graphical-review-of-test-outcomes-that-are-not-in-error"><span class="header-section-number">6.3</span> Graphical Review of Test Outcomes that are Not in Error</h2>
<p>As you review hypothesis testing, it’s essential to remember that we don’t <em>accept</em> the null hypothesis. The possibility of a Type I error means our conclusion might be flawed. Instead of accepting the null hypothesis, we <em>fail to reject</em> H<sub>0</sub>. The scarcity of data with small sample sizes can lead to significant differences between the sample mean and the null mean (μ_0). While it’s tempting to gather more data to be more certain, in the meantime, the best we can do is fail to reject H<sub>0</sub>.</p>
<p>In the figures below, as in the figures above, the blue lines represent the null parent distribution (defined by the null mean and the sample’s standard deviation).</p>
<p>The green solid lines denote the apparent parent distribution of our sample:</p>
<ul>
<li><p><strong>Solid lighter green line</strong>: Represents the distribution described by our sample mean and standard deviation.</p></li>
<li><p>*<strong>Dashed dark green line</strong>: Shows the sampling distribution of the sample mean, described by our sample mean and the standard error (SE).</p></li>
</ul>
<section id="graphical-descriptions" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="graphical-descriptions"><span class="header-section-number">6.3.1</span> Graphical Descriptions:</h3>
<ol type="1">
<li><strong>Fail to Reject the Null Hypothesis</strong> - <em>Sample Mean Supports the Null Hypothesis</em>: The means are far apart, but <strong>not in our direction of interest</strong>. For a one-tailed test, only data on one side of the rejection region can support the null hypothesis. Question to ponder: If we gather more data and obtain the same sample mean, could our conclusion change?</li>
</ol>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5-5.fail_to_reject_1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5-5.fail_to_reject_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5-5.fail_to_reject_1-1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5-5.fail_to_reject_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.9: Ha:μ &lt; X
</figcaption>
</figure>
</div>
</div>
</div>
<ol start="2" type="1">
<li><strong>Fail to Reject the Null Hypothesis</strong>: <em>The sample mean supports the alternate hypothesis (it is on the appropriate side of the rejection region), but the sample size is too small</em>. The sample mean is only about 1SE from the null mean, making it too close to be significant. Hypothetical situation: With more data and the same sample mean, could our conclusion differ?</li>
</ol>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5-5.fail_to_reject_2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5-5.fail_to_reject_2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5-5.fail_to_reject_2-1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5-5.fail_to_reject_2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.10: Ha:μ &lt; X
</figcaption>
</figure>
</div>
</div>
</div>
<ol start="3" type="1">
<li><strong>Reject the Null Hypothesis</strong>: The sample mean is on the appropriate side of the rejection region. It’s significantly distant from the null mean, over 3 SE, which is typically considered significant for most standard values of α.</li>
</ol>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5-5.reject_1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5-5.reject_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5-5.reject_1-1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5-5.reject_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.11: Ha:μ &lt; X
</figcaption>
</figure>
</div>
</div>
</div>
<ol start="4" type="1">
<li><strong>Reject the Null Hypothesis (Two-Tailed Test)</strong>: Reject the null hypothesis for the same reasons as the previous example. This case is two-tailed, but nothing else has changed.</li>
</ol>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5-5.reject_2_two_tailed" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5-5.reject_2_two_tailed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5-5.reject_2_two_tailed-1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5-5.reject_2_two_tailed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.12: Ha:μ ≠ X
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="graphical-review-of-sample-size-effect-when-test-outcomes-are-in-error" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="graphical-review-of-sample-size-effect-when-test-outcomes-are-in-error"><span class="header-section-number">6.4</span> Graphical Review of Sample Size Effect when Test Outcomes are in Error</h2>
<p>It’s a given that we never truly grasp the actual parent distribution of a sample. An unrepresentative sample can lead either to a Type I or a Type II error. The term <em>sampling error</em> is sometimes invoked to depict such unrepresentative samples, but it’s imperative to understand that the researcher hasn’t committed any mistakes.</p>
<section id="graphical-descriptions-1" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="graphical-descriptions-1"><span class="header-section-number">6.4.1</span> Graphical Descriptions:</h3>
<p><strong>Type I Error</strong>: Here, the green curves depict the sampling distribution (dark green) and the apparent parent distribution (lighter green) of our sample. But in reality, the sample is a product of the null distribution (blue). Question to ponder: How would the representation look if we had utilized a smaller sample size?</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5-5.type1_samplesize" class="quarto-float quarto-figure quarto-figure-center anchored" width="100%">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5-5.type1_samplesize-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5-5.type1_samplesize-1.png" id="fig-5-5.type1_samplesize" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-5-5.type1_samplesize-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.13
</figcaption>
</figure>
</div>
</div>
</div>
<p>The main thing that would change with a larger sample size is that the sampling distribution of sample means becomes much tighter, thus making the confidence interval smaller. So here, are we more or less likely to have a type 1 error with the larger sample size?</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5-5.type_1_samplesize_larger" class="quarto-float quarto-figure quarto-figure-center anchored" width="100%">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5-5.type_1_samplesize_larger-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5-5.type_1_samplesize_larger-1.png" id="fig-5-5.type_1_samplesize_larger" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-5-5.type_1_samplesize_larger-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.14
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Type II Error</strong>: The sample genuinely hails from the solid orange parent population. However, it was misleading enough (as depicted by the dashed green line) to seem analogous to the null distribution (blue). Query to reflect upon: How would this representation transform if the sample size was substantially larger?</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5-5.type_2_samplesize" class="quarto-float quarto-figure quarto-figure-center anchored" width="100%">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5-5.type_2_samplesize-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5-5.type_2_samplesize-1.png" id="fig-5-5.type_2_samplesize" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-5-5.type_2_samplesize-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.15
</figcaption>
</figure>
</div>
</div>
</div>
<p>The main thing that would change with a larger sample size is that the sampling distribution of sample means becomes much tighter, thus making the confidence interval smaller. So here, are we more or less likely to have a type 2 error with the larger sample size?</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5-5.type_2_samplesize_larger" class="quarto-float quarto-figure quarto-figure-center anchored" width="100%">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5-5.type_2_samplesize_larger-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5-5.type_2_samplesize_larger-1.png" id="fig-5-5.type_2_samplesize_larger" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-5-5.type_2_samplesize_larger-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.16
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="how-significant-is-significant-interpreting-p-values" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="how-significant-is-significant-interpreting-p-values"><span class="header-section-number">6.4.2</span> How Significant is ‘Significant’ – Interpreting p-values</h3>
<p>When we use a rejection region to test a hypothesis, we get a yes-or-no answer. For a two-tailed test, if we ask whether the confidence interval “captures” the null mean, we get a yes-or-no answer as well.</p>
<section id="calculating-p-values" class="level4" data-number="6.4.2.1">
<h4 data-number="6.4.2.1" class="anchored" data-anchor-id="calculating-p-values"><span class="header-section-number">6.4.2.1</span> Calculating p-values</h4>
<p>We can do better – we can get an actual probability value. The blue box for the z-test tells us how to calculate our p-value if our mean is in the area of interest for the test.</p>
<ol type="1">
<li><p><strong>Calculate Z-value</strong>: First, we calculate how many standard errors our mean is from the null mean. This is the z-value for our mean in the world of the null hypothesis.</p>
<ul>
<li>For (H<sub>0</sub> &lt; X), we ask about the upper tail probability of our mean.</li>
<li>For (H<sub>0</sub> &gt; X), we ask about the lower tail probability of our mean.</li>
<li>For (H<sub>0</sub> = X), we calculate twice the tail probability.</li>
</ul></li>
</ol>
</section>
<section id="one-tailed-vs-two-tailed-tests" class="level4" data-number="6.4.2.2">
<h4 data-number="6.4.2.2" class="anchored" data-anchor-id="one-tailed-vs-two-tailed-tests"><span class="header-section-number">6.4.2.2</span> One-Tailed vs Two-Tailed Tests</h4>
<ul>
<li><p><strong>One-Tailed Test</strong>: The p-value tells us the probability of a mean at least as much greater than (H<sub>0</sub>) as our mean, when the null hypothesis is true or as much less than (H<sub>0</sub>).</p></li>
<li><p><strong>Two-Tailed Test</strong>: The p-value tells us the probability of a mean at least as different from (H<sub>0</sub>) as our mean, when the null hypothesis is true.</p></li>
</ul>
</section>
<section id="rejecting-the-null-hypothesis" class="level4" data-number="6.4.2.3">
<h4 data-number="6.4.2.3" class="anchored" data-anchor-id="rejecting-the-null-hypothesis"><span class="header-section-number">6.4.2.3</span> Rejecting the Null Hypothesis</h4>
<p>We reject (H<sub>0</sub>) when (p)-value (&lt; ). At that point, our data are too unusual when the null hypothesis is true for us to believe that the null hypothesis is true.</p>
<ul>
<li><strong>Small p-value</strong>: When (p) is small, our data provide weak support for (H<sub>0</sub>), and we are more sure that (H<sub>0</sub>) is not true, and that (H<sub>a</sub>) is more likely.</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="review-of-ways-to-test-h0" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="review-of-ways-to-test-h0"><span class="header-section-number">6.5</span> Review of Ways to Test (H<sub>0</sub>)</h2>
<ol type="1">
<li><p><strong>Confidence Interval</strong>: If the alternative is two-tailed, build a 1-() confidence interval. If the CI catches (H<sub>0</sub>) then fail to reject.</p></li>
<li><p><strong>Test Statistic</strong>: Calculate the test statistic and compare to the rejection region of size (). If the test statistic is in the rejection region, reject (H<sub>0</sub>).</p></li>
<li><p><strong>Probability</strong>: Determine the probability of your test statistic. If (p &lt; ) then reject (H<sub>0</sub>).</p></li>
</ol>
<p><strong>Note</strong>: The first two methods give you a yes-or-no answer. The third method gives you some additional information.</p>
<hr>
<section id="results-statement" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="results-statement"><span class="header-section-number">6.5.1</span> Results Statement</h3>
<p>Now that we have started doing statistical tests, we have also started to think about results. A results statement provides an English language version of what we discovered, as well as the statistical results. For a z-test, a results sentence might say:</p>
<blockquote class="blockquote">
<p>The average level of mercury in the ponds within 10 km of the smelter is significantly higher than the legal limit (z = 2.85, n = 32, p = 0.004).</p>
</blockquote>
<p>The information in the parentheses, for a one-sample test, is, in this order,</p>
<ol type="1">
<li>the value of the test statistic, in this case, (z);</li>
<li>the sample size or degrees of freedom; and</li>
<li>the probability of the test statistic when the null hypothesis is true.</li>
</ol>
<p>Most problems that include a test will require a results sentence.</p>
<hr>
</section>
<section id="beyond-the-0.5-cutoff-effect-size-and-power" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="beyond-the-0.5-cutoff-effect-size-and-power"><span class="header-section-number">6.5.2</span> Beyond the 0.5 cutoff: Effect-size and power</h3>
<p>You’ve probably heard me mention that the 0.5 cutoff for statistical significance is somewhat arbitrary. So, what’s the alternative? Enter effect size and statistical power. These aren’t just buzzwords; they’re foundational elements for conducting meaningful environmental research. Many scientific journals even have guidelines on how to report them. Ideally, you should be thinking about these factors before you collect your first data point. Given their importance, it’s time we delve into what these concepts really mean and why they’re crucial for research.</p>
</section>
<section id="the-importance-of-knowing-what-youre-doing" class="level3" data-number="6.5.3">
<h3 data-number="6.5.3" class="anchored" data-anchor-id="the-importance-of-knowing-what-youre-doing"><span class="header-section-number">6.5.3</span> The importance of knowing what you’re doing</h3>
<p>Effect size and power analyses are more than just boxes to tick; they’re essential tools in your research toolkit for understanding environmental data. Rather than using them simply because you were advised to, see them as integral to designing meaningful studies. These tools help you filter out statistical “noise,” revealing actionable insights that can address real-world environmental issues. They shouldn’t be applied blindly but should be part of a thoughtful research strategy aimed at making your data work for you.</p>
</section>
<section id="chance-vs.-real-effects-the-playground-the-superpower-and-the-impact-scale" class="level3" data-number="6.5.4">
<h3 data-number="6.5.4" class="anchored" data-anchor-id="chance-vs.-real-effects-the-playground-the-superpower-and-the-impact-scale"><span class="header-section-number">6.5.4</span> Chance vs.&nbsp;Real Effects: The Playground, the Superpower, and the Impact Scale</h3>
<p>In environmental research, the goal is often to identify meaningful changes—like the improvement of air quality due to reduced pollution. However, researchers sometimes find themselves grappling with statistical “noise” rather than detecting genuine effects. To navigate this complex landscape, let’s use some analogies.</p>
<p><strong>The Playground and the Mischievous Kid</strong></p>
<p>First, consider your sample size as a playground and chance as a mischievous kid running around in it. The smaller the playground, the more room this kid has to create chaos, leading to random variations in your data. On the flip side, a larger playground restricts the kid’s antics, minimizing the influence of chance. So, your first task is to design your study like an ultimate playground—spacious and well-planned to keep chance at bay.</p>
<p><strong>The Balls: Different Sizes, Different Impacts</strong></p>
<p>Next, let’s focus on the “stuff” being thrown around on this playground. Think of different types of balls—soccer balls, tennis balls, ping pong balls, and marbles—as representing different effect sizes:</p>
<ul>
<li><p><strong>Soccer Ball (Strong Effect)</strong>: It’s big and noticeable. When it lands, you know something significant has happened.</p></li>
<li><p><strong>Tennis Ball (Medium Effect)</strong>: Still impactful but not as game-changing as a soccer ball.</p></li>
<li><p><strong>Ping Pong Ball (Small Effect)</strong>: It might bounce around, but it’s not going to change the landscape.</p></li>
<li><p><strong>Marble (Very Small Effect)</strong>: Almost negligible amid the other activities.</p></li>
</ul>
<p><strong>The Superhero: Statistical Power</strong></p>
<p>Here’s where statistical power comes into play. It’s your research superhero, capable of discerning whether the changes you’re observing are due to chance, the size of your playground, or the type of ball being thrown (Effect Size). Imagine it as a keen-eyed playground supervisor who can tell the difference between a random bounce and a meaningful impact.</p>
<p>The Takeaway</p>
<ol type="1">
<li><p><strong>Plan Well</strong>: Design your study like you’re building the ultimate playground—spacious and well-planned to minimize the role of chance.</p></li>
<li><p><strong>Know Your Ball</strong>: Understand the potential impact (effect size) of what you’re introducing into your study. This helps you make meaningful conclusions.</p></li>
<li><p><strong>Power Up</strong>: Conduct a power analysis to ensure your study is equipped to distinguish between meaningful impacts and random noise.</p></li>
</ol>
<p>By focusing on these three elements—chance, effect size, and statistical power—you’re not just adhering to research best practices; you’re elevating the quality and impact of your work.</p>
</section>
<section id="effect-size-concrete-vs.-abstract-notions" class="level3" data-number="6.5.5">
<h3 data-number="6.5.5" class="anchored" data-anchor-id="effect-size-concrete-vs.-abstract-notions"><span class="header-section-number">6.5.5</span> Effect size: concrete vs.&nbsp;abstract notions</h3>
<p>Generally speaking, the big concept of effect size, is simply how big the differences are, that’s it. However, the biggness or smallness of effects quickly becomes a little bit complicated. On the one hand, the raw difference in the means can be very meaningful. Let’s say we are measuring performance on a final exam, and we are testing whether or not a miracle drug can make you do better on the test. Let’s say taking the drug makes you do 5% better on the test, compared to not taking the drug. You know what 5% means, that’s basically a whole letter grade. Pretty good. An effect-size of 25% would be even better, right? Lots of measures have a concrete quality to them, and we often want to the size of the effect expressed in terms of the original measure.</p>
<p>Let’s talk about concrete measures some more. How about learning a musical instrument. Let’s say it takes 10,000 hours to become an expert piano, violin, or guitar player. And, let’s say you found something online that says that using their method, you will learn the instrument in less time than normal. That is a claim about the effect size of their method. You would want to know how big the effect is right? For example, the effect-size could be 10 hours. That would mean it would take you 9,980 hours to become an expert (that’s a whole 10 hours less). If I knew the effect-size was so tiny, I wouldn’t bother with their new method. But, if the effect size was say 1,000 hours, that’s a pretty big deal, that’s 10% less (still doesn’t seem like much, but saving 1,000 hours seems like a lot).</p>
<p>In environmental science, we often encounter measures that are not as straightforward as, say, temperature or pH levels. Take biodiversity indices as an example. These indices can give us a numerical value representing the variety of life in a particular ecosystem, but interpreting what these numbers mean can be challenging.</p>
<p>Imagine you’re assessing the impact of a reforestation project. Your biodiversity index might read 3 before the project and 4 after. That’s a difference of only 1 unit, but what does that actually signify? Is it a significant improvement, or just a minor change? The raw numbers alone don’t provide enough context.</p>
<p>To make these abstract measures more interpretable, we often turn to standardized metrics, like z-scores. If that 1-unit difference in biodiversity corresponds to a shift of one standard deviation, that’s a substantial change worth noting. On the other hand, if the shift is only 0.1 in terms of standard deviation, then the 11-unit difference might not be as impactful as it first seemed. Standardized measures like Cohen’s d can further help us understand the practical significance of our findings.</p>
</section>
<section id="cohens-d" class="level3" data-number="6.5.6">
<h3 data-number="6.5.6" class="anchored" data-anchor-id="cohens-d"><span class="header-section-number">6.5.6</span> Cohen’s d</h3>
<p>Let’s look a few distributions to firm up some ideas about effect-size. <a href="#fig-5.5effectdists" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-5.5effectdists</span></a> has four panels. The first panel (0) represents the null distribution of no differences. This is the idea that your manipulation (A vs.&nbsp;B) doesn’t do anything at all, as a result when you measure scores in conditions A and B, you are effectively sampling scores from the very same overall distribution. The panel shows the distribution as green for condition B, but the red one for condition A is identical and drawn underneath (it’s invisible). There is 0 difference between these distributions, so it represent a null effect.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5.5effectdists" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5.5effectdists-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5.5effectdists-1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5.5effectdists-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.17: Each panel shows hypothetical distributions for two conditions. As the effect-size increases, the difference between the distributions become larger.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The remaining panels are hypothetical examples of what a true effect could look like, when your manipulation actually causes a difference. For example, if condition A is a control group, and condition B is a treatment group, we are looking at three cases where the treatment manipulation causes a positive shift in the mean of distribution. We are using normal curves with mean =0 and sd =1 for this demonstration, so a shift of .5 is a shift of half of a standard deviation. A shift of 1 is a shift of 1 standard deviation, and a shift of 2 is a shift of 2 standard deviations. We could draw many more examples showing even bigger shifts, or shifts that go in the other direction.</p>
<p>Let’s look at another example, but this time we’ll use some concrete measurements. Let’s say we are looking at final exam performance, so our numbers are grade percentages. Let’s also say that we know the mean on the test is 65%, with a standard deviation of 5%. Group A could be a control that just takes the test, Group B could receive some “educational” manipulation designed to improve the test score. These graphs then show us some hypotheses about what the manipulation may or may not be doing.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5.5effectdistsB" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5.5effectdistsB-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5.5effectdistsB-1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5.5effectdistsB-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.18: Each panel shows hypothetical distributions for two conditions. As the effect-size increases, the difference between the distributions become larger.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The first panel shows that both condition A and B will sample test scores from the same distribution (mean =65, with 0 effect). The other panels show shifted mean for condition B (the treatment that is supposed to increase test performance). So, the treatment could increase the test performance by 2.5% (mean 67.5, .5 sd shift), or by 5% (mean 70, 1 sd shift), or by 10% (mean 75%, 2 sd shift), or by any other amount. In terms of our previous metaphor, a shift of 2 standard deviations is more like jack-hammer in terms of size, and a shift of .5 standard deviations is more like using a pencil. The thing about research, is we often have no clue about whether our manipulation will produce a big or small effect, that’s why we are conducting the research.</p>
<p>You might have noticed that the letter <span class="math inline">\(d\)</span> appears in the above figure. Why is that? Jacob Cohen <span class="citation" data-cites="Cohen1988">(<a href="#ref-Cohen1988" role="doc-biblioref">Cohen 1988</a>)</span> used the letter <span class="math inline">\(d\)</span> in defining the effect-size for this situation, and now everyone calls it Cohen’s <span class="math inline">\(d\)</span>. The formula for Cohen’s <span class="math inline">\(d\)</span> is:</p>
<p><span class="math inline">\(d = \frac{\text{mean for condition 1} - \text{mean for condition 2}}{\text{population standard deviation}}\)</span></p>
<p>If you notice, this is just a kind of z-score. It is a way to standardize the mean difference in terms of the population standard deviation.</p>
<p>It is also worth noting again that this measure of effect-size is entirely hypothetical for most purposes. In general, researchers do not know the population standard deviation, they can only guess at it, or estimate it from the sample. The same goes for means, in the formula these are hypothetical mean differences in two population distributions. In practice, researchers do not know these values, they guess at them from their samples.</p>
<p>Before discussing why the concept of effect-size can be useful, we note that Cohen’s <span class="math inline">\(d\)</span> is useful for understanding abstract measures. For example, when you don’t know what a difference of 10 or 20 means as a raw score, you can standardize the difference by the sample standard deviation, then you know roughly how big the effect is in terms of standard units. If you thought a 20 was big, but it turned out to be only 1/10th of a standard deviation, then you would know the effect is actually quite small with respect to the overall variability in the data.</p>
</section>
</section>
<section id="power" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="power"><span class="header-section-number">6.6</span> Power</h2>
<p>When there is a true effect out there to measure, you want to make sure your design is sensitive enough to detect the effect, otherwise what’s the point. We’ve already talked about the idea that an effect can have different sizes. The next idea is that your design can be more less sensitive in its ability to reliabily measure the effect. We have discussed this general idea many times already in the textbook, for example we know that we will be more likely to detect “significant” effects (when there are real differences) when we increase our sample-size. Here, we will talk about the idea of design sensitivity in terms of the concept of power. Interestingly, the concept of power is a somewhat limited concept, in that it only exists as a concept within some philosophies of statistics.</p>
<section id="a-digresssion-about-hypothesis-testing" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="a-digresssion-about-hypothesis-testing"><span class="header-section-number">6.6.1</span> A digresssion about hypothesis testing</h3>
<p>In particular, the concept of power falls out of the Neyman-Pearson concept of null vs.&nbsp;alternative hypothesis testing. Neyman-Pearson ideas are by now the most common and widespread, and in the opinion of some of us, they are also the most widely misunderstood and abused idea.</p>
<p>What we have been mainly doing is talking about hypothesis testing from the Fisherian (Sir Ronald Fisher, the ANOVA guy) perspective. This is a basic perspective that can’t be easily ignored. It is also quite limited. The basic idea is this:</p>
<ol type="1">
<li>We know that chance can cause some differences when we measure something between experimental conditions.</li>
<li>We want to rule out the possibility that the difference that we observed can not be due to chance</li>
<li>We construct large N designs that permit us to do this when a real effect is observed, such that we can confidently say that big differences that we find are so big (well outside the chance window) that it is highly implausible that chance alone could have produced.</li>
<li>The final conclusion is that chance was extremely unlikely to have produced the differences. We then infer that something else, like the manipulation, must have caused the difference.</li>
<li>We don’t say anything else about the something else.</li>
<li>We either reject the null distribution as an explanation (that chance couldn’t have done it), or retain the null (admit that chance could have done it, and if it did we couldn’t tell the difference between what we found and what chance could do)</li>
</ol>
<p>Neyman and Pearson introduced one more idea to this mix, the idea of an alternative hypothesis. The alternative hypothesis is the idea that if there is a true effect, then the data sampled into each condition of the experiment must have come from two different distributions. Remember, when there is no effect we assume all of the data cam from the same distribution (which by definition can’t produce true differences in the long run, because all of the numbers are coming from the same distribution). The graphs of effect-sizes from before show examples of these alternative distributions, with samples for condition A coming from one distribution, and samples from condition B coming from a shifted distribution with a different mean.</p>
<p>So, under the Neyman-Pearson tradition, when a researcher find a signifcant effect they do more than one things. First, they reject the null-hypothesis of no differences, and they accept the alternative hypothesis that there was differences. This seems like a sensible thing to do. And, because the researcher is actually interested in the properties of the real effect, they might be interested in learning more about the actual alternative hypothesis, that is they might want to know if their data come from two different distributions that were separated by some amount…in other words, they would want to know the size of the effect that they were measuring.</p>
</section>
<section id="back-to-power" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="back-to-power"><span class="header-section-number">6.6.2</span> Back to power</h3>
<p>We have now discussed enough ideas to formalize the concept of statistical power. For this concept to exist we need to do a couple things.</p>
<ol type="1">
<li>Agree to set an alpha criterion. When the p-value for our test-statistic is below this value we will call our finding statistically significant, and agree to reject the null hypothesis and accept the “alternative” hypothesis (sidenote, usually it isn’t very clear which specific alternative hypothesis was accepted)</li>
<li>In advance of conducting the study, figure out what kinds of effect-sizes our design is capable of detecting with particular probabilites.</li>
</ol>
<p>The power of a study is determined by the relationship between</p>
<ol type="1">
<li>The sample-size of the study</li>
<li>The effect-size of the manipulation</li>
<li>The alpha value set by the researcher.</li>
</ol>
<p>To see this in practice let’s do a simulation. We will do a t-test on a between-groups design 10 subjects in each group. Group A will be a control group with scores sampled from a normal distribution with mean of 10, and standard deviation of 5. Group B will be a treatment group, we will say the treatment has an effect-size of Cohen’s <span class="math inline">\(d\)</span> = .5, that’s a standard deviation shift of .5, so the scores with come from a normal distribution with mean =12.5 and standard deivation of 5. Remember 1 standard deviation here is 5, so half of a standard deviation is 2.5.</p>
<p>The following R script runs this simulated experiment 1000 times. We set the alpha criterion to .05, this means we will reject the null whenever the <span class="math inline">\(p\)</span>-value is less than .05. With this specific design, how many times out of of 1000 do we reject the null, and accept the alternative hypothesis?</p>
<div class="cell">
<pre><code>#&gt; [1] 190</code></pre>
</div>
<p>The answer is that we reject the null, and accept the alternative 190 times out of 1000. In other words our experiment succesfully accepts the alternative hypothesis 19 percent of the time, this is known as the power of the study. Power is the probability that a design will succesfully detect an effect of a specific size.</p>
<p>Importantly, power is completely abstract idea that is completely determined by many assumptions including N, effect-size, and alpha. As a result, it is best not to think of power as a single number, but instead as a family of numbers.</p>
<p>For example, power is different when we change N. If we increase N, our samples will more precisely estimate the true distributions that they came from. Increasing N reduces sampling error, and shrinks the range of differences that can be produced by chance. Lets’ increase our N in this simulation from 10 to 20 in each group and see what happens.</p>
<div class="cell">
<pre><code>#&gt; [1] 334</code></pre>
</div>
<p>Now the number of significant experiments i 334 out of 1000, or a power of 33.4 percent. That’s roughly doubled from before. We have made the design more sensitive to the effect by increasing N.</p>
<p>We can change the power of the design by changing the alpha-value, which tells us how much evidence we need to reject the null. For example, if we set the alpha criterion to 0.01, then we will be more conservative, only rejecting the null when chance can produce the observed difference 1% of the time. In our example, this will have the effect of reducing power. Let’s keep N at 20, but reduce the alpha to 0.01 and see what happens:</p>
<div class="cell">
<pre><code>#&gt; [1] 133</code></pre>
</div>
<p>Now only 133 out of 1000 experiments are significant, that’s 13.3 power.</p>
<p>Finally, the power of the design depends on the actual size of the effect caused by the manipulation. In our example, we hypothesized that the effect caused a shift of .5 standard deviations. What if the effect causes a bigger shift? Say, a shift of 2 standard deviations. Let’s keep N= 20, and alpha &lt; .01, but change the effect-size to two standard deviations. When the effect in the real-world is bigger, it should be easier to measure, so our power will increase.</p>
<div class="cell">
<pre><code>#&gt; [1] 1000</code></pre>
</div>
<p>Neat, if the effect-size is actually huge (2 standard deviation shift), then we have power 100 percent to detect the true effect.</p>
</section>
<section id="power-curves" class="level3" data-number="6.6.3">
<h3 data-number="6.6.3" class="anchored" data-anchor-id="power-curves"><span class="header-section-number">6.6.3</span> Power curves</h3>
<p>We mentioned that it is best to think of power as a family of numbers, rather than as a single number. To elaborate on this consider the power curve below. This is the power curve for a specific design: a between groups experiments with two levels, that uses an independent samples t-test to test whether an observed difference is due to chance. Critically, N is set to 10 in each group, and alpha is set to .05</p>
<p>In <a href="#fig-5.5powercurve" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-5.5powercurve</span></a> power (as a proportion, not a percentage) is plotted on the y-axis, and effect-size (Cohen’s d) in standard deviation units is plotted on the x-axis.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5.5powercurve" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5.5powercurve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5.5powercurve-1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5.5powercurve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.19: This figure shows power as a function of effect-size (Cohen’s d) for a between-subjects independent samples t-test, with N=10, and alpha criterion 0.05.
</figcaption>
</figure>
</div>
</div>
</div>
<p>A power curve like this one is very helpful to understand the sensitivity of a particular design. For example, we can see that a between subjects design with N=10 in both groups, will detect an effect of d=.5 (half a standard deviation shift) about 20% of the time, will detect an effect of d=.8 about 50% of the time, and will detect an effect of d=2 about 100% of the time. All of the percentages reflect the power of the design, which is the percentage of times the design would be expected to find a <span class="math inline">\(p\)</span> &lt; 0.05.</p>
<p>Let’s imagine that based on prior research, the effect you are interested in measuring is fairly small, d=0.2. If you want to run an experiment that will detect an effect of this size a large percentage of the time, how many subjects do you need to have in each group? We know from the above graph that with N=10, power is very low to detect an effect of d=0.2. Let’s make <a href="#fig-5.5powercurveN" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-5.5powercurveN</span></a> and vary the number of subjects rather than the size of the effect.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5.5powercurveN" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5.5powercurveN-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5.5powercurveN-1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5.5powercurveN-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.20: This figure shows power as a function of N for a between-subjects independent samples t-test, with d=0.2, and alpha criterion 0.05.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The figure plots power to detect an effect of d=0.2, as a function of N. The green line shows where power = .8, or 80%. It looks like we would nee about 380 subjects in each group to measure an effect of d=0.2, with power = .8. This means that 80% of our experiments would succesfully show p &lt; 0.05. Often times power of 80% is recommended as a reasonable level of power, however even when your design has power = 80%, your experiment will still fail to find an effect (associated with that level of power) 20% of the time!</p>
</section>
</section>
<section id="planning-your-design" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="planning-your-design"><span class="header-section-number">6.7</span> Planning your design</h2>
<p>Our discussion of effect size and power highlight the importance of the understanding the statistical limitations of an experimental design. In particular, we have seen the relationship between:</p>
<ol type="1">
<li>Sample-size</li>
<li>Effect-size</li>
<li>Alpha criterion</li>
<li>Power</li>
</ol>
<p>As a general rule of thumb, small N designs can only reliably detect very large effects, whereas large N designs can reliably detect much smaller effects. As a researcher, it is your responsibility to plan your design accordingly so that it is capable of reliably detecting the kinds of effects it is intended to measure.</p>
</section>
<section id="some-considerations" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="some-considerations"><span class="header-section-number">6.8</span> Some considerations</h2>
<section id="low-powered-studies" class="level3" data-number="6.8.1">
<h3 data-number="6.8.1" class="anchored" data-anchor-id="low-powered-studies"><span class="header-section-number">6.8.1</span> Low powered studies</h3>
<p>Consider the following case. A researcher runs a study to detect an effect of interest. There is good reason, from prior research, to believe the effect-size is d=0.5. The researcher uses a design that has 30% power to detect the effect. They run the experiment and find a significant p-value, (p&lt;.05). They conclude their manipulation worked, because it was unlikely that their result could have been caused by chance. How would you interpret the results of a study like this? Would you agree with thte researchers that the manipulation likely caused the difference? Would you be skeptical of the result?</p>
<p>The situation above requires thinking about two kinds of probabilities. On the one hand we know that the result observed by the researchers does not occur often by chance (p is less than 0.05). At the same time, we know that the design was underpowered, it only detects results of the expected size 30% of the time. We are face with wondering what kind of luck was driving the difference. The researchers could have gotten unlucky, and the difference really could be due to chance. In this case, they would be making a type I error (saying the result is real when it isn’t). If the result was not due to chance, then they would also be lucky, as their design only detects this effect 30% of the time.</p>
<p>Perhaps another way to look at this situation is in terms of the replicability of the result. Replicability refers to whether or not the findings of the study would be the same if the experiment was repeated. Because we know that power is low here (only 30%), we would expect that most replications of this experiment would not find a significant effect. Instead, the experiment would be expected to replicate only 30% of the time.</p>
</section>
<section id="large-n-and-small-effects" class="level3" data-number="6.8.2">
<h3 data-number="6.8.2" class="anchored" data-anchor-id="large-n-and-small-effects"><span class="header-section-number">6.8.2</span> Large N and small effects</h3>
<p>Perhaps you have noticed that there is an intriguing relationship between N (sample-size) and power and effect-size. As N increases, so does power to detect an effect of a particular size. Additionally, as N increases, a design is capable of detecting smaller and smaller effects with greater and greater power. For example, if N was large enough, we would have high power to detect very small effects, say d= 0.01, or even d=0.001. Let’s think about what this means.</p>
<p>Imagine a drug company told you that they ran an experiment with 1 billion people to test whether their drug causes a significant change in headache pain. Let’s say they found a significant effect (with power =100%), but the effect was very small, it turns out the drug reduces headache pain by less than 1%, let’s say 0.01%. For our imaginary study we will also assume that this effect is very real, and not caused by chance.</p>
<p>Clearly the design had enough power to detect the effect, and the effect was there, so the design did detect the effect. However, the issue is that there is little practical value to this effect. Nobody is going to by a drug to reduce their headache pain by 0.01%, even if it was “scientifcally proven” to work. This example brings up two issues. First, increasing N to very large levels will allow designs to detect almost any effect (even very tiny ones) with very high power. Second, sometimes effects are meaningless when they are very small, especially in applied research such as drug studies.</p>
<p>These two issues can lead to interesting suggestions. For example, someone might claim that large N studies aren’t very useful, because they can always detect really tiny effects that are practically meaningless. On the other hand, large N studies will also detect larger effects too, and they will give a better estimate of the “true” effect in the population (because we know that larger samples do a better job of estimating population parameters). Additionally, although really small effects are often not interesting in the context of applied research, they can be very important in theoretical research. For example, one theory might predict that manipulating X should have no effect, but another theory might predict that X does have an effect, even if it is a small one. So, detecting a small effect can have theoretical implication that can help rule out false theories. Generally speaking, researchers asking both theoretical and applied questions should think about and establish guidelines for “meaningful” effect-sizes so that they can run designs of appropriate size to detect effects of “meaningful size”.</p>
</section>
<section id="small-n-and-large-effects" class="level3" data-number="6.8.3">
<h3 data-number="6.8.3" class="anchored" data-anchor-id="small-n-and-large-effects"><span class="header-section-number">6.8.3</span> Small N and Large effects</h3>
<p>All other things being equal would you trust the results from a study with small N or large N? This isn’t a trick question, but sometimes people tie themselves into a knot trying to answer it. We already know that large sample-sizes provide better estimates of the distributions the samples come from. As a result, we can safely conclude that we should trust the data from large N studies more than small N studies.</p>
<p>At the same time, you might try to convince yourself otherwise. For example, you know that large N studies can detect very small effects that are practically and possibly even theoretically meaningless. You also know that that small N studies are only capable of reliably detecting very large effects. So, you might reason that a small N study is better than a large N study because if a small N study detects an effect, that effect must be big and meaningful; whereas, a large N study could easily detect an effect that is tiny and meaningless.</p>
<p>This line of thinking needs some improvement. First, just because a large N study can detect small effects, doesn’t mean that it only detects small effects. If the effect is large, a large N study will easily detect it. Large N studies have the power to detect a much wider range of effects, from small to large. Second, just because a small N study detected an effect, does not mean that the effect is real, or that the effect is large. For example, small N studies have more variability, so the estimate of the effect size will have more error. Also, there is 5% (or alpha rate) chance that the effect was spurious. Interestingly, there is a pernicious relationship between effect-size and type I error rate</p>
</section>
<section id="type-i-errors-are-convincing-when-n-is-small" class="level3" data-number="6.8.4">
<h3 data-number="6.8.4" class="anchored" data-anchor-id="type-i-errors-are-convincing-when-n-is-small"><span class="header-section-number">6.8.4</span> Type I errors are convincing when N is small</h3>
<p>So what is this pernicious relationship between Type I errors and effect-size? Mainly, this relationship is pernicious for small N studies. For example, the following figure illustrates the results of 1000s of simulated experiments, all assuming the null distribution. In other words, for all of these simulations there is no true effect, as the numbers are all sampled from an identical distribution (normal distribution with mean =0, and standard deviation =1). The true effect-size is 0 in all cases.</p>
<p>We know that under the null, researchers will find p values that are less 5% about 5% of the time, remember that is the definition. So, if a researcher happened to be in this situation (where there manipulation did absolutely nothing), they would make a type I error 5% of the time, or if they conducted 100 experiments, they would expect to find a significant result for 5 of them.</p>
<p><a href="#fig-5.5effectsizeType1" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-5.5effectsizeType1</span></a> reports the findings from only the type I errors, where the simulated study did produce p &lt; 0.05. For each type I error, we calculated the exact p-value, as well as the effect-size (cohen’s D) (mean difference divided by standard deviation). We already know that the true effect-size is zero, however take a look at this graph, and pay close attention to the smaller sample-sizes.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5.5effectsizeType1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5.5effectsizeType1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5.5effectsizeType1-1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5.5effectsizeType1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.21: Effect size as a function of p-values for type 1 Errors under the null, for a paired samples t-test.
</figcaption>
</figure>
</div>
</div>
</div>
<p>For example, look at the red dots, when sample size is 10. Here we see that the effect-sizes are quite large. When p is near 0.05 the effect-size is around .8, and it goes up and up as when p gets smaller and smaller. What does this mean? It means that when you get unlucky with a small N design, and your manipulation does not work, but you by chance find a “significant” effect, the effect-size measurement will show you a “big effect”. This is the pernicious aspect. When you make a type I error for small N, your data will make you think there is no way it could be a type I error because the effect is just so big!. Notice that when N is very large, like 1000, the measure of effect-size approaches 0 (which is the true effect-size in the simulation shown in <a href="#fig-5.5cohensD" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-5.5cohensD</span></a>).</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5.5cohensD" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5.5cohensD-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-Hypothesis_Testing_files/figure-html/fig-5.5cohensD-1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5.5cohensD-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.22: Each panel shows a histogram of a different sampling statistic.
</figcaption>
</figure>
</div>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-mallory_barnes" class="csl-entry" role="listitem">
Barnes, Mallory L. 2023. <em>Statistics for Environmental Science</em>.
</div>
<div id="ref-Cohen1988" class="csl-entry" role="listitem">
Cohen, J. 1988. <em>Statistical Power Analysis for the Behavioral Sciences</em>. Second. <span>Lawrence Erlbaum</span>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./05-Foundation_Inference.html" class="pagination-link" aria-label="Foundations for inference">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Foundations for inference</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./07-ttests.html" class="pagination-link" aria-label="t-tests">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">t-tests</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/malloryb/statistics_E538/edit/master/06-Hypothesis_Testing.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div></div></footer></body></html>