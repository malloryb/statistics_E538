{
  "hash": "55764d286c2c553e5ee9e66fa691fdb4",
  "result": {
    "engine": "knitr",
    "markdown": "---\nauthor: Mallory L. Barnes\naliases: [redirects/DescribingData.html]\n---\n\n\n\n\n\n\n\n\n# Describing Data {#DescribingData}\n\n> Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise. \\*\\\n---John W. Tukey\n\nStatistics often begins with the problem of **too much data**. Modern environmental science generates vast amounts of information: rainfall at hundreds of stations, thousands of tree measurements in a forest survey, millions of satellite pixels. Looking at raw numbers alone is overwhelming and uninformative. We need tools to compress, summarize, and visualize. These are **descriptive statistics**.\n\nKeep in mind:\n\n1.  There is more than one useful way to describe data.\n2.  You choose a description depending on what helps reveal the pattern you care about.\n3.  All methods were invented for that reason. They are helpful shortcuts.\n\nImagine running a survey of 500 people’s happiness levels. The full dataset is just a long list of values. Staring at hundreds of numbers tells us little. How happy are most people? How unhappy are the unhappiest? Raw data alone can't answer, we need to **summarize.**\n\n## Looking at the data\n\n### Scatter of raw values\n\nOne first step is simply to **plot every value** instead of listing them. Let's look at them!\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Pretend happiness ratings from 500 people](02-Describing_Data_files/figure-html/fig-happyPlot-1.png){#fig-happyPlot fig-alt='Scatter plot mapping each individual\\'s index (x-axis) to their happiness rating (y-axis), summarizing the distribution and central tendency of 500 participants\\' happiness scores' width=75%}\n:::\n:::\n\n\n\n\n\n\n@fig-happyPlot shows 500 measurements of happiness. The horizontal axis is an index of participants (1–500), and the vertical axis shows their reported happiness. Each dot represents one person’s score.\n\nThe way we choose to plot data makes some patterns easier to see and others harder. What do we notice here? There are indeed 500 dots, spread across a wide range: some values climb as high as about 1500, others fall below –1500. Most points cluster somewhere near zero, with fewer at the extremes.\n\n**Take-home:** plotting all the values at once is already much more useful than staring at a raw table of numbers.\n\nStill, the scatter doesn’t make it easy to answer questions like “are most people generally happy or unhappy?” For that, we need a different view, a **histogram**.\n\n### Histograms\n\nMaking a histogram summarizes data by grouping numbers rather than looking at individual data points. @fig-happyHist displays 500 happiness scores grouped into bins. Each bar shows how many people's responses fall within a range. The bar between 0 and 500 indicates about 150 people in that range. The y-axis shows the frequency count of each bar.\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![A histogram of the happiness ratings](02-Describing_Data_files/figure-html/fig-happyHist-1.png){#fig-happyHist fig-alt='Histogram displaying happiness ratings in vertical bars, height signifying frequency, grouped into 500-point bins to convey the distribution and range of the data.' width=75%}\n:::\n:::\n\n\n\n\n\n\nThe histogram makes patterns clearer than the scatterplot. Most of the responses fall between –500 and +500, with only a few extreme highs or lows. We can also see the overall spread of the data: roughly –1500 to +1500.\n\nWhen making histograms, the width of the bins matters. If the bins are wide, the plot looks smooth but may hide detail. If they are narrow, the plot shows more variation but can appear noisy. @fig-manyhistbin compares the same data with different bin widths.\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Four histograms of the same data using different bin widths](02-Describing_Data_files/figure-html/fig-manyhistbin-1.png){#fig-manyhistbin fig-alt='Four histograms, each showing happiness ratings with vertical bars indicating frequency. The bars are grouped into varying bin sizes, demonstrating the effects of bin size on data interpretation' width=75%}\n:::\n:::\n\n\n\n\n\n\nAll of the histograms show the same general shape: few values at the extremes and many near zero. Narrow bins make the bars jump up and down, adding noise. Wider bins smooth the pattern, but if they’re too wide, important variation gets lost.\n\n**Take-home:** histograms summarize individual values into a distribution, revealing both the typical range and how the data are spread. Bin width is a choice you make, and it affects what you see.\n\n## Important Ideas: Distribution, Central Tendency, and Variance\n\nThree terms will keep coming up: **distribution, central tendency, and variance.** Their everyday meanings aren’t far from the statistical ones.\n\n-   **Distribution**: how values are spread across their range. A histogram is one way of showing a distribution. The shape of the distribution tells us where values are concentrated, where they’re rare, and how far the extremes reach. Later we’ll see that distributions don’t just describe data, we also use them to **generate theoretical data**, which is the basis for sampling distributions and tools like *t*-tests.\n\n-   **Central tendency** is about *sameness*: where values cluster. In the happiness histogram, most responses are near zero, so we say the data have a central tendency around zero. Central tendency doesn’t mean “everything is the same”, just that many values gather in a common area. Some datasets even have more than one cluster, and therefore more than one central tendency.\n\n-   **Variance** is about *differentness*: how spread out the values are. If all responses were nearly identical, variance would be low. In our happiness example, values range widely from strongly negative to strongly positive, so variance is high. In practice, variance is a family of measures we use to quantify differences, and we’ll introduce those next.\n\n## Measures of Central Tendency (Sameness)\n\nPlots and histograms show where values fall, but lack precision. We can summarize data with a single number representing its \"center\" - these **measures of central tendency** show what values are generally like.\n\n### Mode\n\nThe **mode** is the most frequently occurring value in a dataset. How do you find it? You have to count the number of times each number appears, then whichever one occurs the most, is the mode.\n\n> Example: 1 1 1 2 3 4 5 6\n\nThe mode is 1, since it appears three times.\n\nIf two or more values tie for most frequent, the dataset has multiple modes:\n\n> Example: 1 1 1 2 2 2 3 4 5 6\n\nHere, 1 and 2 both occur three times each. So, there are two modes, and they are 1 and 2.\n\nIs the mode a good measure of central tendency? That depends on your numbers. The mode can be useful when a dataset has clear repeating values, but it doesn’t always capture what’s “typical.” For example:\n\n> 1 1 2 3 4 5 6 7 8 9\n\nHere, the mode is 1, but most numbers are larger. Like any statistical tool, consider if the mode suits your dataset and justify that choice.\n\n### Median\n\nThe **median** is the exact middle of the data once they are ordered from smallest to largest. For example:\n\n> 1 5 4 3 6 7 9\n\nBefore we can compute the median, we need to order the numbers from smallest to largest. Ordered:\n\n> 1 3 4 **5** 6 7 9\n\nThe median is 5, with three numbers on each side.\n\nWith an even number of values, the median is the midpoint between the two in the center:\n\n> 1 2 3 4 5 6\n\nHere we have six numbers, so there isn’t a single middle value. Instead, the median is the midpoint between the two central numbers, 3 and 4, which gives 3.5.\n\nThe median is often a useful measure of central tendency because it stays put even if some values are extreme. For example:\n\n> 1 2 3 4 4 4 **5** 6 6 6 7 7 1000\n\nMost of these numbers are modest, but 1000 is far from the rest. The median is still 5, which reflects the bulk of the data despite that extreme value. This shows why the median often does a good job of representing a dataset even when one or two values are very different.\n\nIn this case, 1000 would be considered an **outlier**: a value much farther from the others. Outliers can strongly affect some summaries (like the mean) but leave the median unchanged. How to handle outliers is a topic we return to later in the course.\n\n### Mean\n\nHave you noticed this statistics textbook hasn't used a formula yet? That's about to change, but don't worry if you have formula anxiety - we'll explain them. The **mean** is also called the average. You probably know it's the sum of numbers divided by how many numbers there are.\n\n**Here's the formula:**\n\n$Mean = \\bar{X} = \\frac{\\sum_{i=1}^{n} x_{i}}{N}$\n\n-   The $\\sum$ symbol is called **sigma**, and it stands for the operation of summing.\n\n-   The \"i\" and \"n\" refer to all numbers in the set, from first to last.\n\n-   The $x_{i}$ refers to individual numbers in the set.\n\n-   $\\bar{X}$ refers to the mean\n\n-   We sum them all, then divide by $N$, the total count.\n\nI**n simpler terms:**\n\n$mean = \\frac{\\text{Sum of my numbers}}{\\text{Count of my numbers}}$\n\nLet's compute the mean for these five numbers:\n\n> 3 7 9 2 6\n\nAdd them:\n\n> 3+7+9+2+6 = 27\n\nCount them:\n\n> $i_{1}$ = 3, $i_{2}$ = 7, $i_{3}$ = 9, $i_{4}$ = 2, $i_{5}$ = 6; N=5, because $i$ went from 1 to 5\n\nDivide:\n\n> mean = 27 / 5 = 5.4\n\nOr, putting everything in the formula:\n\n$Mean = \\bar{X} = \\frac{\\sum_{i=1}^{n} x_{i}}{N} = \\frac{3+7+9+2+6}{5} = \\frac{27}{5} = 5.4$\n\nThat's how to compute the mean. You probably knew this already, and if not, now you do. But is the mean a good measure of central tendency? By now, you should know: it depends.\n\n### What does the mean mean?\n\nIt’s not enough to know how to calculate a mean, you also need to understand what it represents. The formula divides the sum of all values by the number of values, but what does that operation actually do?\n\nThink about division. If we compute\n\n$\\frac{12}{3} = 4$\n\nwe’re splitting 12 into three equal parts. Each part is 4. Division equalizes the numerator into identical pieces.\n\n\"The same logic applies to the mean. Suppose you add up all 500 happiness ratings into one large total. If that total were redistributed equally across every person, each would get the same value. That value is the mean. Some individuals were originally higher or lower, but the mean is the balance point created by spreading the total evenly.\n\nThis is why the mean is more than just arithmetic. It is the one number that can replace every observation so that, when all those replacements are added together, you get back the original total.\n\n::: {.callout-note title=\"Takeaway\"}\nThe mean is unique: it is the only single value that can replace every observation so that the total stays the same.\n:::\n\n### Comparing the mean, median, and mode\n\n@fig-meanmodemed shows a histogram of simulated data with three vertical lines: the mode (blue, ≈ 1), the median (green, ≈ 6), and the mean (red, ≈ 8).\n\nNotice that these three measures of central tendency do not agree. The **mean** is pulled to the right because of a few very large values in the tail. The **median** is more stable, landing near the middle of the bulk of the data. The **mode** marks the most common single value, which here is close to zero.\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![A histogram with the mean (red), the median (green), and the mode (blue)](02-Describing_Data_files/figure-html/fig-meanmodemed-1.png){#fig-meanmodemed fig-alt='Histogram of \\'exp_num\\' (0-60) on x-axis against frequency on y-axis. Data clusters near zero with a long right tail. Mode (around 1), median (around 6), and mean (around 8) illustrate the disparity among measures of central tendency' width=75%}\n:::\n:::\n\n\n\n\n\n\n::: {.callout-note title=\"Takeaway\"}\nWhen data are skewed—as they are here with a long right tail—the mean, median, and mode can give very different answers. Understanding how each behaves is essential for choosing the right summary.\n:::\n\n## Measures of Variation (Different*ness*)\n\nCentral tendency tells us what values have in common. Measures of **variation** tell us how they differ. Any dataset with more than one value will show some variation, and summarizing that spread is as important as finding the center.\n\n### The Range\n\nConsider these 10 ordered numbers:\n\n> 1 3 4 5 5 6 7 8 9 24\n\nThe smallest is 1, the largest is 24. Together they define the **range**. The range quickly shows the boundaries of the data and can flag possible outliers. For instance, if you expected values between 1 and 7 but found one at 340,500, you’d know something unusual happened and might investigate.\n\n### Difference Scores\n\nIt would be nice to summarize the amount of different*ness* in the data. Think about these 10 ordered numbers:\n\n> 1 3 4 5 5 6 7 8 9 24\n\nWe can compute the differences between each pair of numbers and put them in a matrix:\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|   |   1|   3|   4|   5|   5|   6|   7|   8|   9| 24|\n|:--|---:|---:|---:|---:|---:|---:|---:|---:|---:|--:|\n|1  |   0|   2|   3|   4|   4|   5|   6|   7|   8| 23|\n|3  |  -2|   0|   1|   2|   2|   3|   4|   5|   6| 21|\n|4  |  -3|  -1|   0|   1|   1|   2|   3|   4|   5| 20|\n|5  |  -4|  -2|  -1|   0|   0|   1|   2|   3|   4| 19|\n|5  |  -4|  -2|  -1|   0|   0|   1|   2|   3|   4| 19|\n|6  |  -5|  -3|  -2|  -1|  -1|   0|   1|   2|   3| 18|\n|7  |  -6|  -4|  -3|  -2|  -2|  -1|   0|   1|   2| 17|\n|8  |  -7|  -5|  -4|  -3|  -3|  -2|  -1|   0|   1| 16|\n|9  |  -8|  -6|  -5|  -4|  -4|  -3|  -2|  -1|   0| 15|\n|24 | -23| -21| -20| -19| -19| -18| -17| -16| -15|  0|\n\n\n:::\n:::\n\n\n\n\n\n\nIn the top left, the difference between 1 and itself is 0. One column over, 3–1 = 2, and so on. With 10 numbers this produces 100 differences. With 500 numbers, you’d have 250,000—too many to be useful.\n\nIf all the numbers were the same, every difference would be 0, making the lack of variation obvious. But with different numbers, we end up with a large table. How can we summarize it? One idea is to apply what we learned about central tendency and take the **average difference**.\n\nLet's try it with three numbers:\n\n> 1 2 3\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|   |  1|  2|  3|\n|:--|--:|--:|--:|\n|1  |  0|  1|  2|\n|2  | -1|  0|  1|\n|3  | -2| -1|  0|\n\n\n:::\n:::\n\n\n\n\n\n\nThe mean of these nine difference scores is:\n\n$\\text{mean of difference scores} = \\frac{0+1+2-1+0+1-2-1+0}{9} = \\frac{0}{9} = 0$\n\nThis will always happen: the positives and negatives cancel. So the mean of raw difference scores is not a useful measure of variation.\n\nNotice also that the matrix is redundant: the diagonal is always zero, and values above and below the diagonal are the same except for sign.\n\nThese problems motivate why we compute **variance** and **standard deviation**. They solve the cancellation issue and give us a practical summary of spread.\n\n### The Variance\n\nWe’ve used the words variability, variation, and variance a lot. They all point to the same big idea: numbers differ. When numbers are different, they have variance.\\]\n\nThe word *variance* is used in two ways. First, it can mean the general idea of differences between numbers—when values vary, there is variance. Second, it refers to a specific summary statistic: the **mean of the squared deviations from the mean**.\n\nTo calculate it, we take each score, subtract the mean to find its **difference score**, then square those differences, then average them. Squaring is the mathematical “trick” that keeps positive and negative deviations from canceling each other out. Finally, we average the squared deviations. In short:\n\n$variance = \\frac{\\text{Sum of squared difference scores}}{\\text{Number of Scores}}$\n\nLater we’ll distinguish between dividing by $N$ (all data = population) or $N-1$ (sample). For now, just use $N$.\n\n#### Deviations from the mean\n\nEarlier we compared every number to every other number, which quickly became unmanageable. A simpler approach is to compare each score to the mean.\n\n-   Step 1: Find the mean.\n\n-   Step 2: Subtract the mean from each score.\n\nThis tells us:\n\n1.  How well the mean represents the data\n\n2.  How much spread there is around that mean.\n\nHere's an example:\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|scores |values |mean |Difference_from_Mean |\n|:------|:------|:----|:--------------------|\n|1      |1      |4.5  |-3.5                 |\n|2      |6      |4.5  |1.5                  |\n|3      |4      |4.5  |-0.5                 |\n|4      |2      |4.5  |-2.5                 |\n|5      |6      |4.5  |1.5                  |\n|6      |8      |4.5  |3.5                  |\n|Sums   |27     |27   |0                    |\n|Means  |4.5    |4.5  |0                    |\n\n\n:::\n:::\n\n\n\n\n\n\nThe mean is $4.5$:\n\n$\\frac{1+6+4+2+6+8}{6} = \\frac{27}{6} = 4.5$.\n\nThe third column simply repeats this mean for each row. That looks odd, but it shows the important property I mentioned earlier: the mean distributes the total equally across all points. Six copies of 4.5 add back to the total of 27.\n\nThe fourth column shows the **deviation from the mean:** $X_{i}-\\bar{X}$. For example, 1, is -3.5 from the mean, 6, is +1.5, and so on.\n\nNow notice the problem: the deviations add up to zero. The positive and negative differences cancel, which makes it seem like there’s no variation. Clearly that isn’t true, so we’ll need another step, squaring the deviations, to solve it. But first let's investigate *why* this happens.\n\n------------------------------------------------------------------------\n\n#### Mean as the Balancing Point\n\nThe mean is the balancing point of the data. Imagine laying a ruler across your finger. The place where it balances is the point where the weight on each side is equal. Data works the same way: if we treat each value like a weight, the mean is where the total “mass” to the left and right cancel out.\n\nThis balancing property explains why the deviations always sum to zero. The values below the mean contribute negative deviations, the values above contribute positive ones, and together they cancel:\n\n$-x + x = 0$\n\nTo see this more concretely, consider the numbers\n\n$X = (1,2,6,7,9)$\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Values on a number line.](02-Describing_Data_files/figure-html/fig-number-line-1.png){#fig-number-line fig-alt='Horizontal number line with dots at 1, 2, 6, 7, and 9, each labeled above.' width=75%}\n:::\n:::\n\n\n\n\n\n\nNow imagine the number line as a teeter-totter. Only when the fulcrum is placed at 5 does the board balance:\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Mean as the unique balancing point. Only at the mean do signed deviations cancel.](02-Describing_Data_files/figure-html/fig-mean-balance-panels-1.png){#fig-mean-balance-panels fig-alt='Three panels with a horizontal number line and a triangular fulcrum; the board tilts left/right when the fulcrum is not at the mean, and is level at the mean.' width=75%}\n:::\n:::\n\n\n\n\n\n\nFormally, this means the signed distances from the mean always cancel out:\n\n$(1−5)+(2−5)+(6−5)+(7−5)+(9−5)=0$\n\nThis makes the mean unique. It is the **only value** for which the sum of deviations equals zero:\n\n$\\sum_{i=1}^{n}x_{{i}-a} = 0 \\quad \\Rightarrow \\quad a = \\bar{x}$\n\nThe mean is not just a “typical” value, it is the one point where the data balance. And that balancing property is exactly why the raw deviations always sum to zero. To summarize variation, we need a way around this problem.\n\n#### The squared deviations\n\nThe standard trick is to square the deviations. Squaring converts negatives to positives:\n\n$2^2 = 4$\n\n$-2^2 = 4$.\\\n\\\nSince a squared number is always non-negative, the deviations no longer cancel. We call these **squared deviations**: differences from the mean that have been squared.\n\nLet’s revisit our table, this time adding a column for squared deviations:\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|scores |values |mean |Difference_from_Mean |Squared_Deviations |\n|:------|:------|:----|:--------------------|:------------------|\n|1      |1      |4.5  |-3.5                 |12.25              |\n|2      |6      |4.5  |1.5                  |2.25               |\n|3      |4      |4.5  |-0.5                 |0.25               |\n|4      |2      |4.5  |-2.5                 |6.25               |\n|5      |6      |4.5  |1.5                  |2.25               |\n|6      |8      |4.5  |3.5                  |12.25              |\n|Sums   |27     |27   |0                    |35.5               |\n|Means  |4.5    |4.5  |0                    |5.91666666666667   |\n\n\n:::\n:::\n\n\n\n\n\n\nFor example, the first score is 1, which is 3.5 below the mean. Its deviation is $−3.5$, and its squared deviation is $(−3.5)^2=12.25$.\n\nNow that all deviations are positive, we can add them up. The result is the **sum of squares (SS)**, the sum of squared deviations from the mean. You’ll see this quantity again in the ANOVA chapter, but the idea is simple: it’s just the total of those squared deviations, nothing more.\n\n#### Finally, the variance\n\nGuess what, we’ve already computed the variance. Maybe you didn’t notice.\n\nLet’s remind ourselves of the goal: we want a single number that summarizes how spread out the data are. Deviations from the mean show those differences, but there are as many deviations as data points. To simplify, we want the *average* squared deviation.\n\nLook back at the table. We added up the squared deviations (the *sum of squares*, SS), and then we divided by the number of observations. That's **the variance**. The variance is the mean of the sum of the squared deviations:\n\n$variance = \\frac{SS}{N}$\n\nwhere SS is the sum of squared deviations, and N is the number of observations.\n\nFor our data, this came out to 5.916 (repeating).\n\nSo what does that number mean? Honestly, not much on its own. The problem is that it’s on the *squared* scale, remember, we squared the deviations before averaging. Squaring inflates the values, so the variance is no longer in the same units as the original data.\n\nThe fix is straightforward: take the square root. For our example,\n\n$\\sqrt{5.916} ≈2.43$\n\n### The Standard Deviation\n\nWe did it again, we already computed the **standard deviation (SD)**, without calling it by name. The standard deviation is just the square root of the variance:\n\n$\\text{standard deviation} = \\sqrt{Variance} = \\sqrt{\\frac{SS}{N}}$.\n\nor, written out fully,\n\n$\\text{standard deviation} = \\sqrt{\\frac{\\sum_{i}^{n}({x_{i}-\\bar{x})^2}}{N}}$\n\nThose square root signs simply “unsquare” the variance, bringing the measure of spread back to the original scale of the data. Let's look at our table again:\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|scores |values |mean |Difference_from_Mean |Squared_Deviations |\n|:------|:------|:----|:--------------------|:------------------|\n|1      |1      |4.5  |-3.5                 |12.25              |\n|2      |6      |4.5  |1.5                  |2.25               |\n|3      |4      |4.5  |-0.5                 |0.25               |\n|4      |2      |4.5  |-2.5                 |6.25               |\n|5      |6      |4.5  |1.5                  |2.25               |\n|6      |8      |4.5  |3.5                  |12.25              |\n|Sums   |27     |27   |0                    |35.5               |\n|Means  |4.5    |4.5  |0                    |5.91666666666667   |\n\n\n:::\n:::\n\n\n\n\n\n\nWe calculated our standard deviation as:\n\n$\\sqrt{5.916} ≈2.43$\n\nThis value makes much more sense than the variance. A variance of 5.916 feels too large for this dataset, because it’s on the squared scale. But a standard deviation of 2.43 lines up with the actual differences from the mean—most scores are within about ±2.4 of 4.5.\n\nSo if someone told you their dataset had a mean of 4.5 and a standard deviation of 2.4, you’d already have a good sense of it: the numbers cluster around 4.5, but not exactly at 4.5, and the typical spread is about 2 to 3 units in either direction.\n\nThat’s the power of the standard deviation: it gives you a single number that describes the *typical deviation* from the mean, while staying on the same scale as the original data.\n\n### Mean Absolute Deviation \n\nSo far we’ve handled the “differences cancel out” problem by squaring deviations. But there’s another simple option: take the **absolute value** of each deviation from the mean. That way, every difference becomes positive, and when we add them up we don’t end up at zero.\n\nHere’s what that looks like for our data:\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|scores |values |mean |Difference_from_Mean |Absolute_Deviations |\n|:------|:------|:----|:--------------------|:-------------------|\n|1      |1      |4.5  |-3.5                 |3.5                 |\n|2      |6      |4.5  |1.5                  |1.5                 |\n|3      |4      |4.5  |-0.5                 |0.5                 |\n|4      |2      |4.5  |-2.5                 |2.5                 |\n|5      |6      |4.5  |1.5                  |1.5                 |\n|6      |8      |4.5  |3.5                  |3.5                 |\n|Sums   |27     |27   |0                    |13                  |\n|Means  |4.5    |4.5  |0                    |2.16666666666667    |\n\n\n:::\n:::\n\n\n\n\n\n\nThe last column shows absolute deviations. If we take their mean, we get the mean absolute deviation (MAD). Notice this acts a lot like the standard deviation: it’s the average size of a deviation from the mean, only without squaring.Both approaches—squaring or taking absolute values—solve the same problem in slightly different ways. Squaring is more common because it leads to nice mathematical properties (like in regression and ANOVA), but the MAD can be more intuitive to interpret.\n\n## Remember to look at your data\n\nDescriptive statistics are useful, but they are also compressed summaries. They reduce a dataset to a few numbers, which means they always lose detail. That’s fine if the summary captures the important features, but sometimes it doesn’t.\n\nThe safest habit is to **always combine descriptive statistics with graphs**. Graphs show you the shape of the data and reveal patterns summaries can hide.\n\n### Anscombe's Quartet\n\nFrancis Anscombe (@Anscombe1973) made this point with a famous example, now called *Anscombe’s Quartet*. Each panel in Figure @fig-anscombe shows pairs of $x$ and $y$ values as a scatterplot. \n\nVisually, the four panels are very different: one looks linear, one curved, one has an outlier, and one forms a vertical line.\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Anscombe's Quartet](02-Describing_Data_files/figure-html/fig-anscombe-1.png){#fig-anscombe fig-alt='Anscombe\\'s Quartet, a set of four scatter plots each with a different pattern, despite having identical descriptive statistics. This illustrates the importance of graphing data to understand underlying relationships beyond basic statistical measures' width=75%}\n:::\n:::\n\n\n\n\n\n\nNow here’s the kicker:\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|quartet | mean_x| var_x|   mean_y|    var_y|\n|:-------|------:|-----:|--------:|--------:|\n|1       |      9|    11| 7.500909| 4.127269|\n|2       |      9|    11| 7.500909| 4.127629|\n|3       |      9|    11| 7.500000| 4.122620|\n|4       |      9|    11| 7.500909| 4.123249|\n\n\n:::\n:::\n\n\n\n\n\n\nAll four datasets have the **exact same descriptive statistics**:\n\nsame mean and variance for $x$\n\nsame mean and variance for $y$\n\nsame correlation between $x$ and $y$\n\nYet the scatterplots could not look more different.\n\n::: {.callout-note title=\"Takeaway\"}\nDescriptive statistics alone can be misleading. Always look at the graph. Numbers may be identical, but patterns can be radically different.\n:::\n\n### Datasaurus Dozen\n\nIf you thought that Anscombe's quartet was neat, you should take a look at the [Datasaurus Dozen](https://www.autodeskresearch.com/publications/samestats) [@matejka2017same]. hey constructed 13 datasets with nearly identical means, variances, and correlations — but when plotted, the points form shapes like a star, a circle, or even a dinosaur. Another reminder: your numbers might look like a dinosaur if you don’t plot them.\n\n## Videos\n\n### Measures of center: Mode\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/hQ2p-QQpGso\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n\n</iframe>\n\n### Measures of center: Median and Mean\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/BopmCXCjq08\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n\n</iframe>\n\n### Standard deviation part I\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/8Yguf93s5dI\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n\n</iframe>\n\n### Standard deviation part II\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KodmsOXScBc\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n\n</iframe>\n",
    "supporting": [
      "02-Describing_Data_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}